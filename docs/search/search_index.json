{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>   Welcome to unitorch  </p>    \ud83d\udd25 unitorch is a library that simplifies and accelerates the   development of unified models for NLP, NLG, CV,   CTR, MM and RL. It is built on top of PyTorch and   integrates seamlessly with popular frameworks such as transformers, peft,   diffusers and fastseq.       Get started"},{"location":"configuration/","title":"Configuration","text":"<p>\u00a0\u00a0\u00a0\u00a0unitorch command workflow for modeling training/inference is using a unified configuration system. In this example, we'll explore the configuration of the BartForGeneration class in unitorch.</p> <p>\u00a0\u00a0\u00a0\u00a0 Here is a training command example for bart generation with local files.</p> <pre><code>unitorch-train \\\n    configs/generation/bart.ini \\\n    --train_file train.tsv \\\n    --dev_file dev.tsv \\\n    --core/model/generation/bart@num_beams 20 \\\n    --core/model/generation/bart@no_repeat_ngram_size 0\n</code></pre> <p>\u00a0\u00a0\u00a0\u00a0This is the training command for bart generation. In this command, we provide the path to the configuration file <code>configs/core/generation/bart.ini</code> and specify additional parameters using the <code>--</code> syntax. The parameters provided after <code>--core/model/generation/bart@</code> override the corresponding values in the configuration file. In this example, we override <code>num_beams</code> to 20 and <code>no_repeat_ngram_size</code> to 0. The configuration file using an INI file format is as follows.</p> <pre><code># model\n[core/model/generation/bart]\npretrained_name = bart-base\nno_repeat_ngram_size = 3\nmax_gen_seq_length = 15\n\n# dataset\n[core/dataset/ast]\nnames = ['encode', 'decode']\n\n# ...\n</code></pre> <p>In this configuration, we specify the following parameters:</p> <ul> <li><code>pretrained_name</code>: The name of the pretrained model. In this example, it is set to bart-base.</li> <li><code>no_repeat_ngram_size</code>: The size of n-grams to avoid repeating in the generated sequences. It is set to 3 in this example. Because we override this parameter in command line, it would be set to 0 finally.</li> <li><code>max_gen_seq_length</code>: The maximum length of the generated sequences. It is set to 15 in this example.</li> </ul> <p>Then let's check the BartForGenertaion model class code.</p> <pre><code>class BartForGeneration(_BartForGeneration):\n    def __init__(\n        self,\n        config_path: str,\n        gradient_checkpointing: Optional[bool] = False,\n    ):\n        pass\n\n    @classmethod\n    @add_default_section_for_init(\"core/model/generation/bart\")\n    def from_core_configure(cls, config, **kwargs):\n        config.set_default_section(\"core/model/generation/bart\")\n        pretrained_name = config.getoption(\"pretrained_name\", \"default-bart\")\n        config_path = config.getoption(\"config_path\", None)\n        config_path = pop_value(\n            config_path,\n            nested_dict_value(pretrained_bart_infos, pretrained_name, \"config\"),\n        )\n\n        config_path = cached_path(config_path)\n        gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n        inst = cls(config_path, gradient_checkpointing)\n        pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n        weight_path = pop_value(\n            pretrained_weight_path,\n            nested_dict_value(pretrained_bart_infos, pretrained_name, \"weight\"),\n            check_none=False,\n        )\n        if weight_path is not None:\n            weight_path = cached_path(weight_path)\n            inst.from_pretrained(weight_path)\n\n        return inst\n\n    @add_default_section_for_function(\"core/model/generation/bart\")\n    def generate(\n        self,\n        input_ids: torch.Tensor,\n        num_beams: Optional[int] = 5,\n        decoder_start_token_id: Optional[int] = 2,\n        decoder_end_token_id: Optional[int] = 2,\n        num_return_sequences: Optional[int] = 1,\n        min_gen_seq_length: Optional[int] = 0,\n        max_gen_seq_length: Optional[int] = 48,\n        repetition_penalty: Optional[float] = 1.0,\n        no_repeat_ngram_size: Optional[int] = 0,\n        early_stopping: Optional[bool] = True,\n        length_penalty: Optional[float] = 1.0,\n        num_beam_groups: Optional[int] = 1,\n        diversity_penalty: Optional[float] = 0.0,\n        do_sample: Optional[bool] = False,\n        temperature: Optional[float] = 1.0,\n        top_k: Optional[int] = 50,\n        top_p: Optional[float] = 1.0,\n    ):\n        pass\n</code></pre> <p>\u00a0\u00a0\u00a0\u00a0The <code>from_core_configure</code> method is a class method used to create an instance of BartForGeneration based on a provided configuration object (config). It retrieves various options from the configuration and initializes the instance with the appropriate values. It also loads pretrained weights if a pretrained_weight_path is specified in the configuration. It also has <code>add_default_section_for_function</code> decorator to override the parameter value from coniguration object with specific section. The <code>num_beams</code> is set to 20 and <code>no_repeat_ngram_size</code> is set to 3 in this example.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>python version &gt;= 3.8</li> <li>torch &gt;= 1.13</li> <li>torchvision</li> <li>torchaudio</li> <li>fire</li> <li>configparser</li> <li>pandas &lt;= 1.5.3</li> <li>scikit-learn &gt;= 0.24.2</li> <li>diffusers &gt;= 0.16.1</li> <li>deepspeed &gt;= 0.9.0</li> <li>peft &gt;= 0.3.0</li> <li>datasets &gt;= 2.12.0</li> <li>transformers &gt;= 4.29.1</li> </ul>"},{"location":"installation/#install-pypi","title":"Install Pypi","text":"<pre><code>pip3 install unitorch\npip3 install unitorch[deepspeed]\n</code></pre>"},{"location":"installation/#install-source","title":"Install Source","text":"<pre><code>pip3 install \\\n    \"git+https://github.com/fuliucansheng/unitorch#egg=unitorch[deepspeed]\"\n</code></pre>"},{"location":"installation/#install-extension","title":"Install Extension","text":"<pre><code>UNITORCH_EXTENSIONS=NGRAM pip3 install \\\n    \"git+https://github.com/fuliucansheng/unitorch\"\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p> unitorch is a powerful package designed with a Foundation-Adapter architecture, providing a streamlined development process for unified models in various domains such as natural language understanding, natural language generation, computer vision, click-through rate prediction, multimodal learning, and reinforcement learning. The package is built on top of PyTorch, a popular deep learning framework, and seamlessly integrates with other well-known frameworks like transformers, peft, diffusers, and fastseq.</p> <p></p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0The architecture of unitorch consists of two main components: Foundation Modules, Adapter Modules and Command Line Interface (CLI). </p> <p>Foundation Modules: focus on implementing the core functionality of the package and provide the basic functions required by the models. These modules serve as the building blocks for different workflows and are designed to be modular, efficient, and flexible.</p> <p>Adapter Modules: act as adapters for the Foundation Modules, enabling them to support different workflows. Since different tasks or applications may have unique requirements, Adapter Modules provide the necessary interfaces and configurations to adapt the Foundation Modules to specific use cases. This modular approach allows for easy customization and extensibility of the package.</p> <p>Command Line Interface (CLI): defines the Running workflow to streamline the usage of unitorch. The CLI orchestrates the execution of the pipeline by calling the required Adapter Modules based on the pipeline design. This command line tool simplifies the process of training, evaluating, and deploying models, making it convenient for researchers and developers to experiment with different configurations and workflows.</p> <ul> <li><code>unitorch-train</code> command is used to train models using the unitorch package. It enables you to specify the training data, model architecture, hyperparameters, and other configuration options. By Running this command, the package will utilize the specified data and parameters to train the model and optimize its performance based on the defined objective.</li> <li><code>unitorch-infer</code> command is used for inference or prediction using trained models. Once a model has been trained using unitorch-train, you can employ this command to make predictions or generate outputs for new or unseen data. It takes the trained model and the input data as inputs and produces the predicted results using the learned patterns and knowledge captured during training.</li> <li><code>unitorch-eval</code> command is used to evaluate the performance of trained models. It allows you to assess the quality and effectiveness of the model by comparing its predictions against the ground truth or reference data. This command typically computes various metrics, such as accuracy, precision, recall, F1 score, or other domain-specific metrics, to provide insights into the model's performance.</li> <li><code>unitorch-script</code> command provides a convenient way to execute custom scripts or workflows using the unitorch package. It enables you to define and execute complex operations or experiments by writing scripts that leverage the functionalities of the unitorch library. This command allows for more flexibility and customization in using the package for specific research or development tasks.</li> <li><code>unitorch-service</code> command is used to deploy a service for data serving, including hosting an HTTP server, serving models, or exposing APIs. This command allows you to define the endpoints and routes for your service, configure the behavior and responses, and integrate the necessary functionalities from the unitorch package.</li> </ul> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0In addition to the CLI, unitorch also offers a simple import statement (import unitorch) that allows users to leverage the functionality of the package with just a single line of code. This import statement provides access to the state-of-the-art models and datasets supported by unitorch, without compromising performance or accuracy.</p> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Overall, unitorch empowers developers and researchers to build unified models across different domains quickly and efficiently. By leveraging the Foundation-Adapter architecture and integrating seamlessly with popular frameworks, unitorch simplifies the development process and accelerates the deployment of advanced models in various applications.</p>"},{"location":"cli/ast/","title":"unitorch.cli.datasets.hf","text":""},{"location":"cli/ast/#astdatasets","title":"ASTDatasets","text":"<p>Tip</p> <p><code>core/dataset/ast</code> is the section for configuration of ASTDatasets.</p> <p>Get the dataset for the specified split.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The split to get the dataset for.</p> required <p>Returns:</p> Name Type Description <code>dataset</code> <p>The dataset for the specified split.</p> Source code in <code>src/unitorch/cli/datasets/hf.py</code> <pre><code>def __getdataset__(self, split):\n    \"\"\"\n    Get the dataset for the specified split.\n\n    Args:\n        split (str): The split to get the dataset for.\n\n    Returns:\n        dataset: The dataset for the specified split.\n    \"\"\"\n    config = self.config\n\n    registered_process_mapping = {\n        k.replace(\"/\", \"_\"): k for k, v in registered_process.items()\n    }\n\n    config.set_default_section(f\"core/dataset/ast\")\n    _iterable = config.getoption(\"iterable\", False)\n    _template = config.getoption(\"template\", \"csv\")\n    _data_name = config.getoption(\"data_name\", None)\n    _config_name = config.getoption(\"config_name\", None)\n    _data_dir = config.getoption(\"data_dir\", None)\n    _data_files = config.getoption(\"data_files\", None)\n    _names = config.getoption(\"names\", None)\n    _features = config.getoption(\"features\", None)\n    _sep = config.getoption(\"sep\", \"\\t\")\n    _quoting = config.getoption(\"quoting\", 3)\n    _escapechar = config.getoption(\"escapechar\", None)\n    _field = config.getoption(\"field\", None)\n    _process_functions = config.getoption(\"preprocess_functions\", None)\n    _enable_ddp_partition = config.getoption(\"enable_ddp_partition\", True)\n\n    _HFDatasets = HFIterableDatasets if _iterable else HFDatasets\n    _ASTDatasets = ASTHFIterableDatasets if _iterable else ASTHFDatasets\n\n    config.set_default_section(f\"core/dataset/ast/{split}\")\n\n    template = config.getoption(\"template\", _template)\n    if config.getoption(\"data_name\", _data_name) is not None:\n        template = \"hub\"\n\n    assert template in self.templates\n\n    new_split = \"validation\" if split == \"dev\" else split\n    new_split = config.getoption(\"split\", new_split)\n\n    # get dataset\n    dataset = None\n    if template == \"csv\":\n        data_dir = config.getoption(\"data_dir\", _data_dir)\n        data_files = config.getoption(\"data_files\", _data_files)\n        names = config.getoption(\"names\", _names)\n        sep = config.getoption(\"sep\", _sep)\n        quoting = config.getoption(\"quoting\", _quoting)\n        escapechar = config.getoption(\"escapechar\", _escapechar)\n        dataset = _HFDatasets.from_csv(\n            data_dir=data_dir,\n            data_files=data_files,\n            names=names,\n            sep=sep,\n            quoting=quoting,\n            escapechar=escapechar,\n            split=new_split,\n        )\n\n    if template == \"json\":\n        data_dir = config.getoption(\"data_dir\", _data_dir)\n        data_files = config.getoption(\"data_files\", _data_files)\n        field = config.getoption(\"field\", _field)\n\n        dataset = _HFDatasets.from_json(\n            data_dir=data_dir,\n            data_files=data_files,\n            field=field,\n            split=new_split,\n        )\n\n    if template == \"parquet\":\n        data_dir = config.getoption(\"data_dir\", _data_dir)\n        data_files = config.getoption(\"data_files\", _data_files)\n        features = config.getoption(\"features\", _features)\n        if isinstance(features, str):\n            features = eval(features)\n        dataset = _HFDatasets.from_parquet(\n            data_dir=data_dir,\n            data_files=data_files,\n            split=new_split,\n            features=features,\n        )\n\n    if template == \"hub\":\n        data_name = config.getoption(\"data_name\", _data_name)\n        config_name = config.getoption(\"config_name\", _config_name)\n        data_dir = config.getoption(\"data_dir\", _data_dir)\n        data_files = config.getoption(\"data_files\", _data_files)\n        data_name = (\n            cached_path(data_name) if data_name.endswith(\".py\") else data_name\n        )\n        dataset = _HFDatasets.from_hub(\n            data_name=data_name,\n            config_name=config_name,\n            data_dir=data_dir,\n            data_files=data_files,\n            split=new_split,\n        )\n\n    assert dataset is not None\n\n    # get process functions\n    process_functions = config.getoption(\"preprocess_functions\", _process_functions)\n    if process_functions is None:\n        process_functions = []\n    else:\n        process_functions = [ASTFunction(func) for func in process_functions]\n\n    for pfunc in process_functions:\n        for name in pfunc.__ast_process__:\n            globals()[name] = init_registered_process(\n                registered_process_mapping[name],\n                config,\n            )\n\n    enable_ddp_partition = config.getoption(\n        \"enable_ddp_partition\", _enable_ddp_partition\n    )\n\n    if isinstance(_ASTDatasets, HFIterableDatasets):\n        self.__ASTDatasets__[split] = _ASTDatasets(\n            dataset=dataset.dataset,\n            process_functions=process_functions,\n            enable_ddp_partition=enable_ddp_partition,\n        )\n    else:\n        self.__ASTDatasets__[split] = _ASTDatasets(\n            dataset=dataset.dataset,\n            process_functions=process_functions,\n        )\n\n    return self.__ASTDatasets__.get(split)\n</code></pre> <p>Tip</p> <p><code>core/dataset/ast/train</code> is the section for configuration of ASTDatasets Training Data.</p> <p>Tip</p> <p><code>core/dataset/ast/dev</code> is the section for configuration of ASTDatasets Validation Data.</p> <p>Tip</p> <p><code>core/dataset/ast/test</code> is the section for configuration of ASTDatasets Test Data.</p>"},{"location":"cli/csv/","title":"unitorch.cli.writer","text":""},{"location":"cli/csv/#generalcsvwriter","title":"GeneralCsvWriter","text":"<p>Tip</p> <p><code>core/writer/csv</code> is the section for configuration of GeneralCsvWriter.</p> <p>             Bases: <code>GenericWriter</code></p> <p>Class for writing data in CSV format.</p> <p>Initialize GeneralCsvWriter.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>The path to the output file.</p> required <code>nrows_per_sample</code> <code>int</code> <p>The number of rows per sample. Defaults to None.</p> <code>None</code> <code>header</code> <code>bool</code> <p>Whether to include a header in the output file. Defaults to None.</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>The list of columns to include in the output file. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The separator for the CSV file. Defaults to \"  \".</p> <code>'\\t'</code> <code>quoting</code> <code>int</code> <p>The quoting style for the CSV file. Defaults to 3.</p> <code>3</code> <code>escapechar</code> <code>str</code> <p>The escape character for the CSV file. Defaults to None.</p> <code>None</code> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def __init__(\n    self,\n    output_file: str,\n    nrows_per_sample: Optional[int] = None,\n    header: Optional[bool] = None,\n    columns: Optional[List[str]] = None,\n    sep: Optional[str] = \"\\t\",\n    quoting: Optional[int] = 3,\n    escapechar: Optional[str] = None,\n):\n    \"\"\"\n    Initialize GeneralCsvWriter.\n\n    Args:\n        output_file (str): The path to the output file.\n        nrows_per_sample (int, optional): The number of rows per sample. Defaults to None.\n        header (bool, optional): Whether to include a header in the output file. Defaults to None.\n        columns (List[str], optional): The list of columns to include in the output file. Defaults to None.\n        sep (str, optional): The separator for the CSV file. Defaults to \"\\t\".\n        quoting (int, optional): The quoting style for the CSV file. Defaults to 3.\n        escapechar (str, optional): The escape character for the CSV file. Defaults to None.\n    \"\"\"\n    self.header = header\n    self.columns = columns\n    self.sep = sep\n    self.quoting = quoting\n    self.escapechar = escapechar\n    has_header = int(header is True)\n    self.skip_n_samples = (\n        0\n        if nrows_per_sample is None or not os.path.exists(output_file)\n        else (sum(1 for _ in open(output_file)) - has_header) // nrows_per_sample\n    )\n    if self.skip_n_samples == 0:\n        self.output_file = open(output_file, \"w\", encoding=\"utf-8\")\n    else:\n        self.output_file = open(output_file, \"a\", encoding=\"utf-8\")\n</code></pre>"},{"location":"cli/csv/#unitorch.cli.writer.GeneralCsvWriter.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of GeneralCsvWriter from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GeneralCsvWriter</code> <p>An instance of GeneralCsvWriter.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/writer/csv\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of GeneralCsvWriter from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        GeneralCsvWriter: An instance of GeneralCsvWriter.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"cli/csv/#unitorch.cli.writer.GeneralCsvWriter.process_chunk","title":"process_chunk","text":"<pre><code>process_chunk(outputs: WriterOutputs)\n</code></pre> <p>Process a chunk of data during the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_chunk(self, outputs: WriterOutputs):\n    \"\"\"\n    Process a chunk of data during the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    string = dataframe.to_csv(\n        index=False,\n        sep=self.sep,\n        quoting=self.quoting,\n        header=False,\n        escapechar=self.escapechar,\n    )\n    self.output_file.write(string)\n    self.output_file.flush()\n</code></pre>"},{"location":"cli/csv/#unitorch.cli.writer.GeneralCsvWriter.process_end","title":"process_end","text":"<pre><code>process_end()\n</code></pre> <p>Process the end of the writing process.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_end(self):\n    \"\"\"Process the end of the writing process.\"\"\"\n    self.output_file.close()\n</code></pre>"},{"location":"cli/csv/#unitorch.cli.writer.GeneralCsvWriter.process_start","title":"process_start","text":"<pre><code>process_start(outputs: WriterOutputs)\n</code></pre> <p>Process the start of the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_start(self, outputs: WriterOutputs):\n    \"\"\"\n    Process the start of the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    string = dataframe.to_csv(\n        index=False,\n        sep=self.sep,\n        quoting=self.quoting,\n        header=self.header and self.skip_n_samples == 0,\n        escapechar=self.escapechar,\n    )\n    self.output_file.write(string)\n    self.output_file.flush()\n</code></pre>"},{"location":"cli/datatypes/","title":"unitorch.cli.models","text":""},{"location":"cli/datatypes/#classificationoutputs","title":"ClassificationOutputs","text":"<p>             Bases: <code>TensorsOutputs</code>, <code>WriterMixin</code></p> <p>Outputs for classification models.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Tensor</code> <p>Output tensor containing the classification results.</p> required"},{"location":"cli/datatypes/#classificationtargets","title":"ClassificationTargets","text":"<p>             Bases: <code>TensorsTargets</code></p> <p>Targets for classification models.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>Tensor</code> <p>The target tensor.</p> required <code>sample_weight</code> <code>Optional[Tensor]</code> <p>The weight associated with each target. Defaults to a tensor with a value of 1.0.</p> <code>tensor(1.0)</code>"},{"location":"cli/datatypes/#embeddingoutputs","title":"EmbeddingOutputs","text":"<p>             Bases: <code>TensorsOutputs</code>, <code>WriterMixin</code></p> <p>Outputs for embedding models.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>Tensor</code> <p>The embedding tensor.</p> required <code>embedding1</code> <code>Optional[Tensor]</code> <p>Additional embedding tensor 1. Defaults to an empty tensor.</p> <code>empty(0)</code> <code>embedding2</code> <code>Optional[Tensor]</code> <p>Additional embedding tensor 2. Defaults to an empty tensor.</p> <code>empty(0)</code> <code>embedding3</code> <code>Optional[Tensor]</code> <p>Additional embedding tensor 3. Defaults to an empty tensor.</p> <code>empty(0)</code> <code>embedding4</code> <code>Optional[Tensor]</code> <p>Additional embedding tensor 4. Defaults to an empty tensor.</p> <code>empty(0)</code>"},{"location":"cli/datatypes/#generationoutputs","title":"GenerationOutputs","text":"<p>             Bases: <code>TensorsOutputs</code>, <code>WriterMixin</code></p> <p>Outputs for generation models.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Tensor</code> <p>Generated sequences.</p> required <code>sequences_scores</code> <code>Optional[Tensor]</code> <p>Scores associated with the generated sequences. Defaults to an empty tensor.</p> <code>empty(0)</code>"},{"location":"cli/datatypes/#generationtargets","title":"GenerationTargets","text":"<p>             Bases: <code>TensorsTargets</code></p> <p>Targets for generation models.</p> <p>Parameters:</p> Name Type Description Default <code>refs</code> <code>Tensor</code> <p>Reference sequences.</p> required <code>masks</code> <code>Optional[Tensor]</code> <p>Mask indicating the valid positions in the reference sequences. Defaults to an empty tensor.</p> <code>empty(0)</code> <code>sample_weight</code> <code>Optional[Tensor]</code> <p>Sample weights for the reference sequences. Defaults to an empty tensor.</p> <code>empty(0)</code>"},{"location":"cli/datatypes/#lossoutputs","title":"LossOutputs","text":"<p>             Bases: <code>TensorsOutputs</code></p>"},{"location":"cli/deepspeed/","title":"unitorch.cli.tasks.deepspeed","text":""},{"location":"cli/deepspeed/#deepspeedtask","title":"DeepspeedTask","text":"<p>Tip</p> <p><code>core/task/deepspeed/supervised</code> is the section for configuration of DeepspeedTask.</p> <p>Task class for deepspeed supervised learning.</p> <p>Initialize the DeepspeedTask.</p> <p>Parameters:</p> Name Type Description Default <code>configure</code> <p>The configuration object.</p> required <code>model</code> <p>The model for supervised learning.</p> required <code>datasets</code> <p>The datasets for training and evaluation.</p> required <code>local_rank</code> <code>optional</code> <p>The local rank for distributed training. Defaults to -1.</p> <code>-1</code> <code>seed</code> <code>optional</code> <p>The random seed. Defaults to 1123.</p> <code>1123</code> Source code in <code>src/unitorch/cli/tasks/deepspeed.py</code> <pre><code>def __init__(\n    self,\n    configure,\n    model,\n    datasets,\n    local_rank: Optional[int] = -1,\n    seed: Optional[int] = 1123,\n):\n    \"\"\"\n    Initialize the DeepspeedTask.\n\n    Args:\n        configure: The configuration object.\n        model: The model for supervised learning.\n        datasets: The datasets for training and evaluation.\n        local_rank (optional): The local rank for distributed training. Defaults to -1.\n        seed (optional): The random seed. Defaults to 1123.\n    \"\"\"\n    set_seed(seed)\n    self.n_gpu = 1 if torch.cuda.is_available() else 0\n    if dist.is_initialized():\n        self.n_gpu = dist.get_world_size()\n\n    self.config = configure\n    self.model = model\n    self.datasets = datasets\n    self.local_rank = local_rank\n\n    if self.local_rank != -1:\n        torch.cuda.set_device(self.local_rank)\n\n    if torch.cuda.is_available():\n        self.model = self.model.cuda()\n\n    self.best_score = -np.inf\n</code></pre>"},{"location":"cli/deepspeed/#unitorch.cli.tasks.deepspeed.DeepspeedTask.eval","title":"eval","text":"<pre><code>eval(\n    monitor_fns: Union[str, List[str]],\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Evaluate the model.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_fns</code> <code>Union[str, List[str]]</code> <p>The monitoring functions for evaluation.</p> required <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>dev_batch_size</code> <code>optional</code> <p>The batch size for evaluation. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/deepspeed.py</code> <pre><code>@torch.no_grad()\n@add_default_section_for_function(\"core/task/deepspeed/supervised\")\ndef eval(\n    self,\n    monitor_fns: Union[str, List[str]],\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Evaluate the model.\n\n    Args:\n        monitor_fns: The monitoring functions for evaluation.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        dev_batch_size (optional): The batch size for evaluation. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    monitor_fns = [\n        init_registered_module(monitor_fn, self.config, registered_score)\n        for monitor_fn in monitor_fns\n        if monitor_fn in registered_score\n    ]\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n\n    global_rank = -1\n    if self.n_gpu &gt; 1:\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank],\n            output_device=self.local_rank,\n            find_unused_parameters=False,\n            broadcast_buffers=False,\n        )\n        global_rank = dist.get_rank()\n\n    dev_sampler = DistributedSampler if self.n_gpu &gt; 1 else SequentialSampler\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_dev = self.datasets.get(\"dev\")\n    else:\n        dataset_dev = self.datasets.get(\"dev\")\n    iter_dev = DataLoader(\n        dataset_dev,\n        sampler=dev_sampler(dataset_dev)\n        if not isinstance(dataset_dev, Iterable)\n        else None,\n        batch_size=dev_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    results = infer(self.model.module if self.n_gpu &gt; 1 else self.model, iter_dev)\n    if self.local_rank in [-1, 0]:\n        monitor(\n            outputs=results.outputs,\n            targets=results.targets,\n            monitor_fns=monitor_fns,\n        )\n</code></pre>"},{"location":"cli/deepspeed/#unitorch.cli.tasks.deepspeed.DeepspeedTask.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create a DeepspeedTask instance from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A DeepspeedTask instance.</p> Source code in <code>src/unitorch/cli/tasks/deepspeed.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/task/deepspeed/supervised\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create a DeepspeedTask instance from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A DeepspeedTask instance.\n    \"\"\"\n    try:\n        deepspeed.init_distributed(dist_backend=\"nccl\", init_method=\"env://\")\n    except:\n        logging.info(\"PyTorch is not in distributed mode\")\n\n    config.set_default_section(\"core/task/deepspeed/supervised\")\n\n    model = config.getoption(\"model\", None)\n    dataset = config.getoption(\"dataset\", None)\n\n    if model is not None:\n        model = init_registered_module(model, config, registered_model)\n\n    if dataset is not None:\n        dataset = init_registered_module(dataset, config, registered_dataset)\n\n    local_rank = config.getdefault(\n        \"core/cli\",\n        \"local_rank\",\n        get_local_rank(),\n    )\n\n    return dict(\n        configure=config,\n        model=model,\n        datasets=dataset,\n        local_rank=local_rank,\n    )\n</code></pre>"},{"location":"cli/deepspeed/#unitorch.cli.tasks.deepspeed.DeepspeedTask.infer","title":"infer","text":"<pre><code>infer(\n    postprocess_fn: str,\n    writer: str,\n    test_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    max_size: Optional[int] = 10000,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    output_header: Optional[List] = None,\n    output_path: Optional[str] = \"./cache/predict.txt\",\n    postprocess_workers: Optional[int] = 2,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Perform inference using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>postprocess_fn</code> <code>str</code> <p>The post-processing function for inference.</p> required <code>writer</code> <code>str</code> <p>The writer for writing the results.</p> required <code>test_batch_size</code> <code>optional</code> <p>The batch size for inference. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>max_size</code> <code>optional</code> <p>The maximum size of the dataset for inference. Defaults to 10000.</p> <code>10000</code> <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>output_header</code> <code>optional</code> <p>The header for the output file. Defaults to None.</p> <code>None</code> <code>output_path</code> <code>optional</code> <p>The path to save the output file. Defaults to \"./cache/predict.txt\".</p> <code>'./cache/predict.txt'</code> <code>postprocess_workers</code> <code>optional</code> <p>The number of worker processes for post-processing. Defaults to 2.</p> <code>2</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/deepspeed.py</code> <pre><code>@torch.no_grad()\n@add_default_section_for_function(\"core/task/deepspeed/supervised\")\ndef infer(\n    self,\n    postprocess_fn: str,\n    writer: str,\n    test_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    max_size: Optional[int] = 10000,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    output_header: Optional[List] = None,\n    output_path: Optional[str] = \"./cache/predict.txt\",\n    postprocess_workers: Optional[int] = 2,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Perform inference using the trained model.\n\n    Args:\n        postprocess_fn: The post-processing function for inference.\n        writer: The writer for writing the results.\n        test_batch_size (optional): The batch size for inference. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        max_size (optional): The maximum size of the dataset for inference. Defaults to 10000.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        output_header (optional): The header for the output file. Defaults to None.\n        output_path (optional): The path to save the output file. Defaults to \"./cache/predict.txt\".\n        postprocess_workers (optional): The number of worker processes for post-processing. Defaults to 2.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    assert self.n_gpu &lt;= 1\n    assert writer is not None\n\n    output_dir = os.path.dirname(output_path)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    if postprocess_fn is not None:\n        postprocess_fn = init_registered_process(postprocess_fn, self.config)\n\n    if writer is not None:\n        writer = init_registered_module(\n            writer,\n            self.config,\n            registered_writer,\n            output_file=output_path,\n        )\n\n    skip_step = writer.skip_n_samples\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n\n    if skip_step == 0:\n        sampler = SequentialSampler\n    else:\n        sampler = SequentialSkipSampler\n\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_test = self.datasets.get(\"test\")\n    else:\n        dataset_test = self.datasets.get(\"test\")\n\n    iter_test = DataLoader(\n        dataset_test,\n        sampler=sampler(dataset_test)\n        if not isinstance(dataset_test, Iterable)\n        else None,\n        batch_size=test_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    if skip_step &gt; 0:\n        output_file = open(output_path, \"a\")\n    else:\n        output_file = open(output_path, \"w\")\n\n    if skip_step &gt; 0 and hasattr(dataset_test, \"set_skip_step\"):\n        dataset_test.set_skip_step(skip_step)\n\n    if skip_step &gt; 0 and hasattr(iter_test.sampler, \"set_skip_step\"):\n        iter_test.sampler.set_skip_step(skip_step)\n\n    if hasattr(dataset_test, \"dataset\"):\n        data_info = dataset_test.dataset\n        data_info = DatasetFeature(data_info)\n        iter_data = DataLoader(\n            deepcopy(data_info),\n            sampler=sampler(data_info)\n            if not isinstance(dataset_test, Iterable)\n            else None,\n            batch_size=test_batch_size,\n            shuffle=False,\n            pin_memory=pin_memory,\n            num_workers=num_workers,\n            collate_fn=None,\n        )\n    else:\n        iter_data = None\n\n    if skip_step &gt; 0 and hasattr(iter_data.sampler, \"set_skip_step\"):\n        iter_data.sampler.set_skip_step(skip_step)\n\n    self.model.eval()\n    start = time.time()\n\n    data_queue = Queue(maxsize=max_size)\n    msg_queue = Queue(maxsize=max_size)\n    postprocess_list = []\n    for _ in range(postprocess_workers):\n        p = PostProcess(\n            postprocess_fn,\n            data_queue,\n            msg_queue,\n        )\n        postprocess_list.append(p)\n        p.start()\n\n    io_process = IOProcess(msg_queue, writer=writer)\n    io_process.start()\n\n    if iter_data is None:\n        for step, (inputs, _) in enumerate(iter_test):\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n            outputs = self.model(**inputs.dict())\n            outputs = outputs.cpu()\n            data_queue.put((step, outputs))\n    else:\n        for step, ((inputs, _), _infos) in enumerate(zip(iter_test, iter_data)):\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n            outputs = self.model(**inputs.dict())\n            outputs = outputs.cpu()\n            if output_header is not None:\n                _infos = {k: _infos[k] for k in output_header if k in _infos}\n                outputs.from_pandas(pd.DataFrame(_infos))\n            data_queue.put((step, outputs))\n\n    data_queue.put((-1, GENERATE_FINISHED))\n    for p in postprocess_list:\n        p.join()\n\n    msg_queue.put((-1, GENERATE_FINISHED))\n    io_process.join()\n\n    end = time.time()\n    ms = (end - start) * 1000\n    logging.info(\n        \"{:.2f} ms, {:.2f} sample/s\".format(\n            ms,\n            ((len(dataset_test) - skip_step) / ms * 1000),\n        )\n    )\n</code></pre>"},{"location":"cli/deepspeed/#unitorch.cli.tasks.deepspeed.DeepspeedTask.train","title":"train","text":"<pre><code>train(\n    config_path: str,\n    optim: str,\n    loss_fn: str,\n    score_fn: str,\n    monitor_fns: Optional[Union[str, List[str]]] = None,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    to_ckpt_dir: Optional[str] = \"./to_ckpt\",\n    train_batch_size: Optional[int] = 128,\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    save_optimizer: Optional[bool] = False,\n    save_scheduler: Optional[bool] = False,\n    log_freq: Optional[int] = 100,\n    ckpt_freq: Optional[int] = 10000,\n    grad_acc_step: Optional[int] = 1,\n    max_grad_norm: Optional[float] = 1.0,\n    learning_rate: Optional[float] = None,\n    max_warmup_learning_rate: Optional[float] = None,\n    num_warmup_steps: Optional[int] = None,\n    epochs: Optional[int] = 5,\n    use_amp: Optional[bool] = False,\n    use_ema: Optional[bool] = False,\n    ema_decay: Optional[float] = 0.9999,\n    ema_tau: Optional[int] = 2000,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Train the model using deepspeed.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the deepspeed configuration file.</p> required <code>optim</code> <code>str</code> <p>The optimizer used for training.</p> required <code>loss_fn</code> <code>str</code> <p>The loss function used for training.</p> required <code>score_fn</code> <code>str</code> <p>The score function used for evaluation.</p> required <code>monitor_fns</code> <code>optional</code> <p>The monitoring functions for evaluation. Defaults to None.</p> <code>None</code> <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>to_ckpt_dir</code> <code>optional</code> <p>The directory path to save checkpoints to. Defaults to \"./to_ckpt\".</p> <code>'./to_ckpt'</code> <code>train_batch_size</code> <code>optional</code> <p>The batch size for training. Defaults to 128.</p> <code>128</code> <code>dev_batch_size</code> <code>optional</code> <p>The batch size for evaluation. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>save_optimizer</code> <code>optional</code> <p>Whether to save the optimizer state. Defaults to False.</p> <code>False</code> <code>save_scheduler</code> <code>optional</code> <p>Whether to save the scheduler state. Defaults to False.</p> <code>False</code> <code>log_freq</code> <code>optional</code> <p>The frequency of logging. Defaults to 100.</p> <code>100</code> <code>ckpt_freq</code> <code>optional</code> <p>The frequency of saving checkpoints. Defaults to 10000.</p> <code>10000</code> <code>grad_acc_step</code> <code>optional</code> <p>The number of gradient accumulation steps. Defaults to 1.</p> <code>1</code> <code>max_grad_norm</code> <code>optional</code> <p>The maximum gradient norm. Defaults to 1.0.</p> <code>1.0</code> <code>learning_rate</code> <code>optional</code> <p>The learning rate for the optimizer. Defaults to None.</p> <code>None</code> <code>max_warmup_learning_rate</code> <code>optional</code> <p>The maximum learning rate during warmup. Defaults to None.</p> <code>None</code> <code>num_warmup_steps</code> <code>optional</code> <p>The number of warmup steps. Defaults to None.</p> <code>None</code> <code>epochs</code> <code>optional</code> <p>The number of training epochs. Defaults to 5.</p> <code>5</code> <code>use_ema</code> <code>optional</code> <p>Whether to use exponential moving average. Defaults to False.</p> <code>False</code> <code>ema_decay</code> <code>optional</code> <p>The decay factor for exponential moving average. Defaults to 0.9999.</p> <code>0.9999</code> <code>ema_tau</code> <code>optional</code> <p>The time constant for exponential moving average. Defaults to 2000.</p> <code>2000</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/deepspeed.py</code> <pre><code>@add_default_section_for_function(\"core/task/deepspeed/supervised\")\ndef train(\n    self,\n    config_path: str,\n    optim: str,\n    loss_fn: str,\n    score_fn: str,\n    monitor_fns: Optional[Union[str, List[str]]] = None,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    to_ckpt_dir: Optional[str] = \"./to_ckpt\",\n    train_batch_size: Optional[int] = 128,\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    save_optimizer: Optional[bool] = False,\n    save_scheduler: Optional[bool] = False,\n    log_freq: Optional[int] = 100,\n    ckpt_freq: Optional[int] = 10000,\n    grad_acc_step: Optional[int] = 1,\n    max_grad_norm: Optional[float] = 1.0,\n    learning_rate: Optional[float] = None,\n    max_warmup_learning_rate: Optional[float] = None,\n    num_warmup_steps: Optional[int] = None,\n    epochs: Optional[int] = 5,\n    use_amp: Optional[bool] = False,\n    use_ema: Optional[bool] = False,\n    ema_decay: Optional[float] = 0.9999,\n    ema_tau: Optional[int] = 2000,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Train the model using deepspeed.\n\n    Args:\n        config_path: The path to the deepspeed configuration file.\n        optim: The optimizer used for training.\n        loss_fn: The loss function used for training.\n        score_fn: The score function used for evaluation.\n        monitor_fns (optional): The monitoring functions for evaluation. Defaults to None.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        to_ckpt_dir (optional): The directory path to save checkpoints to. Defaults to \"./to_ckpt\".\n        train_batch_size (optional): The batch size for training. Defaults to 128.\n        dev_batch_size (optional): The batch size for evaluation. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        save_optimizer (optional): Whether to save the optimizer state. Defaults to False.\n        save_scheduler (optional): Whether to save the scheduler state. Defaults to False.\n        log_freq (optional): The frequency of logging. Defaults to 100.\n        ckpt_freq (optional): The frequency of saving checkpoints. Defaults to 10000.\n        grad_acc_step (optional): The number of gradient accumulation steps. Defaults to 1.\n        max_grad_norm (optional): The maximum gradient norm. Defaults to 1.0.\n        learning_rate (optional): The learning rate for the optimizer. Defaults to None.\n        max_warmup_learning_rate (optional): The maximum learning rate during warmup. Defaults to None.\n        num_warmup_steps (optional): The number of warmup steps. Defaults to None.\n        epochs (optional): The number of training epochs. Defaults to 5.\n        use_ema (optional): Whether to use exponential moving average. Defaults to False.\n        ema_decay (optional): The decay factor for exponential moving average. Defaults to 0.9999.\n        ema_tau (optional): The time constant for exponential moving average. Defaults to 2000.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    if not os.path.exists(to_ckpt_dir) and self.local_rank in [-1, 0]:\n        os.makedirs(to_ckpt_dir, exist_ok=True)\n\n    if loss_fn is not None:\n        loss_fn = init_registered_module(loss_fn, self.config, registered_loss)\n\n    if score_fn is not None:\n        score_fn = init_registered_module(score_fn, self.config, registered_score)\n\n    if monitor_fns is not None:\n        monitor_fns = [\n            init_registered_module(monitor_fn, self.config, registered_score)\n            for monitor_fn in monitor_fns\n            if monitor_fn in registered_score\n        ]\n\n    config_file = cached_path(config_path)\n    config_dict = json.load(open(config_file, \"r\"))\n    config_dict[\"train_micro_batch_size_per_gpu\"] = train_batch_size\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n\n    if os.path.exists(to_ckpt_dir):\n        self.model.from_checkpoint(\n            to_ckpt_dir,\n            weight_name=\"pytorch_model_latest.bin\",\n        )\n\n    params = self.model.parameters()\n    params = filter(lambda x: x.requires_grad, params)\n\n    assert \"optimizer\" in config_dict\n\n    if \"params\" not in config_dict[\"optimizer\"]:\n        config_dict[\"optimizer\"][\"params\"] = dict()\n\n    if \"scheduler\" in config_dict:\n        if \"params\" not in config_dict[\"scheduler\"]:\n            config_dict[\"scheduler\"][\"params\"] = dict()\n\n    if learning_rate is not None:\n        config_dict[\"optimizer\"][\"params\"][\"lr\"] = learning_rate\n        if \"scheduler\" in config_dict:\n            config_dict[\"scheduler\"][\"params\"][\"warmup_max_lr\"] = learning_rate\n\n    if max_warmup_learning_rate is not None and \"scheduler\" in config_dict:\n        config_dict[\"scheduler\"][\"params\"][\n            \"warmup_max_lr\"\n        ] = max_warmup_learning_rate\n\n    if num_warmup_steps is not None and \"scheduler\" in config_dict:\n        if \"params\" not in config_dict[\"scheduler\"]:\n            config_dict[\"scheduler\"][\"params\"] = dict()\n        config_dict[\"scheduler\"][\"params\"][\"warmup_num_steps\"] = num_warmup_steps\n\n    info_path = os.path.join(to_ckpt_dir, \"info.json\")\n    if os.path.exists(info_path):\n        info = json.load(open(os.path.join(to_ckpt_dir, \"info.json\")))\n    else:\n        info = dict()\n\n    global_epoch = info.get(\"global_epoch\", 0)\n    global_step = info.get(\"global_step\", 0)\n    self.best_score = info.get(\"best_score\", self.best_score)\n\n    logging.info(f\"the best score is {self.best_score}\")\n\n    self.ema_model = None\n    if use_ema:\n        num_ema_steps = info.get(\"num_ema_steps\", 0)\n        self.ema_model = ExponentialMovingAverage(\n            self.model,\n            decay=ema_decay,\n            tau=ema_tau,\n            num_steps=num_ema_steps,\n        )\n        if os.path.exists(from_ckpt_dir):\n            self.ema_model.from_checkpoint(\n                from_ckpt_dir,\n                weight_name=\"pytorch_ema_model.bin\",\n            )\n        if os.path.exists(to_ckpt_dir):\n            self.ema_model.from_checkpoint(\n                to_ckpt_dir,\n                weight_name=\"pytorch_ema_model_latest.bin\",\n            )\n\n    for n, p in self.model.named_parameters():\n        logging.debug(\n            f\"{n}: trainable - {p.requires_grad} | tensor shape - {p.shape}\"\n        )\n\n    self.model, optim, _, scheduler = deepspeed.initialize(\n        model=self.model,\n        config=config_dict,\n        model_parameters=params,\n    )\n\n    global_rank = -1\n    if self.n_gpu &gt; 1:\n        global_rank = dist.get_rank()\n\n    train_sampler = DistributedSkipSampler if self.n_gpu &gt; 1 else RandomSkipSampler\n    dev_sampler = DistributedSampler if self.n_gpu &gt; 1 else SequentialSampler\n\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_train = self.datasets.get(\"train\")\n            dataset_dev = self.datasets.get(\"dev\")\n    else:\n        dataset_train = self.datasets.get(\"train\")\n        dataset_dev = self.datasets.get(\"dev\")\n\n    iter_train = DataLoader(\n        dataset_train,\n        sampler=train_sampler(dataset_train)\n        if not isinstance(dataset_train, Iterable)\n        else None,\n        batch_size=train_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    iter_dev = DataLoader(\n        dataset_dev,\n        sampler=dev_sampler(dataset_dev)\n        if not isinstance(dataset_dev, Iterable)\n        else None,\n        batch_size=dev_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    log_loss = 0\n    dev_epoch = 0\n    for e in range(0, epochs):\n        torch.cuda.empty_cache()\n        if e &lt; global_epoch:\n            continue\n\n        if hasattr(dataset_train, \"set_epoch\"):\n            dataset_train.set_epoch(e)\n\n        if hasattr(dataset_train, \"set_skip_step\"):\n            dataset_train.set_skip_step(global_step * train_batch_size)\n\n        if hasattr(iter_train.sampler, \"set_epoch\"):\n            iter_train.sampler.set_epoch(e)\n\n        if hasattr(iter_train.sampler, \"set_skip_step\"):\n            iter_train.sampler.set_skip_step(global_step * train_batch_size)\n\n        self.model.train()\n        is_update_step = True\n        for step, (inputs, targets) in enumerate(iter_train):\n            step = step + global_step\n            is_update_step = False\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n\n            if is_torch2_available():\n                with torch.cuda.amp.autocast(\n                    enabled=use_amp\n                ) as autocast, torch.backends.cuda.sdp_kernel(\n                    enable_flash=False\n                ) as disable:\n                    outputs = self.model(**inputs.dict())\n                    if isinstance(outputs, LossOutputs):\n                        loss = outputs.loss / grad_acc_step\n                    else:\n                        loss = (\n                            loss_fn(outputs=outputs, targets=targets)\n                            / grad_acc_step\n                        )\n            else:\n                with torch.cuda.amp.autocast(enabled=use_amp) as autocast:\n                    outputs = self.model(**inputs.dict())\n                    if isinstance(outputs, LossOutputs):\n                        loss = outputs.loss / grad_acc_step\n                    else:\n                        loss = (\n                            loss_fn(outputs=outputs, targets=targets)\n                            / grad_acc_step\n                        )\n\n            nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n            self.model.backward(loss)\n\n            log_loss += loss.data * grad_acc_step\n            if (step + 1) % grad_acc_step == 0:\n                is_update_step = True\n                optim.step()\n                if scheduler is not None:\n                    scheduler.step()\n                optim.zero_grad()\n\n                if use_ema and self.ema_model is not None:\n                    self.ema_model.step(\n                        self.model.module if self.n_gpu &gt; 1 else self.model\n                    )\n\n            if (step + 1) % log_freq == 0 and global_rank in [-1, 0]:\n                logging.info(\n                    f\"epoch {e} step {step}: loss -- { log_loss / log_freq }\"\n                )\n                log_loss = 0\n\n            if (step + 1) % ckpt_freq == 0:\n                if hasattr(dataset_dev, \"set_epoch\"):\n                    dataset_dev.set_epoch(dev_epoch)\n\n                if hasattr(iter_dev.sampler, \"set_epoch\"):\n                    iter_dev.sampler.set_epoch(dev_epoch)\n\n                dev_epoch += 1\n                self.best_score = save_checkpoint(\n                    self.model.module if self.n_gpu &gt; 1 else self.model,\n                    to_ckpt_dir,\n                    iter_dev,\n                    score_fn,\n                    monitor_fns,\n                    optim=optim if save_optimizer else None,\n                    scheduler=scheduler if save_scheduler else None,\n                    ema_model=self.ema_model if use_ema else None,\n                    best_score=self.best_score,\n                    info_path=info_path,\n                    local_rank=self.local_rank,\n                    global_epoch=e,\n                    global_step=step + 1,\n                )\n\n        if not is_update_step:\n            optim.step()\n            if scheduler is not None:\n                scheduler.step()\n            optim.zero_grad()\n\n            if use_ema and self.ema_model is not None:\n                self.ema_model.step(\n                    self.model.module if self.n_gpu &gt; 1 else self.model\n                )\n\n        log_loss = 0\n\n        if hasattr(dataset_dev, \"set_epoch\"):\n            dataset_dev.set_epoch(dev_epoch)\n\n        if hasattr(iter_dev.sampler, \"set_epoch\"):\n            iter_dev.sampler.set_epoch(dev_epoch)\n\n        dev_epoch += 1\n\n        global_step = 0\n        self.best_score = save_checkpoint(\n            self.model.module if self.n_gpu &gt; 1 else self.model,\n            to_ckpt_dir,\n            iter_dev,\n            score_fn,\n            monitor_fns,\n            optim=optim if save_optimizer else None,\n            scheduler=scheduler if save_scheduler else None,\n            ema_model=self.ema_model if use_ema else None,\n            best_score=self.best_score,\n            info_path=info_path,\n            local_rank=self.local_rank,\n            global_epoch=e,\n            global_step=0,\n        )\n</code></pre>"},{"location":"cli/jsonl/","title":"unitorch.cli.writer","text":""},{"location":"cli/jsonl/#generaljsonlwriter","title":"GeneralJsonlWriter","text":"<p>Tip</p> <p><code>core/writer/jsonl</code> is the section for configuration of GeneralJsonlWriter.</p> <p>             Bases: <code>GenericWriter</code></p> <p>Class for writing data in JSONL format.</p> <p>Initialize GeneralJsonlWriter.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>The path to the output file.</p> required <code>nrows_per_sample</code> <code>int</code> <p>The number of rows per sample. Defaults to None.</p> <code>None</code> <code>header</code> <code>bool</code> <p>Whether to include a header in the output file. Defaults to None.</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>The list of columns to include in the output file. Defaults to None.</p> <code>None</code> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def __init__(\n    self,\n    output_file: str,\n    nrows_per_sample: Optional[int] = None,\n    header: Optional[bool] = None,\n    columns: Optional[List[str]] = None,\n):\n    \"\"\"\n    Initialize GeneralJsonlWriter.\n\n    Args:\n        output_file (str): The path to the output file.\n        nrows_per_sample (int, optional): The number of rows per sample. Defaults to None.\n        header (bool, optional): Whether to include a header in the output file. Defaults to None.\n        columns (List[str], optional): The list of columns to include in the output file. Defaults to None.\n    \"\"\"\n    self.header = header\n    self.columns = columns\n    self.skip_n_samples = (\n        0\n        if nrows_per_sample is None or not os.path.exists(output_file)\n        else sum(1 for _ in open(output_file)) // nrows_per_sample\n    )\n    if self.skip_n_samples == 0:\n        self.output_file = open(output_file, \"w\", encoding=\"utf-8\")\n    else:\n        self.output_file = open(output_file, \"a\", encoding=\"utf-8\")\n</code></pre>"},{"location":"cli/jsonl/#unitorch.cli.writer.GeneralJsonlWriter.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of GeneralJsonlWriter from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GeneralJsonlWriter</code> <p>An instance of GeneralJsonlWriter.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/writer/jsonl\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of GeneralJsonlWriter from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        GeneralJsonlWriter: An instance of GeneralJsonlWriter.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"cli/jsonl/#unitorch.cli.writer.GeneralJsonlWriter.process_chunk","title":"process_chunk","text":"<pre><code>process_chunk(outputs: WriterOutputs)\n</code></pre> <p>Process a chunk of data during the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_chunk(self, outputs: WriterOutputs):\n    \"\"\"\n    Process a chunk of data during the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    string = dataframe.to_json(orient=\"records\", lines=True)\n    self.output_file.write(string)\n    self.output_file.flush()\n</code></pre>"},{"location":"cli/jsonl/#unitorch.cli.writer.GeneralJsonlWriter.process_end","title":"process_end","text":"<pre><code>process_end()\n</code></pre> <p>Process the end of the writing process.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_end(self):\n    \"\"\"Process the end of the writing process.\"\"\"\n    self.output_file.close()\n</code></pre>"},{"location":"cli/jsonl/#unitorch.cli.writer.GeneralJsonlWriter.process_start","title":"process_start","text":"<pre><code>process_start(outputs: WriterOutputs)\n</code></pre> <p>Process the start of the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_start(self, outputs: WriterOutputs):\n    \"\"\"\n    Process the start of the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    string = dataframe.to_json(orient=\"records\", lines=True)\n    self.output_file.write(string)\n    self.output_file.flush()\n</code></pre>"},{"location":"cli/parquet/","title":"unitorch.cli.writer","text":""},{"location":"cli/parquet/#generalparquetwriter","title":"GeneralParquetWriter","text":"<p>Tip</p> <p><code>core/writer/parquet</code> is the section for configuration of GeneralParquetWriter.</p> <p>             Bases: <code>GenericWriter</code></p> <p>Initialize GeneralParquetWriter.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>str</code> <p>The path to the output file.</p> required <code>nrows_per_sample</code> <code>int</code> <p>The number of rows per sample. Defaults to None.</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>The list of columns to include in the output file. Defaults to None.</p> <code>None</code> <code>schema</code> <code>str</code> <p>The Parquet schema in string format. Defaults to None.</p> <code>None</code> <code>compression</code> <code>str</code> <p>The compression algorithm to use. Defaults to \"snappy\".</p> <code>'snappy'</code> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def __init__(\n    self,\n    output_file: str,\n    nrows_per_sample: Optional[int] = None,\n    columns: Optional[List[str]] = None,\n    schema: Optional[str] = None,\n    compression: Optional[str] = \"snappy\",\n):\n    \"\"\"\n    Initialize GeneralParquetWriter.\n\n    Args:\n        output_file (str): The path to the output file.\n        nrows_per_sample (int, optional): The number of rows per sample. Defaults to None.\n        columns (List[str], optional): The list of columns to include in the output file. Defaults to None.\n        schema (str, optional): The Parquet schema in string format. Defaults to None.\n        compression (str, optional): The compression algorithm to use. Defaults to \"snappy\".\n    \"\"\"\n    self.columns = columns\n    self.skip_n_samples = 0\n    self.output_file = output_file\n    self.pq_writer = None\n    self.pq_schema = None if schema is None else eval(schema)\n    self.compression = compression\n</code></pre>"},{"location":"cli/parquet/#unitorch.cli.writer.GeneralParquetWriter.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of GeneralParquetWriter from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>GeneralParquetWriter</code> <p>An instance of GeneralParquetWriter.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/writer/parquet\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of GeneralParquetWriter from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        GeneralParquetWriter: An instance of GeneralParquetWriter.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"cli/parquet/#unitorch.cli.writer.GeneralParquetWriter.process_chunk","title":"process_chunk","text":"<pre><code>process_chunk(outputs: WriterOutputs)\n</code></pre> <p>Process a chunk of data during the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_chunk(self, outputs: WriterOutputs):\n    \"\"\"\n    Process a chunk of data during the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    assert self.pq_writer is not None\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    pa_table = pa.Table.from_pandas(dataframe, schema=self.pq_schema)\n    self.pq_writer.write_table(pa_table)\n</code></pre>"},{"location":"cli/parquet/#unitorch.cli.writer.GeneralParquetWriter.process_end","title":"process_end","text":"<pre><code>process_end()\n</code></pre> <p>Process the end of the writing process.</p> Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_end(self):\n    \"\"\"Process the end of the writing process.\"\"\"\n    self.pq_writer.close()\n</code></pre>"},{"location":"cli/parquet/#unitorch.cli.writer.GeneralParquetWriter.process_start","title":"process_start","text":"<pre><code>process_start(outputs: WriterOutputs)\n</code></pre> <p>Process the start of the writing process.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>WriterOutputs</code> <p>The writer outputs.</p> required Source code in <code>src/unitorch/cli/writer/__init__.py</code> <pre><code>def process_start(self, outputs: WriterOutputs):\n    \"\"\"\n    Process the start of the writing process.\n\n    Args:\n        outputs (WriterOutputs): The writer outputs.\n    \"\"\"\n    dataframe = outputs.to_pandas()\n    if self.columns is not None:\n        columns = set(dataframe.columns)\n        dataframe = dataframe[[h for h in self.columns if h in columns]]\n    pa_table = pa.Table.from_pandas(dataframe)\n    if self.pq_schema is None:\n        self.pq_schema = pa_table.schema\n    pa_table = pa.Table.from_pandas(dataframe, schema=self.pq_schema)\n    self.pq_writer = pq.ParquetWriter(\n        self.output_file,\n        self.pq_schema,\n        version=\"1.0\",\n        use_dictionary=False,\n        flavor=\"spark\",\n        compression=self.compression,\n        use_compliant_nested_type=True,\n    )\n    self.pq_writer.write_table(pa_table)\n</code></pre>"},{"location":"cli/postprocess/","title":"unitorch.cli.models","text":""},{"location":"cli/postprocess/#classification","title":"Classification","text":"<p>Tip</p> <p><code>core/postprocess/classification/binary_score</code> in configuration file to use the postprocess function.</p> <p>Postprocess the classification outputs for binary classification with scores.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ClassificationOutputs</code> <p>Outputs from the classification model.</p> required <p>Returns:</p> Name Type Description <code>WriterOutputs</code> <p>Processed outputs with scores.</p> Source code in <code>src/unitorch/cli/models/classification_utils.py</code> <pre><code>@register_process(\"core/postprocess/classification/binary_score\")\ndef _binary_score(\n    self,\n    outputs: ClassificationOutputs,\n):\n    \"\"\"\n    Postprocess the classification outputs for binary classification with scores.\n\n    Args:\n        outputs (ClassificationOutputs): Outputs from the classification model.\n\n    Returns:\n        WriterOutputs: Processed outputs with scores.\n    \"\"\"\n    assert outputs.outputs.dim() == 2\n\n    results = outputs.to_pandas()\n    assert results.shape[0] == 0 or results.shape[0] == outputs.outputs.shape[0]\n\n    outputs = outputs.outputs.numpy()\n    if self.act_fn is not None:\n        outputs = self.act_fn(outputs)\n\n    if outputs.ndim == 2:\n        pscore = outputs[:, 1] if outputs.shape[-1] &gt; 1 else outputs[:, 0]\n        results[\"pscore\"] = pscore.tolist()\n    else:\n        results[\"pscore\"] = outputs.tolist()\n    return WriterOutputs(results)\n</code></pre> <p>Tip</p> <p><code>core/postprocess/classification/score</code> in configuration file to use the postprocess function.</p> <p>Postprocess the classification outputs for multi-classes classification with scores.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>ClassificationOutputs</code> <p>Outputs from the classification model.</p> required <p>Returns:</p> Name Type Description <code>WriterOutputs</code> <p>Processed outputs with scores and predicted classes.</p> Source code in <code>src/unitorch/cli/models/classification_utils.py</code> <pre><code>@register_process(\"core/postprocess/classification/score\")\ndef _classifier_score(\n    self,\n    outputs: ClassificationOutputs,\n):\n    \"\"\"\n    Postprocess the classification outputs for multi-classes classification with scores.\n\n    Args:\n        outputs (ClassificationOutputs): Outputs from the classification model.\n\n    Returns:\n        WriterOutputs: Processed outputs with scores and predicted classes.\n    \"\"\"\n    assert outputs.outputs.dim() == 2\n\n    results = outputs.to_pandas()\n    assert results.shape[0] == 0 or results.shape[0] == outputs.outputs.shape[0]\n    outputs = outputs.outputs.numpy()\n    if self.act_fn is not None:\n        outputs = self.act_fn(outputs)\n\n    results[\"pscore\"] = outputs.max(-1)\n    results[\"pclass\"] = outputs.argmax(-1)\n    if self.return_scores:\n        results[\"scores\"] = outputs.tolist()\n    return WriterOutputs(results)\n</code></pre> <p>Tip</p> <p><code>core/postprocess/classification/embedding</code> in configuration file to use the postprocess function.</p> <p>Postprocess the embedding outputs.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>EmbeddingOutputs</code> <p>Outputs from the embedding model.</p> required <p>Returns:</p> Name Type Description <code>WriterOutputs</code> <p>Processed outputs with embeddings.</p> Source code in <code>src/unitorch/cli/models/classification_utils.py</code> <pre><code>@register_process(\"core/postprocess/classification/embedding\")\ndef _embedding(\n    self,\n    outputs: EmbeddingOutputs,\n):\n    \"\"\"\n    Postprocess the embedding outputs.\n\n    Args:\n        outputs (EmbeddingOutputs): Outputs from the embedding model.\n\n    Returns:\n        WriterOutputs: Processed outputs with embeddings.\n    \"\"\"\n    results = outputs.to_pandas()\n    assert results.shape[0] == 0 or results.shape[0] == outputs.embedding.shape[0]\n\n    embedding = outputs.embedding.numpy()\n    if embedding.ndim &gt; 2:\n        embedding = embedding.reshape(embedding.size(0), -1)\n    results[\"embedding\"] = embedding.tolist()\n\n    embedding1 = outputs.embedding1.numpy()\n    if embedding1.size &gt; 0:\n        if embedding1.ndim &gt; 2:\n            embedding1 = embedding1.reshape(embedding1.size(0), -1)\n        results[\"embedding1\"] = embedding1.tolist()\n\n    embedding2 = outputs.embedding2.numpy()\n    if embedding2.size &gt; 0:\n        if embedding2.ndim &gt; 2:\n            embedding2 = embedding2.reshape(embedding2.size(0), -1)\n        results[\"embedding2\"] = embedding2.tolist()\n\n    embedding3 = outputs.embedding3.numpy()\n    if embedding3.size &gt; 0:\n        if embedding3.ndim &gt; 2:\n            embedding3 = embedding3.reshape(embedding3.size(0), -1)\n        results[\"embedding3\"] = embedding3.tolist()\n\n    embedding4 = outputs.embedding4.numpy()\n    if embedding4.size &gt; 0:\n        if embedding4.ndim &gt; 2:\n            embedding4 = embedding4.reshape(embedding4.size(0), -1)\n        results[\"embedding4\"] = embedding4.tolist()\n\n    return WriterOutputs(results)\n</code></pre> <p>Tip</p> <p><code>core/postprocess/classification/embedding/string</code> in configuration file to use the postprocess function.</p> <p>Postprocess the embedding outputs as string representations.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>EmbeddingOutputs</code> <p>Outputs from the embedding model.</p> required <p>Returns:</p> Name Type Description <code>WriterOutputs</code> <p>Processed outputs with string representations of embeddings.</p> Source code in <code>src/unitorch/cli/models/classification_utils.py</code> <pre><code>@register_process(\"core/postprocess/classification/embedding/string\")\ndef _embedding_string(\n    self,\n    outputs: EmbeddingOutputs,\n):\n    \"\"\"\n    Postprocess the embedding outputs as string representations.\n\n    Args:\n        outputs (EmbeddingOutputs): Outputs from the embedding model.\n\n    Returns:\n        WriterOutputs: Processed outputs with string representations of embeddings.\n    \"\"\"\n    results = outputs.to_pandas()\n    assert results.shape[0] == 0 or results.shape[0] == outputs.embedding.shape[0]\n\n    embedding = outputs.embedding.numpy()\n    if embedding.ndim &gt; 2:\n        embedding = embedding.reshape(embedding.size(0), -1)\n    results[\"embedding\"] = embedding.tolist()\n    results[\"embedding\"] = results[\"embedding\"].map(\n        lambda x: \" \".join([str(i) for i in x])\n    )\n\n    embedding1 = outputs.embedding1.numpy()\n    if embedding1.size &gt; 0:\n        if embedding1.ndim &gt; 2:\n            embedding1 = embedding1.reshape(embedding1.size(0), -1)\n        results[\"embedding1\"] = embedding1.tolist()\n        results[\"embedding1\"] = results[\"embedding1\"].map(\n            lambda x: \" \".join([str(i) for i in x])\n        )\n\n    embedding2 = outputs.embedding2.numpy()\n    if embedding2.size &gt; 0:\n        if embedding2.ndim &gt; 2:\n            embedding2 = embedding2.reshape(embedding2.size(0), -1)\n        results[\"embedding2\"] = embedding2.tolist()\n        results[\"embedding2\"] = results[\"embedding2\"].map(\n            lambda x: \" \".join([str(i) for i in x])\n        )\n\n    embedding3 = outputs.embedding3.numpy()\n    if embedding3.size &gt; 0:\n        if embedding3.ndim &gt; 2:\n            embedding3 = embedding3.reshape(embedding3.size(0), -1)\n        results[\"embedding3\"] = embedding3.tolist()\n        results[\"embedding3\"] = results[\"embedding3\"].map(\n            lambda x: \" \".join([str(i) for i in x])\n        )\n\n    embedding4 = outputs.embedding4.numpy()\n    if embedding4.size &gt; 0:\n        if embedding4.ndim &gt; 2:\n            embedding4 = embedding4.reshape(embedding4.size(0), -1)\n        results[\"embedding4\"] = embedding4.tolist()\n        results[\"embedding4\"] = results[\"embedding4\"].map(\n            lambda x: \" \".join([str(i) for i in x])\n        )\n\n    return WriterOutputs(results)\n</code></pre>"},{"location":"cli/preprocess/","title":"unitorch.cli.models","text":""},{"location":"cli/preprocess/#image","title":"Image","text":"<p>Processor for image-related operations.</p> <p>Initializes a new instance of the ImageProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>image_type</code> <code>Optional[str]</code> <p>The type of the image. Defaults to None.</p> <code>None</code> <code>image_size</code> <code>tuple</code> <p>The size of the image. Defaults to (256, 256).</p> <code>(256, 256)</code> <code>http_url</code> <code>Optional[str]</code> <p>The URL for fetching images. Defaults to None.</p> <code>None</code> Source code in <code>src/unitorch/cli/models/image_utils.py</code> <pre><code>def __init__(\n    self,\n    image_type: Optional[str] = None,\n    image_size: tuple = (256, 256),\n    http_url: Optional[str] = None,\n):\n    \"\"\"\n    Initializes a new instance of the ImageProcessor.\n\n    Args:\n        image_type (Optional[str]): The type of the image. Defaults to None.\n        image_size (tuple): The size of the image. Defaults to (256, 256).\n        http_url (Optional[str]): The URL for fetching images. Defaults to None.\n    \"\"\"\n    self.image_type = image_type\n    self.image_size = image_size\n    self.http_url = http_url\n</code></pre>"},{"location":"cli/preprocess/#unitorch.cli.models.image_utils.ImageProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Creates a new instance of the ImageProcessor using the configuration from the core.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>An instance of the ImageProcessor.</p> Source code in <code>src/unitorch/cli/models/image_utils.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/image\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Creates a new instance of the ImageProcessor using the configuration from the core.\n\n    Args:\n        config: The configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        An instance of the ImageProcessor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"cli/preprocess/#labels","title":"Labels","text":"<p>Processor for label-related operations.</p> <p>Initializes a new instance of the LabelProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes. Defaults to None.</p> <code>None</code> <code>sep</code> <code>Optional[str]</code> <p>The separator used for splitting text. Defaults to \",\".</p> <code>','</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>map_dict</code> <code>Optional[Dict]</code> <p>A dictionary for mapping labels. Defaults to an empty dictionary.</p> <code>dict()</code> Source code in <code>src/unitorch/cli/models/label_utils.py</code> <pre><code>def __init__(\n    self,\n    num_classes: Optional[int] = None,\n    sep: Optional[str] = \",\",\n    max_seq_length: Optional[int] = 128,\n    map_dict: Optional[Dict] = dict(),\n):\n    \"\"\"\n    Initializes a new instance of the LabelProcessor.\n\n    Args:\n        num_classes (Optional[int]): The number of classes. Defaults to None.\n        sep (Optional[str]): The separator used for splitting text. Defaults to \",\".\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to 128.\n        map_dict (Optional[Dict]): A dictionary for mapping labels. Defaults to an empty dictionary.\n    \"\"\"\n    self.num_classes = num_classes\n    self.sep = sep\n    self.map_dict = map_dict\n</code></pre>"},{"location":"cli/preprocess/#unitorch.cli.models.label_utils.LabelProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Creates a new instance of the LabelProcessor using the configuration from the core.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>An instance of the LabelProcessor.</p> Source code in <code>src/unitorch/cli/models/label_utils.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/label\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Creates a new instance of the LabelProcessor using the configuration from the core.\n\n    Args:\n        config: The configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        An instance of the LabelProcessor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"cli/supervised/","title":"unitorch.cli.tasks.supervised","text":""},{"location":"cli/supervised/#supervisedtask","title":"SupervisedTask","text":"<p>Tip</p> <p><code>core/task/supervised</code> is the section for configuration of SupervisedTask.</p> <p>Initialize the SupervisedTask.</p> <p>Parameters:</p> Name Type Description Default <code>configure</code> <p>The configuration object.</p> required <code>model</code> <p>The model for the task.</p> required <code>datasets</code> <p>The datasets for training and evaluation.</p> required <code>local_rank</code> <code>optional</code> <p>The local rank for distributed training. Defaults to -1.</p> <code>-1</code> <code>seed</code> <code>optional</code> <p>The random seed. Defaults to 1123.</p> <code>1123</code> Source code in <code>src/unitorch/cli/tasks/supervised.py</code> <pre><code>def __init__(\n    self,\n    configure,\n    model,\n    datasets,\n    local_rank: Optional[int] = -1,\n    seed: Optional[int] = 1123,\n):\n    \"\"\"\n    Initialize the SupervisedTask.\n\n    Args:\n        configure: The configuration object.\n        model: The model for the task.\n        datasets: The datasets for training and evaluation.\n        local_rank (optional): The local rank for distributed training. Defaults to -1.\n        seed (optional): The random seed. Defaults to 1123.\n    \"\"\"\n    set_seed(seed)\n    self.n_gpu = 1 if torch.cuda.is_available() else 0\n    if dist.is_initialized():\n        self.n_gpu = dist.get_world_size()\n\n    self.config = configure\n    self.model = model\n    self.datasets = datasets\n    self.local_rank = local_rank\n\n    if self.local_rank != -1:\n        torch.cuda.set_device(self.local_rank)\n\n    if torch.cuda.is_available():\n        self.model = self.model.cuda()\n\n    self.best_score = -np.inf\n</code></pre>"},{"location":"cli/supervised/#unitorch.cli.tasks.supervised.SupervisedTask.eval","title":"eval","text":"<pre><code>eval(\n    monitor_fns: Union[str, List[str]],\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Perform evaluation on the model.</p> <p>Parameters:</p> Name Type Description Default <code>monitor_fns</code> <code>Union[str, List[str]]</code> <p>The monitoring functions for evaluation.</p> required <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>dev_batch_size</code> <code>optional</code> <p>The batch size for evaluation. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/supervised.py</code> <pre><code>@torch.no_grad()\n@add_default_section_for_function(\"core/task/supervised\")\ndef eval(\n    self,\n    monitor_fns: Union[str, List[str]],\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Perform evaluation on the model.\n\n    Args:\n        monitor_fns: The monitoring functions for evaluation.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        dev_batch_size (optional): The batch size for evaluation. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    monitor_fns = [\n        init_registered_module(monitor_fn, self.config, registered_score)\n        for monitor_fn in monitor_fns\n        if monitor_fn in registered_score\n    ]\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n\n    global_rank = -1\n    if self.n_gpu &gt; 1:\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank],\n            output_device=self.local_rank,\n            find_unused_parameters=False,\n            broadcast_buffers=False,\n        )\n        global_rank = dist.get_rank()\n\n    dev_sampler = DistributedSampler if self.n_gpu &gt; 1 else SequentialSampler\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_dev = self.datasets.get(\"dev\")\n    else:\n        dataset_dev = self.datasets.get(\"dev\")\n    iter_dev = DataLoader(\n        dataset_dev,\n        sampler=dev_sampler(dataset_dev)\n        if not isinstance(dataset_dev, Iterable)\n        else None,\n        batch_size=dev_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    results = infer(self.model.module if self.n_gpu &gt; 1 else self.model, iter_dev)\n    if self.local_rank in [-1, 0]:\n        monitor(\n            outputs=results.outputs,\n            targets=results.targets,\n            monitor_fns=monitor_fns,\n        )\n</code></pre>"},{"location":"cli/supervised/#unitorch.cli.tasks.supervised.SupervisedTask.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create a SupervisedTask instance from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A dictionary containing the configuration, model, datasets, and local rank.</p> Source code in <code>src/unitorch/cli/tasks/supervised.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/task/supervised\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create a SupervisedTask instance from the core configuration.\n\n    Args:\n        config: The core configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        A dictionary containing the configuration, model, datasets, and local rank.\n    \"\"\"\n    try:\n        torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n    except:\n        logging.info(\"PyTorch is not in distributed mode\")\n\n    config.set_default_section(\"core/task/supervised\")\n\n    model = config.getoption(\"model\", None)\n    dataset = config.getoption(\"dataset\", None)\n\n    if model is not None:\n        model = init_registered_module(model, config, registered_model)\n\n    if dataset is not None:\n        dataset = init_registered_module(dataset, config, registered_dataset)\n\n    local_rank = config.getdefault(\n        \"core/cli\",\n        \"local_rank\",\n        get_local_rank(),\n    )\n\n    return dict(\n        configure=config,\n        model=model,\n        datasets=dataset,\n        local_rank=local_rank,\n    )\n</code></pre>"},{"location":"cli/supervised/#unitorch.cli.tasks.supervised.SupervisedTask.infer","title":"infer","text":"<pre><code>infer(\n    postprocess_fn: str,\n    writer: str,\n    test_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    max_size: Optional[int] = 10000,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    output_header: Optional[List] = None,\n    output_path: Optional[str] = \"./cache/predict.txt\",\n    postprocess_workers: Optional[int] = 2,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Perform inference using the model.</p> <p>Parameters:</p> Name Type Description Default <code>postprocess_fn</code> <code>str</code> <p>The postprocessing function for inference.</p> required <code>writer</code> <code>str</code> <p>The writer to save the inference results.</p> required <code>test_batch_size</code> <code>optional</code> <p>The batch size for inference. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>max_size</code> <code>optional</code> <p>The maximum number of samples to process. Defaults to 10000.</p> <code>10000</code> <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>output_header</code> <code>optional</code> <p>The header for the output file. Defaults to None.</p> <code>None</code> <code>output_path</code> <code>optional</code> <p>The path to save the output file. Defaults to \"./cache/predict.txt\".</p> <code>'./cache/predict.txt'</code> <code>postprocess_workers</code> <code>optional</code> <p>The number of worker processes for postprocessing. Defaults to 2.</p> <code>2</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/supervised.py</code> <pre><code>@torch.no_grad()\n@add_default_section_for_function(\"core/task/supervised\")\ndef infer(\n    self,\n    postprocess_fn: str,\n    writer: str,\n    test_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    max_size: Optional[int] = 10000,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    output_header: Optional[List] = None,\n    output_path: Optional[str] = \"./cache/predict.txt\",\n    postprocess_workers: Optional[int] = 2,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Perform inference using the model.\n\n    Args:\n        postprocess_fn: The postprocessing function for inference.\n        writer: The writer to save the inference results.\n        test_batch_size (optional): The batch size for inference. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        max_size (optional): The maximum number of samples to process. Defaults to 10000.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        output_header (optional): The header for the output file. Defaults to None.\n        output_path (optional): The path to save the output file. Defaults to \"./cache/predict.txt\".\n        postprocess_workers (optional): The number of worker processes for postprocessing. Defaults to 2.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    assert self.n_gpu &lt;= 1\n    assert writer is not None\n\n    output_dir = os.path.dirname(output_path)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir, exist_ok=True)\n\n    if postprocess_fn is not None:\n        postprocess_fn = init_registered_process(postprocess_fn, self.config)\n\n    if writer is not None:\n        writer = init_registered_module(\n            writer,\n            self.config,\n            registered_writer,\n            output_file=output_path,\n        )\n\n    skip_step = writer.skip_n_samples\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n\n    if skip_step == 0:\n        sampler = SequentialSampler\n    else:\n        sampler = SequentialSkipSampler\n\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_test = self.datasets.get(\"test\")\n    else:\n        dataset_test = self.datasets.get(\"test\")\n\n    iter_test = DataLoader(\n        dataset_test,\n        sampler=sampler(dataset_test)\n        if not isinstance(dataset_test, Iterable)\n        else None,\n        batch_size=test_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    if skip_step &gt; 0 and hasattr(dataset_test, \"set_skip_step\"):\n        dataset_test.set_skip_step(skip_step)\n\n    if skip_step &gt; 0 and hasattr(iter_test.sampler, \"set_skip_step\"):\n        iter_test.sampler.set_skip_step(skip_step)\n\n    if hasattr(dataset_test, \"dataset\"):\n        data_info = dataset_test.dataset\n        data_info = DatasetFeature(data_info)\n        iter_data = DataLoader(\n            deepcopy(data_info),\n            sampler=sampler(data_info)\n            if not isinstance(dataset_test, Iterable)\n            else None,\n            batch_size=test_batch_size,\n            shuffle=False,\n            pin_memory=pin_memory,\n            num_workers=num_workers,\n            collate_fn=None,\n        )\n    else:\n        iter_data = None\n\n    if skip_step &gt; 0 and hasattr(iter_data.sampler, \"set_skip_step\"):\n        iter_data.sampler.set_skip_step(skip_step)\n\n    self.model.eval()\n    start = time.time()\n\n    data_queue = Queue(maxsize=max_size)\n    msg_queue = Queue(maxsize=max_size)\n    postprocess_list = []\n    for _ in range(postprocess_workers):\n        p = PostProcess(\n            postprocess_fn,\n            data_queue,\n            msg_queue,\n        )\n        postprocess_list.append(p)\n        p.start()\n\n    io_process = IOProcess(msg_queue, writer=writer)\n    io_process.start()\n\n    if iter_data is None:\n        for step, (inputs, _) in enumerate(iter_test):\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n            outputs = self.model(**inputs.dict())\n            outputs = outputs.cpu()\n            data_queue.put((step, outputs))\n    else:\n        for step, ((inputs, _), _infos) in enumerate(zip(iter_test, iter_data)):\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n            outputs = self.model(**inputs.dict())\n            outputs = outputs.cpu()\n            if output_header is not None:\n                _infos = {k: _infos[k] for k in output_header if k in _infos}\n                outputs.from_pandas(pd.DataFrame(_infos))\n            data_queue.put((step, outputs))\n\n    data_queue.put((-1, GENERATE_FINISHED))\n    for p in postprocess_list:\n        p.join()\n\n    msg_queue.put((-1, GENERATE_FINISHED))\n    io_process.join()\n\n    end = time.time()\n    ms = (end - start) * 1000\n    logging.info(\n        \"{:.2f} ms, {:.2f} sample/s\".format(\n            ms,\n            ((len(dataset_test) - skip_step) / ms * 1000),\n        )\n    )\n</code></pre>"},{"location":"cli/supervised/#unitorch.cli.tasks.supervised.SupervisedTask.train","title":"train","text":"<pre><code>train(\n    optim: str,\n    loss_fn: str,\n    score_fn: str,\n    monitor_fns: Optional[Union[str, List[str]]] = None,\n    scheduler: Optional[str] = None,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    to_ckpt_dir: Optional[str] = \"./to_ckpt\",\n    train_batch_size: Optional[int] = 128,\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    save_optimizer: Optional[bool] = True,\n    save_scheduler: Optional[bool] = True,\n    log_freq: Optional[int] = 100,\n    ckpt_freq: Optional[int] = 10000,\n    grad_acc_step: Optional[int] = 1,\n    max_grad_norm: Optional[float] = 1.0,\n    num_training_samples: Optional[int] = 1000000000,\n    epochs: Optional[int] = 5,\n    use_ema: Optional[bool] = False,\n    ema_decay: Optional[float] = 0.9999,\n    ema_tau: Optional[int] = 2000,\n    use_amp: Optional[bool] = True,\n    gpu_mode: Optional[bool] = False,\n)\n</code></pre> <p>Train the model.</p> <p>Parameters:</p> Name Type Description Default <code>optim</code> <code>str</code> <p>The optimizer for training.</p> required <code>loss_fn</code> <code>str</code> <p>The loss function for training.</p> required <code>score_fn</code> <code>str</code> <p>The scoring function for evaluation.</p> required <code>monitor_fns</code> <code>optional</code> <p>The monitoring functions for evaluation. Defaults to None.</p> <code>None</code> <code>scheduler</code> <code>optional</code> <p>The scheduler for adjusting the learning rate. Defaults to None.</p> <code>None</code> <code>from_ckpt_dir</code> <code>optional</code> <p>The directory path to load checkpoints from. Defaults to \"./from_ckpt\".</p> <code>'./from_ckpt'</code> <code>to_ckpt_dir</code> <code>optional</code> <p>The directory path to save checkpoints to. Defaults to \"./to_ckpt\".</p> <code>'./to_ckpt'</code> <code>train_batch_size</code> <code>optional</code> <p>The batch size for training. Defaults to 128.</p> <code>128</code> <code>dev_batch_size</code> <code>optional</code> <p>The batch size for evaluation. Defaults to 128.</p> <code>128</code> <code>pin_memory</code> <code>optional</code> <p>Whether to pin memory during data loading. Defaults to True.</p> <code>True</code> <code>num_workers</code> <code>optional</code> <p>The number of worker processes for data loading. Defaults to 4.</p> <code>4</code> <code>save_optimizer</code> <code>optional</code> <p>Whether to save the optimizer. Defaults to True.</p> <code>True</code> <code>save_scheduler</code> <code>optional</code> <p>Whether to save the scheduler. Defaults to True.</p> <code>True</code> <code>log_freq</code> <code>optional</code> <p>The frequency of logging training information. Defaults to 100.</p> <code>100</code> <code>ckpt_freq</code> <code>optional</code> <p>The frequency of saving checkpoints. Defaults to 10000.</p> <code>10000</code> <code>grad_acc_step</code> <code>optional</code> <p>The number of gradient accumulation steps. Defaults to 1.</p> <code>1</code> <code>max_grad_norm</code> <code>optional</code> <p>The maximum gradient norm for gradient clipping. Defaults to 1.0.</p> <code>1.0</code> <code>num_training_samples</code> <code>optional</code> <p>The number of training samples. Defaults to 1000000000.</p> <code>1000000000</code> <code>epochs</code> <code>optional</code> <p>The number of training epochs. Defaults to 5.</p> <code>5</code> <code>use_ema</code> <code>optional</code> <p>Whether to use exponential moving average. Defaults to False.</p> <code>False</code> <code>ema_decay</code> <code>optional</code> <p>The decay rate for exponential moving average. Defaults to 0.9999.</p> <code>0.9999</code> <code>ema_tau</code> <code>optional</code> <p>The time constant for exponential moving average. Defaults to 2000.</p> <code>2000</code> <code>use_amp</code> <code>optional</code> <p>Whether to use automatic mixed precision. Defaults to True.</p> <code>True</code> <code>gpu_mode</code> <code>optional</code> <p>Whether to make GPU active. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/tasks/supervised.py</code> <pre><code>@add_default_section_for_function(\"core/task/supervised\")\ndef train(\n    self,\n    optim: str,\n    loss_fn: str,\n    score_fn: str,\n    monitor_fns: Optional[Union[str, List[str]]] = None,\n    scheduler: Optional[str] = None,\n    from_ckpt_dir: Optional[str] = \"./from_ckpt\",\n    to_ckpt_dir: Optional[str] = \"./to_ckpt\",\n    train_batch_size: Optional[int] = 128,\n    dev_batch_size: Optional[int] = 128,\n    pin_memory: Optional[bool] = True,\n    num_workers: Optional[int] = 4,\n    save_optimizer: Optional[bool] = True,\n    save_scheduler: Optional[bool] = True,\n    log_freq: Optional[int] = 100,\n    ckpt_freq: Optional[int] = 10000,\n    grad_acc_step: Optional[int] = 1,\n    max_grad_norm: Optional[float] = 1.0,\n    num_training_samples: Optional[int] = 1000000000,\n    epochs: Optional[int] = 5,\n    use_ema: Optional[bool] = False,\n    ema_decay: Optional[float] = 0.9999,\n    ema_tau: Optional[int] = 2000,\n    use_amp: Optional[bool] = True,\n    gpu_mode: Optional[bool] = False,\n):\n    \"\"\"\n    Train the model.\n\n    Args:\n        optim: The optimizer for training.\n        loss_fn: The loss function for training.\n        score_fn: The scoring function for evaluation.\n        monitor_fns (optional): The monitoring functions for evaluation. Defaults to None.\n        scheduler (optional): The scheduler for adjusting the learning rate. Defaults to None.\n        from_ckpt_dir (optional): The directory path to load checkpoints from. Defaults to \"./from_ckpt\".\n        to_ckpt_dir (optional): The directory path to save checkpoints to. Defaults to \"./to_ckpt\".\n        train_batch_size (optional): The batch size for training. Defaults to 128.\n        dev_batch_size (optional): The batch size for evaluation. Defaults to 128.\n        pin_memory (optional): Whether to pin memory during data loading. Defaults to True.\n        num_workers (optional): The number of worker processes for data loading. Defaults to 4.\n        save_optimizer (optional): Whether to save the optimizer. Defaults to True.\n        save_scheduler (optional): Whether to save the scheduler. Defaults to True.\n        log_freq (optional): The frequency of logging training information. Defaults to 100.\n        ckpt_freq (optional): The frequency of saving checkpoints. Defaults to 10000.\n        grad_acc_step (optional): The number of gradient accumulation steps. Defaults to 1.\n        max_grad_norm (optional): The maximum gradient norm for gradient clipping. Defaults to 1.0.\n        num_training_samples (optional): The number of training samples. Defaults to 1000000000.\n        epochs (optional): The number of training epochs. Defaults to 5.\n        use_ema (optional): Whether to use exponential moving average. Defaults to False.\n        ema_decay (optional): The decay rate for exponential moving average. Defaults to 0.9999.\n        ema_tau (optional): The time constant for exponential moving average. Defaults to 2000.\n        use_amp (optional): Whether to use automatic mixed precision. Defaults to True.\n        gpu_mode (optional): Whether to make GPU active. Defaults to False.\n    \"\"\"\n    if not os.path.exists(to_ckpt_dir) and self.local_rank in [-1, 0]:\n        os.makedirs(to_ckpt_dir, exist_ok=True)\n\n    if loss_fn is not None:\n        loss_fn = init_registered_module(loss_fn, self.config, registered_loss)\n\n    if score_fn is not None:\n        score_fn = init_registered_module(score_fn, self.config, registered_score)\n\n    if monitor_fns is not None:\n        monitor_fns = [\n            init_registered_module(monitor_fn, self.config, registered_score)\n            for monitor_fn in monitor_fns\n            if monitor_fn in registered_score\n        ]\n\n    if optim is not None and self.model is not None:\n        optim = init_registered_module(\n            optim,\n            self.config,\n            registered_optim,\n            params=filter(lambda x: x.requires_grad, self.model.parameters()),\n        )\n\n    if os.path.exists(from_ckpt_dir):\n        self.model.from_checkpoint(from_ckpt_dir)\n        optim.from_checkpoint(\n            from_ckpt_dir,\n            weight_name=\"pytorch_optim.bin\",\n        )\n\n    if os.path.exists(to_ckpt_dir):\n        self.model.from_checkpoint(\n            to_ckpt_dir,\n            weight_name=\"pytorch_model_latest.bin\",\n        )\n        optim.from_checkpoint(\n            to_ckpt_dir,\n            weight_name=\"pytorch_optim_latest.bin\",\n        )\n\n    info_path = os.path.join(to_ckpt_dir, \"info.json\")\n    if os.path.exists(info_path):\n        info = json.load(open(os.path.join(to_ckpt_dir, \"info.json\")))\n    else:\n        info = dict()\n\n    global_epoch = info.get(\"global_epoch\", 0)\n    global_step = info.get(\"global_step\", 0)\n    self.best_score = info.get(\"best_score\", self.best_score)\n\n    logging.info(f\"the best score is {self.best_score}\")\n\n    self.ema_model = None\n    if use_ema:\n        num_ema_steps = info.get(\"num_ema_steps\", 0)\n        self.ema_model = ExponentialMovingAverage(\n            self.model,\n            decay=ema_decay,\n            tau=ema_tau,\n            num_steps=num_ema_steps,\n        )\n        if os.path.exists(from_ckpt_dir):\n            self.ema_model.from_checkpoint(\n                from_ckpt_dir,\n                weight_name=\"pytorch_ema_model.bin\",\n            )\n        if os.path.exists(to_ckpt_dir):\n            self.ema_model.from_checkpoint(\n                to_ckpt_dir,\n                weight_name=\"pytorch_ema_model_latest.bin\",\n            )\n\n    for n, p in self.model.named_parameters():\n        logging.debug(\n            f\"{n}: trainable - {p.requires_grad} | tensor shape - {p.shape}\"\n        )\n\n    global_rank = -1\n    if self.n_gpu &gt; 1:\n        self.model = nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.local_rank],\n            output_device=self.local_rank,\n            find_unused_parameters=False,\n            broadcast_buffers=False,\n        )\n        global_rank = dist.get_rank()\n\n    train_sampler = DistributedSkipSampler if self.n_gpu &gt; 1 else RandomSkipSampler\n    dev_sampler = DistributedSampler if self.n_gpu &gt; 1 else SequentialSampler\n\n    if gpu_mode:\n        with ActiveGPUJob() as _:\n            dataset_train = self.datasets.get(\"train\")\n            dataset_dev = self.datasets.get(\"dev\")\n    else:\n        dataset_train = self.datasets.get(\"train\")\n        dataset_dev = self.datasets.get(\"dev\")\n\n    iter_train = DataLoader(\n        dataset_train,\n        sampler=train_sampler(dataset_train)\n        if not isinstance(dataset_train, Iterable)\n        else None,\n        batch_size=train_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    iter_dev = DataLoader(\n        dataset_dev,\n        sampler=dev_sampler(dataset_dev)\n        if not isinstance(dataset_dev, Iterable)\n        else None,\n        batch_size=dev_batch_size,\n        shuffle=False,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        collate_fn=collate_fn,\n    )\n\n    if scheduler is not None:\n        if not isinstance(dataset_train, Iterable):\n            num_training_steps = int(\n                epochs\n                * len(dataset_train)\n                // train_batch_size\n                // max(1, self.n_gpu)\n                // grad_acc_step\n            )\n        else:\n            num_training_steps = int(\n                epochs\n                * num_training_samples\n                // train_batch_size\n                // max(1, self.n_gpu)\n                // grad_acc_step\n            )\n\n        scheduler = init_registered_module(\n            scheduler,\n            self.config,\n            registered_scheduler,\n            optimizer=optim,\n            num_training_steps=num_training_steps,\n        )\n\n    if scheduler and os.path.exists(to_ckpt_dir):\n        scheduler.from_checkpoint(\n            to_ckpt_dir,\n            weight_name=\"pytorch_scheduler_latest.bin\",\n        )\n\n    if use_amp:\n        scaler = GradScaler()\n\n    log_loss = 0\n    dev_epoch = 0\n    for e in range(0, epochs):\n        torch.cuda.empty_cache()\n        if e &lt; global_epoch:\n            continue\n\n        if hasattr(dataset_train, \"set_epoch\"):\n            dataset_train.set_epoch(e)\n\n        if hasattr(dataset_train, \"set_skip_step\"):\n            dataset_train.set_skip_step(global_step * train_batch_size)\n\n        if hasattr(iter_train.sampler, \"set_epoch\"):\n            iter_train.sampler.set_epoch(e)\n\n        if hasattr(iter_train.sampler, \"set_skip_step\"):\n            iter_train.sampler.set_skip_step(global_step * train_batch_size)\n\n        self.model.train()\n        is_update_step = True\n        for step, (inputs, targets) in enumerate(iter_train):\n            step = step + global_step\n            is_update_step = False\n            if torch.cuda.is_available():\n                inputs = inputs.cuda()\n                targets = targets.cuda()\n\n            if is_torch2_available():\n                with torch.cuda.amp.autocast(\n                    enabled=use_amp\n                ) as autocast, torch.backends.cuda.sdp_kernel(\n                    enable_flash=False\n                ) as disable:\n                    outputs = self.model(**inputs.dict())\n                    if isinstance(outputs, LossOutputs):\n                        loss = outputs.loss / grad_acc_step\n                    else:\n                        loss = (\n                            loss_fn(outputs=outputs, targets=targets)\n                            / grad_acc_step\n                        )\n            else:\n                with torch.cuda.amp.autocast(enabled=use_amp) as autocast:\n                    outputs = self.model(**inputs.dict())\n                    if isinstance(outputs, LossOutputs):\n                        loss = outputs.loss / grad_acc_step\n                    else:\n                        loss = (\n                            loss_fn(outputs=outputs, targets=targets)\n                            / grad_acc_step\n                        )\n\n            nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n            if use_amp:\n                scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            log_loss += loss.data * grad_acc_step\n            if (step + 1) % grad_acc_step == 0:\n                is_update_step = True\n                if use_amp:\n                    scaler.step(optim)\n                    scaler.update()\n                else:\n                    optim.step()\n                if scheduler is not None:\n                    scheduler.step()\n                optim.zero_grad()\n\n                if use_ema and self.ema_model is not None:\n                    self.ema_model.step(\n                        self.model.module if self.n_gpu &gt; 1 else self.model\n                    )\n\n            if (step + 1) % log_freq == 0 and global_rank in [-1, 0]:\n                logging.info(\n                    f\"epoch {e} step {step}: loss -- { log_loss / log_freq }\"\n                )\n                log_loss = 0\n\n            if (step + 1) % ckpt_freq == 0:\n                if hasattr(dataset_dev, \"set_epoch\"):\n                    dataset_dev.set_epoch(dev_epoch)\n\n                if hasattr(iter_dev.sampler, \"set_epoch\"):\n                    iter_dev.sampler.set_epoch(dev_epoch)\n\n                dev_epoch += 1\n                self.best_score = save_checkpoint(\n                    self.model.module if self.n_gpu &gt; 1 else self.model,\n                    to_ckpt_dir,\n                    iter_dev,\n                    score_fn,\n                    monitor_fns,\n                    optim=optim if save_optimizer else None,\n                    scheduler=scheduler if save_scheduler else None,\n                    ema_model=self.ema_model if use_ema else None,\n                    best_score=self.best_score,\n                    info_path=info_path,\n                    local_rank=self.local_rank,\n                    global_epoch=e,\n                    global_step=step + 1,\n                )\n\n        if not is_update_step:\n            scaler.step(optim)\n            scaler.update()\n            if scheduler is not None:\n                scheduler.step()\n            optim.zero_grad()\n\n            if use_ema and self.ema_model is not None:\n                self.ema_model.step(\n                    self.model.module if self.n_gpu &gt; 1 else self.model\n                )\n\n        log_loss = 0\n\n        if hasattr(dataset_dev, \"set_epoch\"):\n            dataset_dev.set_epoch(dev_epoch)\n\n        if hasattr(iter_dev.sampler, \"set_epoch\"):\n            iter_dev.sampler.set_epoch(dev_epoch)\n\n        dev_epoch += 1\n\n        global_step = 0\n        self.best_score = save_checkpoint(\n            self.model.module if self.n_gpu &gt; 1 else self.model,\n            to_ckpt_dir,\n            iter_dev,\n            score_fn,\n            monitor_fns,\n            optim=optim if save_optimizer else None,\n            scheduler=scheduler if save_scheduler else None,\n            ema_model=self.ema_model if use_ema else None,\n            best_score=self.best_score,\n            info_path=info_path,\n            local_rank=self.local_rank,\n            global_epoch=e,\n            global_step=0,\n        )\n</code></pre>"},{"location":"cli/models/bart/","title":"unitorch.cli.models.bart","text":""},{"location":"cli/models/bart/#bartprocessor","title":"BartProcessor","text":"<p>Tip</p> <p><code>core/process/bart</code> is the section for configuration of BartProcessor.</p> <p>             Bases: <code>BartProcessor</code></p> <p>Class for processing data with BART model.</p> <p>Initialize BartProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the merge file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generation sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/cli/models/bart/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize BartProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the merge file.\n        special_input_ids (Dict, optional): Special input IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generation sequence length. Defaults to 48.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        special_input_ids=special_input_ids,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/bart/#unitorch.cli.models.bart.BartProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BartProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BartProcessor</code> <p>An instance of BartProcessor.</p> Source code in <code>src/unitorch/cli/models/bart/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/bart\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BartProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BartProcessor: An instance of BartProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/bart\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bart\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_bart_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    merge_path = config.getoption(\"merge_path\", None)\n    merge_path = pop_value(\n        merge_path,\n        nested_dict_value(pretrained_bart_infos, pretrained_name, \"merge\"),\n    )\n    merge_path = cached_path(merge_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"merge_path\": merge_path,\n    }\n</code></pre>"},{"location":"cli/models/bart/#bartforgeneration","title":"BartForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/bart</code> is the section for configuration of BartForGeneration.</p> <p>             Bases: <code>BartForGeneration</code></p> <p>BART model for generation tasks.</p> <p>Initialize the BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing for memory optimization.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/bart/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BartForGeneration model.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing for memory optimization.\n    \"\"\"\n    super().__init__(\n        config_path=config_path, gradient_checkpointing=gradient_checkpointing\n    )\n</code></pre>"},{"location":"cli/models/bart/#unitorch.cli.models.bart.BartForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/cli/models/bart/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BartForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Input IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/bart/#unitorch.cli.models.bart.BartForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BartForGeneration from core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BartForGeneration</code> <p>The initialized BartForGeneration instance.</p> Source code in <code>src/unitorch/cli/models/bart/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/bart\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BartForGeneration from core configuration.\n\n    Args:\n        config: The core configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BartForGeneration: The initialized BartForGeneration instance.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/bart\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bart\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bart_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bart_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/bart/#unitorch.cli.models.bart.BartForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>ID of the decoder start token.</p> <code>2</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>ID of the decoder end token.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum length of generated sequences.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum length of generated sequences.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to avoid repeating.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty for generated sequences.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of groups for diverse beam search.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/cli/models/bart/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/bart\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the BartForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Input IDs.\n        num_beams (int, optional): Number of beams for beam search.\n        decoder_start_token_id (int, optional): ID of the decoder start token.\n        decoder_end_token_id (int or List[int], optional): ID of the decoder end token.\n        num_return_sequences (int, optional): Number of generated sequences to return.\n        min_gen_seq_length (int, optional): Minimum length of generated sequences.\n        max_gen_seq_length (int, optional): Maximum length of generated sequences.\n        repetition_penalty (float, optional): Repetition penalty.\n        no_repeat_ngram_size (int, optional): Size of n-grams to avoid repeating.\n        early_stopping (bool, optional): Whether to stop generation early.\n        length_penalty (float, optional): Length penalty for generated sequences.\n        num_beam_groups (int, optional): Number of groups for diverse beam search.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search.\n        do_sample (bool, optional): Whether to use sampling for generation.\n        temperature (float, optional): Sampling temperature.\n        top_k (int, optional): Top-k sampling parameter.\n        top_p (float, optional): Top-p sampling parameter.\n\n    Returns:\n        GenerationOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = super().generate(\n        input_ids=input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/beit/","title":"unitorch.cli.models.beit","text":""},{"location":"cli/models/beit/#beitprocessor","title":"BeitProcessor","text":"<p>Tip</p> <p><code>core/process/beit</code> is the section for configuration of BeitProcessor.</p> <p>             Bases: <code>BeitProcessor</code></p> <p>Class for processing images using the Beit model.</p> <p>Initialize BeitProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required Source code in <code>src/unitorch/cli/models/beit/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initialize BeitProcessor.\n\n    Args:\n        vision_config_path (str): The path to the vision configuration file.\n    \"\"\"\n    super().__init__(\n        vision_config_path=vision_config_path,\n    )\n</code></pre>"},{"location":"cli/models/beit/#unitorch.cli.models.beit.BeitProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BeitProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BeitProcessor</code> <p>An instance of BeitProcessor.</p> Source code in <code>src/unitorch/cli/models/beit/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/beit\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BeitProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BeitProcessor: An instance of BeitProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/beit\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-beit\")\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(pretrained_beit_infos, pretrained_name, \"vision_config\"),\n    )\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/beit/#beitforimageclassification","title":"BeitForImageClassification","text":"<p>Tip</p> <p><code>core/model/classification/beit</code> is the section for configuration of BeitForImageClassification.</p> <p>             Bases: <code>BeitForImageClassification</code></p> <p>Class for image classification using the Beit model.</p> <p>Initialize BeitForImageClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/cli/models/beit/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initialize BeitForImageClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of output classes. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n</code></pre>"},{"location":"cli/models/beit/#unitorch.cli.models.beit.BeitForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the BeitForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input pixel values.</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The model outputs.</p> Source code in <code>src/unitorch/cli/models/beit/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BeitForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The input pixel values.\n\n    Returns:\n        ClassificationOutputs: The model outputs.\n    \"\"\"\n    outputs = super().forward(pixel_values=pixel_values)\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/beit/#unitorch.cli.models.beit.BeitForImageClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BeitForImageClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BeitForImageClassification</code> <p>An instance of BeitForImageClassification.</p> Source code in <code>src/unitorch/cli/models/beit/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/beit\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BeitForImageClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BeitForImageClassification: An instance of BeitForImageClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/beit\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-beit\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_beit_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_beit_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/bert/","title":"unitorch.cli.models.bert","text":""},{"location":"cli/models/bert/#bertprocessor","title":"BertProcessor","text":"<p>Tip</p> <p><code>core/process/bert</code> is the section for configuration of BertProcessor.</p> <p>             Bases: <code>BertProcessor</code></p> <p>Processor for BERT models.</p> <p>Initialize BertProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>special_input_ids</code> <code>Dict</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>do_lower_case</code> <code>bool</code> <p>Whether to lower case the input text. Defaults to True.</p> <code>True</code> <code>do_basic_tokenize</code> <code>bool</code> <p>Whether to perform basic tokenization. Defaults to True.</p> <code>True</code> <code>do_whole_word_mask</code> <code>bool</code> <p>Whether to use whole word masking. Defaults to True.</p> <code>True</code> <code>masked_lm_prob</code> <code>float</code> <p>The probability of masking a token for masked language modeling. Defaults to 0.15.</p> <code>0.15</code> <code>max_predictions_per_seq</code> <code>int</code> <p>The maximum number of masked LM predictions per sequence. Defaults to 20.</p> <code>20</code> Source code in <code>src/unitorch/cli/models/bert/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    special_input_ids: Optional[Dict] = dict(),\n    do_lower_case: Optional[bool] = True,\n    do_basic_tokenize: Optional[bool] = True,\n    do_whole_word_mask: Optional[bool] = True,\n    masked_lm_prob: Optional[float] = 0.15,\n    max_predictions_per_seq: Optional[int] = 20,\n):\n    \"\"\"\n    Initialize BertProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        special_input_ids (Dict, optional): Special input IDs. Defaults to an empty dictionary.\n        do_lower_case (bool, optional): Whether to lower case the input text. Defaults to True.\n        do_basic_tokenize (bool, optional): Whether to perform basic tokenization. Defaults to True.\n        do_whole_word_mask (bool, optional): Whether to use whole word masking. Defaults to True.\n        masked_lm_prob (float, optional): The probability of masking a token for masked language modeling. Defaults to 0.15.\n        max_predictions_per_seq (int, optional): The maximum number of masked LM predictions per sequence. Defaults to 20.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        special_input_ids=special_input_ids,\n        do_lower_case=do_lower_case,\n        do_basic_tokenize=do_basic_tokenize,\n        do_whole_word_mask=do_whole_word_mask,\n        masked_lm_prob=masked_lm_prob,\n        max_predictions_per_seq=max_predictions_per_seq,\n    )\n</code></pre>"},{"location":"cli/models/bert/#unitorch.cli.models.bert.BertProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BertProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BertProcessor</code> <p>An instance of BertProcessor.</p> Source code in <code>src/unitorch/cli/models/bert/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/bert\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BertProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BertProcessor: An instance of BertProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/bert\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bert\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_bert_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/bert/#bertforclassification","title":"BertForClassification","text":"<p>Tip</p> <p><code>core/model/classification/bert</code> is the section for configuration of BertForClassification.</p> <p>             Bases: <code>BertForClassification</code></p> <p>BERT model for classification tasks.</p> <p>Initialize BertForClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/bert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize BertForClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/bert/#unitorch.cli.models.bert.BertForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the BertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>Model outputs for classification.</p> Source code in <code>src/unitorch/cli/models/bert/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the BertForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Input IDs.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: Model outputs for classification.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/bert/#unitorch.cli.models.bert.BertForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BertForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BertForClassification</code> <p>An instance of BertForClassification.</p> Source code in <code>src/unitorch/cli/models/bert/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/bert\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BertForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BertForClassification: An instance of BertForClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/bert\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bert\")\n    config_path = config.getoption(\"config_path\", None)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bert_infos, pretrained_name, \"config\"),\n    )\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bert_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/","title":"unitorch.cli.models.blip","text":""},{"location":"cli/models/blip/#blipprocessor","title":"BlipProcessor","text":"<p>Tip</p> <p><code>core/process/blip</code> is the section for configuration of BlipProcessor.</p> <p>             Bases: <code>BlipProcessor</code></p> <p>Processor for the BLIP model.</p> <p>Initialize BlipProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generated sequence length. Defaults to 48.</p> <code>48</code> <code>position_start_id</code> <code>int</code> <p>The start position ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/cli/models/blip/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    vision_config_path: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n    position_start_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initialize BlipProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        vision_config_path (str): The path to the vision configuration file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generated sequence length. Defaults to 48.\n        position_start_id (int, optional): The start position ID. Defaults to 0.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        vision_config_path=vision_config_path,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        position_start_id=position_start_id,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the processor parameters.</p> Source code in <code>src/unitorch/cli/models/blip/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/blip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the processor parameters.\n    \"\"\"\n    config.set_default_section(\"core/process/blip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"vision_config\"),\n    )\n\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/blip/#blipforpretrain","title":"BlipForPretrain","text":"<p>Tip</p> <p><code>core/model/pretrain/blip</code> is the section for configuration of BlipForPretrain.</p> <p>             Bases: <code>BlipForPretrain</code></p> <p>BLIP model for pretraining.</p> <p>Initialize BlipForPretrain.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> <code>use_all_gather</code> <code>bool</code> <p>Whether to use all_gather operation. Defaults to True.</p> <code>True</code> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n    use_all_gather: Optional[bool] = True,\n):\n    \"\"\"\n    Initialize BlipForPretrain.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n        use_all_gather (bool, optional): Whether to use all_gather operation. Defaults to True.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n        use_all_gather=use_all_gather,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor = None,\n    position_ids: torch.Tensor = None,\n)\n</code></pre> <p>Forward pass of the BlipForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>The pixel values of the images.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LossOutputs</code> <p>The loss outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor = None,\n    position_ids: torch.Tensor = None,\n):\n    \"\"\"\n    Forward pass of the BlipForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        pixel_values (torch.Tensor): The pixel values of the images.\n        attention_mask (torch.Tensor, optional): The attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs. Defaults to None.\n\n    Returns:\n        LossOutputs: The loss outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        pixel_values=pixel_values,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return LossOutputs(loss=outputs)\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForPretrain.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipForPretrain from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BlipForPretrain</code> <p>An instance of BlipForPretrain.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/pretrain/blip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipForPretrain from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BlipForPretrain: An instance of BlipForPretrain.\n    \"\"\"\n    config.set_default_section(\"core/model/pretrain/blip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    use_all_gather = config.getoption(\"use_all_gather\", True)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n        use_all_gather=use_all_gather,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/#blipforclassification","title":"BlipForClassification","text":"<p>Tip</p> <p><code>core/model/classification/blip</code> is the section for configuration of BlipForClassification.</p> <p>             Bases: <code>BlipForClassification</code></p> <p>BLIP model for classification.</p> <p>Initialize BlipForClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize BlipForClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor = None,\n    position_ids: torch.Tensor = None,\n)\n</code></pre> <p>Forward pass of the BlipForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>The pixel values of the images.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor = None,\n    position_ids: torch.Tensor = None,\n):\n    \"\"\"\n    Forward pass of the BlipForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        pixel_values (torch.Tensor): The pixel values of the images.\n        attention_mask (torch.Tensor, optional): The attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        pixel_values=pixel_values,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BlipForClassification</code> <p>An instance of BlipForClassification.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/blip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BlipForClassification: An instance of BlipForClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/blip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/#blipfortextclassification","title":"BlipForTextClassification","text":"<p>Tip</p> <p><code>core/model/classification/blip/text</code> is the section for configuration of BlipForTextClassification.</p> <p>             Bases: <code>BlipForTextClassification</code></p> <p>BLIP model for text classification.</p> <p>Initialize BlipForTextClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize BlipForTextClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForTextClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids=None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the BlipForTextClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>The attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids=None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the BlipForTextClassification model.\n\n    Args:\n        input_ids (torch.Tensor, optional): The input token IDs. Defaults to None.\n        attention_mask (torch.Tensor, optional): The attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForTextClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipForTextClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BlipForTextClassification</code> <p>An instance of BlipForTextClassification.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/blip/text\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipForTextClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BlipForTextClassification: An instance of BlipForTextClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/blip/text\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_Truemodel\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/#blipforimageclassification","title":"BlipForImageClassification","text":"<p>Tip</p> <p><code>core/model/classification/blip/image</code> is the section for configuration of BlipForImageClassification.</p> <p>             Bases: <code>BlipForImageClassification</code></p> <p>BLIP model for image classification.</p> <p>Initialize BlipForImageClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize BlipForImageClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the BlipForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values of the images.</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BlipForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values of the images.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(pixel_values=pixel_values)\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForImageClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipForImageClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BlipForImageClassification</code> <p>An instance of BlipForImageClassification.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/blip/image\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipForImageClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BlipForImageClassification: An instance of BlipForImageClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/blip/image\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/#blipforimagecaption","title":"BlipForImageCaption","text":"<p>Tip</p> <p><code>core/model/caption/blip</code> is the section for configuration of BlipForImageCaption.</p> <p>             Bases: <code>BlipForImageCaption</code></p> <p>BLIP model for image captioning.</p> <p>Initialize BlipForImageCaption.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize BlipForImageCaption.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForImageCaption.forward","title":"forward","text":"<pre><code>forward(\n    pixel_values: torch.Tensor,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the BlipForImageCaption model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values of the images.</p> required <code>input_ids</code> <code>Tensor</code> <p>The input captions.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the BlipForImageCaption model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values of the images.\n        input_ids (torch.Tensor): The input captions.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        pixel_values=pixel_values,\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForImageCaption.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BlipForImageCaption from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BlipForImageCaption</code> <p>An instance of BlipForImageCaption.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/caption/blip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BlipForImageCaption from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BlipForImageCaption: An instance of BlipForImageCaption.\n    \"\"\"\n    config.set_default_section(\"core/model/caption/blip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-blip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_blip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/blip/#unitorch.cli.models.blip.BlipForImageCaption.generate","title":"generate","text":"<pre><code>generate(\n    pixel_values: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 101,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 102,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate captions using the BlipForImageCaption model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values of the images.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The start token ID for the decoder. Defaults to 30522.</p> <code>101</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The end token ID for the decoder. Defaults to 2.</p> <code>102</code> <code>num_return_sequences</code> <code>int</code> <p>The number of sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature value for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The top-k value for sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The top-p value for sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/blip/modeling.py</code> <pre><code>@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    pixel_values: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 101,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 102,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate captions using the BlipForImageCaption model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values of the images.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The start token ID for the decoder. Defaults to 30522.\n        decoder_end_token_id (int or List[int], optional): The end token ID for the decoder. Defaults to 2.\n        num_return_sequences (int, optional): The number of sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n        top_k (int, optional): The top-k value for sampling. Defaults to 50.\n        top_p (float, optional): The top-p value for sampling. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        pixel_values=pixel_values,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/bloom/","title":"unitorch.cli.models.bloom","text":""},{"location":"cli/models/bloom/#bloomprocessor","title":"BloomProcessor","text":"<p>Tip</p> <p><code>core/process/bloom</code> is the section for configuration of BloomProcessor.</p> <p>             Bases: <code>BloomProcessor</code></p> <p>Processor for Bloom language models.</p> <p>Initialize the BloomProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_file</code> <code>str</code> <p>The path to the tokenizer file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generation sequence length. Defaults to 128.</p> <code>128</code> Source code in <code>src/unitorch/cli/models/bloom/processing.py</code> <pre><code>def __init__(\n    self,\n    tokenizer_file: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 128,\n):\n    \"\"\"\n    Initialize the BloomProcessor.\n\n    Args:\n        tokenizer_file (str): The path to the tokenizer file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generation sequence length. Defaults to 128.\n    \"\"\"\n    super().__init__(\n        tokenizer_file=tokenizer_file,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomProcessor from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The core configuration object.</p> required <p>Returns:</p> Name Type Description <code>BloomProcessor</code> <p>An instance of BloomProcessor initialized with the provided configuration.</p> Source code in <code>src/unitorch/cli/models/bloom/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomProcessor from the core configuration.\n\n    Args:\n        config (Config): The core configuration object.\n\n    Returns:\n        BloomProcessor: An instance of BloomProcessor initialized with the provided configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    tokenizer_file = config.getoption(\"tokenizer_file\", None)\n    tokenizer_file = pop_value(\n        tokenizer_file,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"tokenizer\"),\n    )\n    tokenizer_file = cached_path(tokenizer_file)\n\n    return {\n        \"tokenizer_file\": tokenizer_file,\n    }\n</code></pre>"},{"location":"cli/models/bloom/#bloomforclassification","title":"BloomForClassification","text":"<p>Tip</p> <p><code>core/model/classification/bloom</code> is the section for configuration of BloomForClassification.</p> <p>             Bases: <code>BloomForClassification</code></p> <p>Bloom model for classification.</p> <p>Initialize the BloomForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BloomForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the BloomForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the BloomForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor, optional): The attention mask. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomForClassification from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The core configuration object.</p> required <p>Returns:</p> Name Type Description <code>BloomForClassification</code> <p>An instance of BloomForClassification initialized with the provided configuration.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomForClassification from the core configuration.\n\n    Args:\n        config (Config): The core configuration object.\n\n    Returns:\n        BloomForClassification: An instance of BloomForClassification initialized with the provided configuration.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    config_path = config.getoption(\"config_path\", None)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/bloom/#bloomforpretrain","title":"BloomForPretrain","text":"<p>Tip</p> <p><code>core/model/pretrain/bloom</code> is the section for configuration of BloomForPretrain.</p> <p>             Bases: <code>BloomForPretrain</code></p> <p>Bloom model for pretraining.</p> <p>Initialize the BloomForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BloomForPretrain model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    input_ids_label: torch.Tensor,\n    attention_mask_label: torch.Tensor,\n)\n</code></pre> <p>Perform forward pass of the BloomForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>input_ids_label</code> <code>Tensor</code> <p>The input token IDs for the masked language modeling task.</p> required <code>attention_mask_label</code> <code>Tensor</code> <p>The attention mask for the masked language modeling task.</p> required <p>Returns:</p> Name Type Description <code>LossOutputs</code> <p>The loss outputs.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    input_ids_label: torch.Tensor,\n    attention_mask_label: torch.Tensor,\n):\n    \"\"\"\n    Perform forward pass of the BloomForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        input_ids_label (torch.Tensor): The input token IDs for the masked language modeling task.\n        attention_mask_label (torch.Tensor): The attention mask for the masked language modeling task.\n\n    Returns:\n        LossOutputs: The loss outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        input_ids_label=input_ids_label,\n        attention_mask_label=attention_mask_label,\n    )\n    return LossOutputs(loss=outputs)\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForPretrain.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomForPretrain from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The core configuration object.</p> required <p>Returns:</p> Name Type Description <code>BloomForPretrain</code> <p>An instance of BloomForPretrain initialized with the provided configuration.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/pretrain/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomForPretrain from the core configuration.\n\n    Args:\n        config (Config): The core configuration object.\n\n    Returns:\n        BloomForPretrain: An instance of BloomForPretrain initialized with the provided configuration.\n    \"\"\"\n    config.set_default_section(\"core/model/pretrain/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/bloom/#bloomforgeneration","title":"BloomForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/bloom</code> is the section for configuration of BloomForGeneration.</p> <p>             Bases: <code>BloomForGeneration</code></p> <p>Bloom model for text generation.</p> <p>Initialize the BloomForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BloomForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the BloomForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the BloomForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor, optional): The attention mask. Defaults to None.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomForGeneration from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The core configuration object.</p> required <p>Returns:</p> Name Type Description <code>BloomForGeneration</code> <p>An instance of BloomForGeneration initialized with the provided configuration.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomForGeneration from the core configuration.\n\n    Args:\n        config (Config): The core configuration object.\n\n    Returns:\n        BloomForGeneration: An instance of BloomForGeneration initialized with the provided configuration.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/bloom/#unitorch.cli.models.bloom.BloomForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the Bloom model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/bloom/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/bloom\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the Bloom model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/clip/","title":"unitorch.cli.models.clip","text":""},{"location":"cli/models/clip/#clipprocessor","title":"ClipProcessor","text":"<p>Tip</p> <p><code>core/process/clip</code> is the section for configuration of ClipProcessor.</p> <p>             Bases: <code>ClipProcessor</code></p> <p>Processor for the CLIP model.</p> <p>Initialize the ClipProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the BPE merge file.</p> required <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>position_start_id</code> <code>int</code> <p>The position start ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/cli/models/clip/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vision_config_path: str,\n    max_seq_length: Optional[int] = 128,\n    position_start_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initialize the ClipProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the BPE merge file.\n        vision_config_path (str): The path to the vision configuration file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        position_start_id (int, optional): The position start ID. Defaults to 0.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        vision_config_path=vision_config_path,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ClipProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the processor's initialization arguments.</p> Source code in <code>src/unitorch/cli/models/clip/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/clip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ClipProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the processor's initialization arguments.\n    \"\"\"\n    config.set_default_section(\"core/process/clip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-clip\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    merge_path = config.getoption(\"merge_path\", None)\n    merge_path = pop_value(\n        merge_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"merge\"),\n    )\n    merge_path = cached_path(merge_path)\n\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"vision_config\"),\n    )\n\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"merge_path\": merge_path,\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/clip/#clipforpretrain","title":"ClipForPretrain","text":"<p>Tip</p> <p><code>core/model/pretrain/clip</code> is the section for configuration of ClipForPretrain.</p> <p>             Bases: <code>ClipForPretrain</code></p> <p>CLIP model for pretraining.</p> <p>Initialize the ClipForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> <code>use_all_gather</code> <code>bool</code> <p>Whether to use all_gather operation. Defaults to True.</p> <code>True</code> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n    use_all_gather: Optional[bool] = True,\n):\n    \"\"\"\n    Initialize the ClipForPretrain model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        freeze_base_model (bool, optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n        use_all_gather (bool, optional): Whether to use all_gather operation. Defaults to True.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n        use_all_gather=use_all_gather,\n    )\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LossOutputs</code> <p>The loss outputs.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        pixel_values (torch.Tensor): Input pixel values.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        LossOutputs: The loss outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        pixel_values=pixel_values,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return LossOutputs(loss=outputs)\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForPretrain.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ClipForPretrain from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClipForPretrain</code> <p>An instance of the ClipForPretrain model.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/pretrain/clip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ClipForPretrain from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ClipForPretrain: An instance of the ClipForPretrain model.\n    \"\"\"\n    config.set_default_section(\"core/model/pretrain/clip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-clip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    use_all_gather = config.getoption(\"use_all_gather\", True)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n        use_all_gather=use_all_gather,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/clip/#clipforclassification","title":"ClipForClassification","text":"<p>Tip</p> <p><code>core/model/classification/clip</code> is the section for configuration of ClipForClassification.</p> <p>             Bases: <code>ClipForClassification</code></p> <p>CLIP model for classification.</p> <p>Initialize the ClipForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the ClipForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of output classes. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        pixel_values (torch.Tensor): Input pixel values.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        pixel_values=pixel_values,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ClipForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClipForClassification</code> <p>An instance of the ClipForClassification model.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/clip\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ClipForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ClipForClassification: An instance of the ClipForClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/clip\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-clip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/clip/#clipfortextclassification","title":"ClipForTextClassification","text":"<p>Tip</p> <p><code>core/model/classification/clip/text</code> is the section for configuration of ClipForTextClassification.</p> <p>             Bases: <code>ClipForTextClassification</code></p> <p>CLIP model for text classification.</p> <p>Initialize the ClipForTextClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the ClipForTextClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of output classes. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForTextClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids=None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids=None,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input token IDs. Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForTextClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ClipForTextClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClipForTextClassification</code> <p>An instance of the ClipForTextClassification model.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/clip/text\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ClipForTextClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ClipForTextClassification: An instance of the ClipForTextClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/clip/text\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-clip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_Truemodel\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/clip/#clipforimageclassification","title":"ClipForImageClassification","text":"<p>Tip</p> <p><code>core/model/classification/clip/image</code> is the section for configuration of ClipForImageClassification.</p> <p>             Bases: <code>ClipForImageClassification</code></p> <p>CLIP model for image classification.</p> <p>Initialize the ClipForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection head. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the ClipForImageClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        projection_dim (int, optional): The dimension of the projection head. Defaults to 512.\n        num_classes (int, optional): The number of output classes. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        pixel_values (torch.Tensor): Input pixel values.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(pixel_values=pixel_values)\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/clip/#unitorch.cli.models.clip.ClipForImageClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ClipForImageClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ClipForImageClassification</code> <p>An instance of the ClipForImageClassification model.</p> Source code in <code>src/unitorch/cli/models/clip/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/clip/image\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ClipForImageClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ClipForImageClassification: An instance of the ClipForImageClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/clip/image\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-clip\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n\n    projection_dim = config.getoption(\"projection_dim\", 512)\n    num_classes = config.getoption(\"num_classes\", 1)\n    freeze_base_model = config.getoption(\"freeze_base_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path=config_path,\n        projection_dim=projection_dim,\n        num_classes=num_classes,\n        freeze_base_model=freeze_base_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_clip_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/deberta/","title":"unitorch.cli.models.deberta","text":""},{"location":"cli/models/deberta/#debertaprocessor","title":"DebertaProcessor","text":"<p>Tip</p> <p><code>core/process/deberta</code> is the section for configuration of DebertaProcessor.</p> <p>             Bases: <code>DebertaProcessor</code></p> <p>Processor for Deberta models.</p> <p>Initialize the DebertaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the merge file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>The source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>The target type ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/cli/models/deberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    merge_path,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initialize the DebertaProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the merge file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        source_type_id (int, optional): The source type ID. Defaults to 0.\n        target_type_id (int, optional): The target type ID. Defaults to 0.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n    )\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of DebertaProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the configuration parameters for DebertaProcessor.</p> Source code in <code>src/unitorch/cli/models/deberta/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/deberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of DebertaProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the configuration parameters for DebertaProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/deberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-deberta\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_deberta_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    merge_path = config.getoption(\"merge_path\", None)\n    merge_path = pop_value(\n        merge_path,\n        nested_dict_value(pretrained_deberta_infos, pretrained_name, \"merge\"),\n    )\n    merge_path = cached_path(merge_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"merge_path\": merge_path,\n    }\n</code></pre>"},{"location":"cli/models/deberta/#debertaforclassification","title":"DebertaForClassification","text":"<p>Tip</p> <p><code>core/model/classification/deberta</code> is the section for configuration of DebertaForClassification.</p> <p>             Bases: <code>DebertaForClassification</code></p> <p>Deberta model for classification tasks.</p> <p>Initialize the DebertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/deberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the DebertaForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass on the DebertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing the input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>The token type IDs tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output of the classification model.</p> Source code in <code>src/unitorch/cli/models/deberta/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass on the DebertaForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing the input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        token_type_ids (torch.Tensor, optional): The token type IDs tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs tensor. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The output of the classification model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of DebertaForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DebertaForClassification</code> <p>An instance of DebertaForClassification.</p> Source code in <code>src/unitorch/cli/models/deberta/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/deberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of DebertaForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        DebertaForClassification: An instance of DebertaForClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/deberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-deberta\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_deberta_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_deberta_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/deberta/#debertav2processor","title":"DebertaV2Processor","text":"<p>Tip</p> <p><code>core/process/deberta/v2</code> is the section for configuration of DebertaV2Processor.</p> <p>             Bases: <code>DebertaV2Processor</code></p> <p>Processor for Deberta V2 models.</p> <p>Initialize the DebertaV2Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>The source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>The target type ID. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/cli/models/deberta/processing_v2.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 1,\n):\n    \"\"\"\n    Initialize the DebertaV2Processor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        source_type_id (int, optional): The source type ID. Defaults to 0.\n        target_type_id (int, optional): The target type ID. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n    )\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaV2Processor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of DebertaV2Processor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the configuration parameters for DebertaV2Processor.</p> Source code in <code>src/unitorch/cli/models/deberta/processing_v2.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/deberta/v2\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of DebertaV2Processor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the configuration parameters for DebertaV2Processor.\n    \"\"\"\n    config.set_default_section(\"core/process/deberta/v2\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-deberta-v2\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_deberta_v2_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/deberta/#debertav2forclassification","title":"DebertaV2ForClassification","text":"<p>Tip</p> <p><code>core/model/classification/deberta/v2</code> is the section for configuration of DebertaV2ForClassification.</p> <p>             Bases: <code>DebertaV2ForClassification</code></p> <p>Deberta V2 model for classification tasks.</p> <p>Initialize the DebertaV2ForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/deberta/modeling_v2.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the DebertaV2ForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaV2ForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass on the DebertaV2ForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing the input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output of the classification model.</p> Source code in <code>src/unitorch/cli/models/deberta/modeling_v2.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass on the DebertaV2ForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing the input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs tensor. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The output of the classification model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/deberta/#unitorch.cli.models.deberta.DebertaV2ForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of DebertaV2ForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DebertaV2ForClassification</code> <p>An instance of DebertaV2ForClassification.</p> Source code in <code>src/unitorch/cli/models/deberta/modeling_v2.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/deberta/v2\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of DebertaV2ForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        DebertaV2ForClassification: An instance of DebertaV2ForClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/deberta/v2\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-deberta-v2\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_deberta_v2_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_deberta_v2_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/diffusers/","title":"unitorch.cli.models.diffusers","text":""},{"location":"cli/models/diffusers/#stableprocessor","title":"StableProcessor","text":"<p>Tip</p> <p><code>core/process/diffusers/stable</code> is the section for configuration of BartProcessor.</p> <p>             Bases: <code>StableProcessor</code></p> Source code in <code>src/unitorch/cli/models/diffusers/processing_stable.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n    random_flip: Optional[bool] = False,\n):\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        vae_config_path=vae_config_path,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n        pad_token=pad_token,\n        image_size=image_size,\n        center_crop=center_crop,\n        random_flip=random_flip,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablefortext2imagegeneration","title":"StableForText2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/text2image/stable</code> is the section for configuration of StableForText2ImageGeneration.</p> <p>             Bases: <code>StableForText2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stableforimage2imagegeneration","title":"StableForImage2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/image2image/stable</code> is the section for configuration of StableForImage2ImageGeneration.</p> <p>             Bases: <code>StableForImage2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stableforimageinpainting","title":"StableForImageInpainting","text":"<p>Tip</p> <p><code>core/model/diffusers/inpainting/stable</code> is the section for configuration of StableForImageInpainting.</p> <p>             Bases: <code>StableForImageInpainting</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stableforimageresolution","title":"StableForImageResolution","text":"<p>Tip</p> <p><code>core/model/diffusers/resolution/stable</code> is the section for configuration of StableForImageResolution.</p> <p>             Bases: <code>StableForImageResolution</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlprocessor","title":"StableXLProcessor","text":"<p>Tip</p> <p><code>core/process/diffusers/stable_xl</code> is the section for configuration of StableXLProcessor.</p> <p>             Bases: <code>StableXLProcessor</code></p> Source code in <code>src/unitorch/cli/models/diffusers/processing_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n):\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        vocab2_path=vocab2_path,\n        merge2_path=merge2_path,\n        vae_config_path=vae_config_path,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n        pad_token=pad_token,\n        pad_token2=pad_token2,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlfortext2imagegeneration","title":"StableXLForText2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/text2image/stable_xl</code> is the section for configuration of StableXLForText2ImageGeneration.</p> <p>             Bases: <code>StableXLForText2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlforimage2imagegeneration","title":"StableXLForImage2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/image2image/stable_xl</code> is the section for configuration of StableXLForImage2ImageGeneration.</p> <p>             Bases: <code>StableXLForImage2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlforimageinpainting","title":"StableXLForImageInpainting","text":"<p>Tip</p> <p><code>core/model/diffusers/inpainting/stable_xl</code> is the section for configuration of StableXLForImageInpainting.</p> <p>             Bases: <code>StableXLForImageInpainting</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlrefinerprocessor","title":"StableXLRefinerProcessor","text":"<p>Tip</p> <p><code>core/process/diffusers/stable_xl_refiner</code> is the section for configuration of StableXLRefinerProcessor.</p> <p>             Bases: <code>StableXLRefinerProcessor</code></p> Source code in <code>src/unitorch/cli/models/diffusers/processing_stable_xl_refiner.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    vae_config_path: str,\n    refiner_vocab_path: Optional[str] = None,\n    refiner_merge_path: Optional[str] = None,\n    refiner_vocab2_path: Optional[str] = None,\n    refiner_merge2_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n    refiner_pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    refiner_pad_token2: Optional[str] = \"!\",\n):\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        vocab2_path=vocab2_path,\n        merge2_path=merge2_path,\n        refiner_vocab_path=refiner_vocab_path,\n        refiner_merge_path=refiner_merge_path,\n        refiner_vocab2_path=refiner_vocab2_path,\n        refiner_merge2_path=refiner_merge2_path,\n        vae_config_path=vae_config_path,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n        pad_token=pad_token,\n        pad_token2=pad_token2,\n        refiner_pad_token=refiner_pad_token,\n        refiner_pad_token2=refiner_pad_token2,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#stablexlrefinerfortext2imagegeneration","title":"StableXLRefinerForText2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/text2image/stable_xl_refiner</code> is the section for configuration of StableXLRefinerForText2ImageGeneration.</p> <p>             Bases: <code>StableXLRefinerForText2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_stable_xl_refiner.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    refiner_config_path: Optional[str] = None,\n    refiner_text_config_path: Optional[str] = None,\n    refiner_text2_config_path: Optional[str] = None,\n    refiner_vae_config_path: Optional[str] = None,\n    refiner_scheduler_config_path: Optional[str] = None,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        refiner_config_path=refiner_config_path,\n        refiner_text_config_path=refiner_text_config_path,\n        refiner_text2_config_path=refiner_text2_config_path,\n        refiner_vae_config_path=refiner_vae_config_path,\n        refiner_scheduler_config_path=refiner_scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#controlnetprocessor","title":"ControlNetProcessor","text":"<p>Tip</p> <p><code>core/process/diffusers/controlnet</code> is the section for configuration of ControlNetProcessor.</p> <p>             Bases: <code>ControlNetProcessor</code></p> Source code in <code>src/unitorch/cli/models/diffusers/processing_controlnet.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n):\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        vae_config_path=vae_config_path,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n        pad_token=pad_token,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#controlnetfortext2imagegeneration","title":"ControlNetForText2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/text2image/controlnet</code> is the section for configuration of ControlNetForText2ImageGeneration.</p> <p>             Bases: <code>ControlNetForText2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#controlnetforimage2imagegeneration","title":"ControlNetForImage2ImageGeneration","text":"<p>Tip</p> <p><code>core/model/diffusers/image2image/controlnet</code> is the section for configuration of ControlNetForImage2ImageGeneration.</p> <p>             Bases: <code>ControlNetForImage2ImageGeneration</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/diffusers/#controlnetforimageinpainting","title":"ControlNetForImageInpainting","text":"<p>Tip</p> <p><code>core/model/diffusers/inpainting/controlnet</code> is the section for configuration of ControlNetForImageInpainting.</p> <p>             Bases: <code>ControlNetForImageInpainting</code></p> Source code in <code>src/unitorch/cli/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        seed=seed,\n    )\n</code></pre>"},{"location":"cli/models/llama/","title":"unitorch.cli.models.llama","text":""},{"location":"cli/models/llama/#llamaprocessor","title":"LlamaProcessor","text":"<p>Tip</p> <p><code>core/process/llama</code> is the section for configuration of LlamaProcessor.</p> <p>             Bases: <code>LlamaProcessor</code></p> <p>Processor for Llama models.</p> <p>Initialize the LlamaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generated sequence length. Defaults to 128.</p> <code>128</code> Source code in <code>src/unitorch/cli/models/llama/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 128,\n):\n    \"\"\"\n    Initialize the LlamaProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generated sequence length. Defaults to 128.\n    \"\"\"\n    super().__init__(\n        vocab_file=vocab_path,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaProcessor</code> <p>An instance of LlamaProcessor.</p> Source code in <code>src/unitorch/cli/models/llama/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaProcessor: An instance of LlamaProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/llama/#llamaforclassification","title":"LlamaForClassification","text":"<p>Tip</p> <p><code>core/model/classification/llama</code> is the section for configuration of LlamaForClassification.</p> <p>             Bases: <code>LlamaForClassification</code></p> <p>Llama model for classification tasks.</p> <p>Initialize the LlamaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the LlamaForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        quant_config_path=quant_config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass on the LlamaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing the input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output of the classification model.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass on the LlamaForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing the input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs tensor. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The output of the classification model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaForClassification</code> <p>An instance of LlamaForClassification.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaForClassification: An instance of LlamaForClassification.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path,\n        quant_config_path=quant_config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/llama/#llamaforpretrain","title":"LlamaForPretrain","text":"<p>Tip</p> <p><code>core/model/pretrain/llama</code> is the section for configuration of LlamaForPretrain.</p> <p>             Bases: <code>LlamaForPretrain</code></p> <p>Llama model for pretraining tasks.</p> <p>Initialize the LlamaForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the LlamaForPretrain model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        quant_config_path=quant_config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    input_ids_label: Optional[torch.Tensor] = None,\n    attention_mask_label: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass on the LlamaForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing the input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <code>input_ids_label</code> <code>Tensor</code> <p>The input tensor containing the input IDs for the masked language model. Defaults to None.</p> <code>None</code> <code>attention_mask_label</code> <code>Tensor</code> <p>The attention mask tensor for the masked language model. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LossOutputs</code> <p>The output of the pretraining model.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    input_ids_label: Optional[torch.Tensor] = None,\n    attention_mask_label: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass on the LlamaForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing the input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs tensor. Defaults to None.\n        input_ids_label (torch.Tensor, optional): The input tensor containing the input IDs for the masked language model. Defaults to None.\n        attention_mask_label (torch.Tensor, optional): The attention mask tensor for the masked language model. Defaults to None.\n\n    Returns:\n        LossOutputs: The output of the pretraining model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        input_ids_label=input_ids_label,\n        attention_mask_label=attention_mask_label,\n    )\n    return LossOutputs(loss=outputs)\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForPretrain.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaForPretrain from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaForPretrain</code> <p>An instance of LlamaForPretrain.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/pretrain/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaForPretrain from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaForPretrain: An instance of LlamaForPretrain.\n    \"\"\"\n    config.set_default_section(\"core/model/pretrain/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path,\n        quant_config_path=quant_config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/llama/#llamaforgeneration","title":"LlamaForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/llama</code> is the section for configuration of LlamaForGeneration.</p> <p>             Bases: <code>LlamaForGeneration</code></p> <p>Llama model for generation tasks.</p> <p>Initialize the LlamaForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the LlamaForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        quant_config_path=quant_config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass on the LlamaForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing the input IDs. Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The output of the generation model.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass on the LlamaForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor, optional): The input tensor containing the input IDs. Defaults to None.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position IDs tensor. Defaults to None.\n\n    Returns:\n        GenerationOutputs: The output of the generation model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaForGeneration</code> <p>An instance of LlamaForGeneration.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaForGeneration: An instance of LlamaForGeneration.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path,\n        quant_config_path=quant_config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/llama/#unitorch.cli.models.llama.LlamaForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 1.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/llama/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/llama\")\n@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the Llama model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 1.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/mbart/","title":"unitorch.cli.models.mbart","text":""},{"location":"cli/models/mbart/#mbartprocessor","title":"MBartProcessor","text":"<p>Tip</p> <p><code>core/process/mbart</code> is the section for configuration of MBartProcessor.</p> <p>             Bases: <code>MBartProcessor</code></p> <p>Processor for the MBart model.</p> <p>Initialize the MBartProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum input sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generated sequence length. Defaults to 48.</p> <code>48</code> <code>special_input_ids</code> <code>Dict</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> Source code in <code>src/unitorch/cli/models/mbart/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n    special_input_ids: Optional[Dict] = dict(),\n):\n    \"\"\"\n    Initialize the MBartProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum input sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generated sequence length. Defaults to 48.\n        special_input_ids (Dict, optional): Special input IDs. Defaults to an empty dictionary.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        special_input_ids=special_input_ids,\n    )\n</code></pre>"},{"location":"cli/models/mbart/#unitorch.cli.models.mbart.MBartProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of MBartProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MBartProcessor</code> <p>An instance of MBartProcessor.</p> Source code in <code>src/unitorch/cli/models/mbart/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/mbart\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of MBartProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        MBartProcessor: An instance of MBartProcessor.\n    \"\"\"\n    config.set_default_section(\"core/process/mbart\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-mbart\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_mbart_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/mbart/#mbartforgeneration","title":"MBartForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/mbart</code> is the section for configuration of MBartForGeneration.</p> <p>             Bases: <code>MBartForGeneration</code></p> <p>MBart model for generation tasks.</p> <p>Initialize the MBartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>freeze_input_embedding</code> <code>bool</code> <p>Whether to freeze the input embeddings. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/mbart/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    freeze_input_embedding: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the MBartForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        freeze_input_embedding (bool, optional): Whether to freeze the input embeddings. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        freeze_input_embedding=freeze_input_embedding,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/mbart/#unitorch.cli.models.mbart.MBartForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/mbart/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/mbart/#unitorch.cli.models.mbart.MBartForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of MBartForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MBartForGeneration</code> <p>An instance of MBartForGeneration.</p> Source code in <code>src/unitorch/cli/models/mbart/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/mbart\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of MBartForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        MBartForGeneration: An instance of MBartForGeneration.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/mbart\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-mbart\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_mbart_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    freeze_input_embedding = config.getoption(\"freeze_input_embedding\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path,\n        freeze_input_embedding=freeze_input_embedding,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_mbart_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/mbart/#unitorch.cli.models.mbart.MBartForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the MBart model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>2</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/mbart/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/mbart\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the MBart model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/minigpt4/","title":"unitorch.cli.models.minigpt4","text":""},{"location":"cli/models/minigpt4/#minigpt4blip2llamaprocessor","title":"MiniGPT4Blip2LlamaProcessor","text":"<p>             Bases: <code>MiniGPT4Blip2LlamaProcessor</code></p> <p>MiniGPT4Blip2LlamaProcessor is a class for processing inputs and outputs of the MiniGPT4 model with Blip2 and Llama. It inherits from the _MiniGPT4Blip2LlamaProcessor class.</p> <p>Initializes a MiniGPT4Blip2LlamaProcessor instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The file path to the vocabulary.</p> required <code>vision_config_path</code> <code>str</code> <p>The file path to the vision configuration.</p> required <code>max_prefix_seq_length</code> <code>int</code> <p>The maximum length of the prefix sequence. Defaults to 64.</p> <code>64</code> <code>max_suffix_seq_length</code> <code>int</code> <p>The maximum length of the suffix sequence. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequence. Defaults to 128.</p> <code>128</code> Source code in <code>src/unitorch/cli/models/minigpt4/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    vision_config_path: str,\n    max_prefix_seq_length: Optional[int] = 64,\n    max_suffix_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 128,\n    weight_path: Optional[Union[str, List[str]]] = None,\n    state_dict: Optional[Dict[str, Any]] = None,\n    device: Optional[Union[str, int]] = \"cpu\",\n):\n    \"\"\"\n    Initializes a MiniGPT4Blip2LlamaProcessor instance.\n\n    Args:\n        vocab_path (str): The file path to the vocabulary.\n        vision_config_path (str): The file path to the vision configuration.\n        max_prefix_seq_length (int, optional): The maximum length of the prefix sequence. Defaults to 64.\n        max_suffix_seq_length (int, optional): The maximum length of the suffix sequence. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequence. Defaults to 128.\n    \"\"\"\n    super().__init__(\n        vocab_file=vocab_path,\n        vision_config_path=vision_config_path,\n        max_prefix_seq_length=max_prefix_seq_length,\n        max_suffix_seq_length=max_suffix_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/minigpt4/#unitorch.cli.models.minigpt4.MiniGPT4Blip2LlamaProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Creates a MiniGPT4Blip2LlamaProcessor instance from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The dictionary containing the processor configuration.</p> Source code in <code>src/unitorch/cli/models/minigpt4/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/minigpt4\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Creates a MiniGPT4Blip2LlamaProcessor instance from a core configuration.\n\n    Args:\n        config: The configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The dictionary containing the processor configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/minigpt4\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-minigpt4\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_minigpt4_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(\n            pretrained_minigpt4_infos, pretrained_name, \"vision_config\"\n        ),\n    )\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/minigpt4/#minigpt4blip2llamaforgeneration","title":"MiniGPT4Blip2LlamaForGeneration","text":"<p>             Bases: <code>MiniGPT4Blip2LlamaForGeneration</code></p> <p>MiniGPT4Blip2LlamaForGeneration is a class for generating sequences using the MiniGPT4 model with Blip2 and Llama. It inherits from the _MiniGPT4Blip2LlamaForGeneration class.</p> <p>Initializes a MiniGPT4Blip2LlamaForGeneration instance.</p> <p>Parameters:</p> Name Type Description Default <code>blip2_config_path</code> <code>str</code> <p>The file path to the Blip2 configuration.</p> required <code>llama_config_path</code> <code>str</code> <p>The file path to the Llama configuration.</p> required <code>pad_token_id</code> <code>int</code> <p>The ID of the padding token. Defaults to 0.</p> <code>0</code> <code>freeze_vision_model</code> <code>bool</code> <p>Whether to freeze the vision model. Defaults to True.</p> <code>True</code> <code>freeze_qformer_model</code> <code>bool</code> <p>Whether to freeze the query transformer model. Defaults to True.</p> <code>True</code> <code>freeze_llama_model</code> <code>bool</code> <p>Whether to freeze the Llama model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/minigpt4/modeling.py</code> <pre><code>def __init__(\n    self,\n    blip2_config_path: str,\n    llama_config_path: str,\n    quant_config_path: Optional[str] = None,\n    pad_token_id: Optional[int] = 0,\n    freeze_vision_model: Optional[bool] = True,\n    freeze_qformer_model: Optional[bool] = True,\n    freeze_llama_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a MiniGPT4Blip2LlamaForGeneration instance.\n\n    Args:\n        blip2_config_path (str): The file path to the Blip2 configuration.\n        llama_config_path (str): The file path to the Llama configuration.\n        pad_token_id (int, optional): The ID of the padding token. Defaults to 0.\n        freeze_vision_model (bool, optional): Whether to freeze the vision model. Defaults to True.\n        freeze_qformer_model (bool, optional): Whether to freeze the query transformer model. Defaults to True.\n        freeze_llama_model (bool, optional): Whether to freeze the Llama model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        blip2_config_path=blip2_config_path,\n        llama_config_path=llama_config_path,\n        quant_config_path=quant_config_path,\n        pad_token_id=pad_token_id,\n        freeze_vision_model=freeze_vision_model,\n        freeze_qformer_model=freeze_qformer_model,\n        freeze_llama_model=freeze_llama_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/minigpt4/#unitorch.cli.models.minigpt4.MiniGPT4Blip2LlamaForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens.</p> required <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>The input IDs for the decoder tokens.</p> required <code>prefix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the prefix tokens. Defaults to None.</p> <code>None</code> <code>suffix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the suffix tokens. Defaults to None.</p> <code>None</code> <code>decoder_attention_mask</code> <code>Tensor</code> <p>The attention mask for the decoder tokens. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/minigpt4/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs a forward pass through the model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values.\n        prefix_input_ids (torch.Tensor): The input IDs for the prefix tokens.\n        suffix_input_ids (torch.Tensor): The input IDs for the suffix tokens.\n        decoder_input_ids (torch.Tensor): The input IDs for the decoder tokens.\n        prefix_attention_mask (torch.Tensor, optional): The attention mask for the prefix tokens.\n            Defaults to None.\n        suffix_attention_mask (torch.Tensor, optional): The attention mask for the suffix tokens.\n            Defaults to None.\n        decoder_attention_mask (torch.Tensor, optional): The attention mask for the decoder tokens.\n            Defaults to None.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        pixel_values=pixel_values,\n        prefix_input_ids=prefix_input_ids,\n        suffix_input_ids=suffix_input_ids,\n        decoder_input_ids=decoder_input_ids,\n        prefix_attention_mask=prefix_attention_mask,\n        suffix_attention_mask=suffix_attention_mask,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/minigpt4/#unitorch.cli.models.minigpt4.MiniGPT4Blip2LlamaForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Creates a MiniGPT4Blip2LlamaForGeneration instance from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The configuration object.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MiniGPT4Blip2LlamaForGeneration</code> <p>The created instance.</p> Source code in <code>src/unitorch/cli/models/minigpt4/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/minigpt4\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Creates a MiniGPT4Blip2LlamaForGeneration instance from a core configuration.\n\n    Args:\n        config: The configuration object.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        MiniGPT4Blip2LlamaForGeneration: The created instance.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/minigpt4\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-minigpt4\")\n\n    blip2_config_path = config.getoption(\"blip2_config_path\", None)\n    blip2_config_path = pop_value(\n        blip2_config_path,\n        nested_dict_value(\n            pretrained_minigpt4_infos, pretrained_name, \"blip2_config_path\"\n        ),\n    )\n    blip2_config_path = cached_path(blip2_config_path)\n\n    llama_config_path = config.getoption(\"llama_config_path\", None)\n    llama_config_path = pop_value(\n        llama_config_path,\n        nested_dict_value(\n            pretrained_minigpt4_infos, pretrained_name, \"llama_config_path\"\n        ),\n    )\n    llama_config_path = cached_path(llama_config_path)\n\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n\n    freeze_vision_model = config.getoption(\"freeze_vision_model\", True)\n    freeze_qformer_model = config.getoption(\"freeze_qformer_model\", True)\n    freeze_llama_model = config.getoption(\"freeze_llama_model\", True)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        blip2_config_path,\n        llama_config_path,\n        quant_config_path=quant_config_path,\n        freeze_vision_model=freeze_vision_model,\n        freeze_qformer_model=freeze_qformer_model,\n        freeze_llama_model=freeze_llama_model,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_minigpt4_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/minigpt4/#unitorch.cli.models.minigpt4.MiniGPT4Blip2LlamaForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens.</p> required <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 1.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>The number of sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum generated sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generated sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of the n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling instead of beam search. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature value for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/minigpt4/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/minigpt4\")\n@torch.no_grad()\ndef generate(\n    self,\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values.\n        prefix_input_ids (torch.Tensor): The input IDs for the prefix tokens.\n        suffix_input_ids (torch.Tensor): The input IDs for the suffix tokens.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 1.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s).\n            Defaults to 2.\n        num_return_sequences (int, optional): The number of sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum generated sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum generated sequence length. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of the n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search.\n            Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling instead of beam search. Defaults to False.\n        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        pixel_values=pixel_values,\n        prefix_input_ids=prefix_input_ids,\n        suffix_input_ids=suffix_input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/mt5/","title":"unitorch.cli.models.mt5","text":""},{"location":"cli/models/mt5/#mt5processor","title":"MT5Processor","text":"<p>Tip</p> <p><code>core/process/mt5</code> is the section for configuration of MT5Processor.</p> <p>             Bases: <code>MT5Processor</code></p> <p>Processor for MT5 models.</p> <p>Initialize the MT5Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generation sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/cli/models/mt5/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize the MT5Processor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Dict, optional): Special input IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generation sequence length. Defaults to 48.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        special_input_ids=special_input_ids,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/mt5/#unitorch.cli.models.mt5.MT5Processor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of MT5Processor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the arguments for initializing the MT5Processor.</p> Source code in <code>src/unitorch/cli/models/mt5/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/mt5\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of MT5Processor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the arguments for initializing the MT5Processor.\n    \"\"\"\n    config.set_default_section(\"core/process/mt5\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-mt5\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_mt5_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/mt5/#mt5forgeneration","title":"MT5ForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/mt5</code> is the section for configuration of MT5ForGeneration.</p> <p>             Bases: <code>MT5ForGeneration</code></p> <p>MT5 model for generation tasks.</p> <p>Initialize the MT5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/mt5/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the MT5ForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path, gradient_checkpointing=gradient_checkpointing\n    )\n</code></pre>"},{"location":"cli/models/mt5/#unitorch.cli.models.mt5.MT5ForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/mt5/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/mt5/#unitorch.cli.models.mt5.MT5ForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of MT5ForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>MT5ForGeneration</code> <p>An instance of MT5ForGeneration.</p> Source code in <code>src/unitorch/cli/models/mt5/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/mt5\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of MT5ForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        MT5ForGeneration: An instance of MT5ForGeneration.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/mt5\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-mt5\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_mt5_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_mt5_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/mt5/#unitorch.cli.models.mt5.MT5ForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the MT5 model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/mt5/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/mt5\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the MT5 model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/peft/","title":"unitorch.cli.models.peft","text":""},{"location":"cli/models/peft/#bloomloraforclassification","title":"BloomLoraForClassification","text":"<p>Tip</p> <p><code>classification/peft/lora/bloom</code> is the section for configuration of BloomLoraForClassification.</p> <p>             Bases: <code>BloomLoraForClassification</code></p> <p>Initialize the BloomLoraForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>lora_r</code> <code>int</code> <p>The number of Lora ranks. Defaults to 16.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>The Lora alpha value. Defaults to 32.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>The Lora dropout rate. Defaults to 0.05.</p> <code>0.05</code> <code>fan_in_fan_out</code> <code>bool</code> <p>Whether to use fan-in/fan-out weight initialization. Defaults to True.</p> <code>True</code> <code>target_modules</code> <code>Union[List[str], str]</code> <p>The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].</p> <code>['query_key_value']</code> <code>num_classes</code> <code>int</code> <p>The number of classes. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"query_key_value\"],\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BloomLoraForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        lora_r (int, optional): The number of Lora ranks. Defaults to 16.\n        lora_alpha (int, optional): The Lora alpha value. Defaults to 32.\n        lora_dropout (float, optional): The Lora dropout rate. Defaults to 0.05.\n        fan_in_fan_out (bool, optional): Whether to use fan-in/fan-out weight initialization. Defaults to True.\n        target_modules (Union[List[str], str], optional): The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].\n        num_classes (int, optional): The number of classes. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.BloomLoraForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the BloomLoraForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output of the classification task.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the BloomLoraForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask.\n        position_ids (torch.Tensor, optional): The position IDs.\n\n    Returns:\n        ClassificationOutputs: The output of the classification task.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.BloomLoraForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomLoraForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BloomLoraForClassification</code> <p>The initialized BloomLoraForClassification instance.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/peft/lora/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomLoraForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BloomLoraForClassification: The initialized BloomLoraForClassification instance.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/peft/lora/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"config\"),\n    )\n    config_path = cached_path(config_path)\n\n    lora_r = config.getoption(\"lora_r\", 16)\n    lora_alpha = config.getoption(\"lora_alpha\", 32)\n    lora_dropout = config.getoption(\"lora_dropout\", 0.05)\n    fan_in_fan_out = config.getoption(\"fan_in_fan_out\", True)\n    target_modules = config.getoption(\"target_modules\", [\"query_key_value\"])\n\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n\n    weight_path = []\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    pretrained_weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if pretrained_weight_path is not None:\n        if isinstance(pretrained_weight_path, str):\n            weight_path.append(pretrained_weight_path)\n        elif isinstance(pretrained_weight_path, list):\n            weight_path.extend(pretrained_weight_path)\n\n    pretrained_lora_weight_path = config.getoption(\n        \"pretrained_lora_weight_path\", None\n    )\n    if pretrained_lora_weight_path is not None:\n        weight_path.append(pretrained_lora_weight_path)\n\n    if len(weight_path) &gt; 0:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/peft/#bloomloraforgeneration","title":"BloomLoraForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/peft/lora/bloom</code> is the section for configuration of BloomLoraForGeneration.</p> <p>             Bases: <code>BloomLoraForGeneration</code></p> <p>BloomLora model for generation tasks.</p> <p>Initialize the BloomLoraForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>lora_r</code> <code>int</code> <p>The number of Lora ranks. Defaults to 16.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>The Lora alpha value. Defaults to 32.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>The Lora dropout rate. Defaults to 0.05.</p> <code>0.05</code> <code>fan_in_fan_out</code> <code>bool</code> <p>Whether to use fan-in/fan-out weight initialization. Defaults to True.</p> <code>True</code> <code>target_modules</code> <code>Union[List[str], str]</code> <p>The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].</p> <code>['query_key_value']</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"query_key_value\"],\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the BloomLoraForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        lora_r (int, optional): The number of Lora ranks. Defaults to 16.\n        lora_alpha (int, optional): The Lora alpha value. Defaults to 32.\n        lora_dropout (float, optional): The Lora dropout rate. Defaults to 0.05.\n        fan_in_fan_out (bool, optional): Whether to use fan-in/fan-out weight initialization. Defaults to True.\n        target_modules (Union[List[str], str], optional): The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.BloomLoraForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the BloomLoraForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The output of the generation task.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the BloomLoraForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor, optional): The input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask.\n        position_ids (torch.Tensor, optional): The position IDs.\n\n    Returns:\n        GenerationOutputs: The output of the generation task.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.BloomLoraForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of BloomLoraForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BloomLoraForGeneration</code> <p>The initialized BloomLoraForGeneration instance.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/peft/lora/bloom\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of BloomLoraForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        BloomLoraForGeneration: The initialized BloomLoraForGeneration instance.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/peft/lora/bloom\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-bloom\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"config\"),\n    )\n    config_path = cached_path(config_path)\n\n    lora_r = config.getoption(\"lora_r\", 16)\n    lora_alpha = config.getoption(\"lora_alpha\", 32)\n    lora_dropout = config.getoption(\"lora_dropout\", 0.05)\n    fan_in_fan_out = config.getoption(\"fan_in_fan_out\", True)\n    target_modules = config.getoption(\"target_modules\", [\"query_key_value\"])\n\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n\n    weight_path = []\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    pretrained_weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_bloom_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if pretrained_weight_path is not None:\n        if isinstance(pretrained_weight_path, str):\n            weight_path.append(pretrained_weight_path)\n        elif isinstance(pretrained_weight_path, list):\n            weight_path.extend(pretrained_weight_path)\n\n    pretrained_lora_weight_path = config.getoption(\n        \"pretrained_lora_weight_path\", None\n    )\n    if pretrained_lora_weight_path is not None:\n        weight_path.append(pretrained_lora_weight_path)\n\n    if len(weight_path) &gt; 0:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.BloomLoraForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the Bloom model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_bloom.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/peft/lora/bloom\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the Bloom model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/peft/#llamaloraforclassification","title":"LlamaLoraForClassification","text":"<p>Tip</p> <p><code>core/model/classification/peft/lora/llama</code> is the section for configuration of LlamaLoraForClassification.</p> <p>             Bases: <code>LlamaLoraForClassification</code></p> <p>LlamaLora model for classification tasks.</p> <p>Initialize the LlamaLoraForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>lora_r</code> <code>int</code> <p>The number of Lora ranks. Defaults to 16.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>The Lora alpha value. Defaults to 32.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>The Lora dropout rate. Defaults to 0.05.</p> <code>0.05</code> <code>fan_in_fan_out</code> <code>bool</code> <p>Whether to use fan-in/fan-out weight initialization. Defaults to True.</p> <code>True</code> <code>target_modules</code> <code>Union[List[str], str]</code> <p>The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].</p> <code>['q_proj', 'v_proj']</code> <code>num_classes</code> <code>int</code> <p>The number of classes. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"q_proj\", \"v_proj\"],\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the LlamaLoraForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        lora_r (int, optional): The number of Lora ranks. Defaults to 16.\n        lora_alpha (int, optional): The Lora alpha value. Defaults to 32.\n        lora_dropout (float, optional): The Lora dropout rate. Defaults to 0.05.\n        fan_in_fan_out (bool, optional): Whether to use fan-in/fan-out weight initialization. Defaults to True.\n        target_modules (Union[List[str], str], optional): The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].\n        num_classes (int, optional): The number of classes. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        quant_config_path=quant_config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.LlamaLoraForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the LlamaLoraForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output of the classification task.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the LlamaLoraForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask.\n        position_ids (torch.Tensor, optional): The position IDs.\n\n    Returns:\n        ClassificationOutputs: The output of the classification task.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.LlamaLoraForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaLoraForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaLoraForClassification</code> <p>The initialized LlamaLoraForClassification instance.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/peft/lora/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaLoraForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaLoraForClassification: The initialized LlamaLoraForClassification instance.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/peft/lora/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"config\"),\n    )\n    config_path = cached_path(config_path)\n\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n\n    lora_r = config.getoption(\"lora_r\", 16)\n    lora_alpha = config.getoption(\"lora_alpha\", 32)\n    lora_dropout = config.getoption(\"lora_dropout\", 0.05)\n    fan_in_fan_out = config.getoption(\"fan_in_fan_out\", True)\n    target_modules = config.getoption(\"target_modules\", [\"q_proj\", \"v_proj\"])\n\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path,\n        quant_config_path=quant_config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n\n    weight_path = []\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    pretrained_weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if pretrained_weight_path is not None:\n        if isinstance(pretrained_weight_path, str):\n            weight_path.append(pretrained_weight_path)\n        elif isinstance(pretrained_weight_path, list):\n            weight_path.extend(pretrained_weight_path)\n\n    pretrained_lora_weight_path = config.getoption(\n        \"pretrained_lora_weight_path\", None\n    )\n    if pretrained_lora_weight_path is not None:\n        weight_path.append(pretrained_lora_weight_path)\n\n    if len(weight_path) &gt; 0:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/peft/#llamaloraforgeneration","title":"LlamaLoraForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/peft/lora/llama</code> is the section for configuration of LlamaLoraForGeneration.</p> <p>             Bases: <code>LlamaLoraForGeneration</code></p> <p>LlamaLora model for generation tasks.</p> <p>Initialize the LlamaLoraForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>lora_r</code> <code>int</code> <p>The number of Lora ranks. Defaults to 16.</p> <code>16</code> <code>lora_alpha</code> <code>int</code> <p>The Lora alpha value. Defaults to 32.</p> <code>32</code> <code>lora_dropout</code> <code>float</code> <p>The Lora dropout rate. Defaults to 0.05.</p> <code>0.05</code> <code>fan_in_fan_out</code> <code>bool</code> <p>Whether to use fan-in/fan-out weight initialization. Defaults to True.</p> <code>True</code> <code>target_modules</code> <code>Union[List[str], str]</code> <p>The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].</p> <code>['q_proj', 'v_proj']</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"q_proj\", \"v_proj\"],\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the LlamaLoraForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        lora_r (int, optional): The number of Lora ranks. Defaults to 16.\n        lora_alpha (int, optional): The Lora alpha value. Defaults to 32.\n        lora_dropout (float, optional): The Lora dropout rate. Defaults to 0.05.\n        fan_in_fan_out (bool, optional): Whether to use fan-in/fan-out weight initialization. Defaults to True.\n        target_modules (Union[List[str], str], optional): The target modules for Lora regularization. Defaults to [\"q_proj\", \"v_proj\"].\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        quant_config_path=quant_config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.LlamaLoraForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform forward pass of the LlamaLoraForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The output of the generation task.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform forward pass of the LlamaLoraForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor, optional): The input IDs.\n        attention_mask (torch.Tensor, optional): The attention mask.\n        position_ids (torch.Tensor, optional): The position IDs.\n\n    Returns:\n        GenerationOutputs: The output of the generation task.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.LlamaLoraForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of LlamaLoraForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LlamaLoraForGeneration</code> <p>The initialized LlamaLoraForGeneration instance.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/peft/lora/llama\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of LlamaLoraForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        LlamaLoraForGeneration: The initialized LlamaLoraForGeneration instance.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/peft/lora/llama\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-llama\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"config\"),\n    )\n    config_path = cached_path(config_path)\n    quant_config_path = config.getoption(\"quant_config_path\", None)\n    if quant_config_path is not None:\n        quant_config_path = cached_path(quant_config_path)\n\n    lora_r = config.getoption(\"lora_r\", 16)\n    lora_alpha = config.getoption(\"lora_alpha\", 32)\n    lora_dropout = config.getoption(\"lora_dropout\", 0.05)\n    fan_in_fan_out = config.getoption(\"fan_in_fan_out\", True)\n    target_modules = config.getoption(\"target_modules\", [\"q_proj\", \"v_proj\"])\n\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(\n        config_path,\n        quant_config_path=quant_config_path,\n        lora_r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n\n    weight_path = []\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    pretrained_weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_llama_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if pretrained_weight_path is not None:\n        if isinstance(pretrained_weight_path, str):\n            weight_path.append(pretrained_weight_path)\n        elif isinstance(pretrained_weight_path, list):\n            weight_path.extend(pretrained_weight_path)\n\n    pretrained_lora_weight_path = config.getoption(\n        \"pretrained_lora_weight_path\", None\n    )\n    if pretrained_lora_weight_path is not None:\n        weight_path.append(pretrained_lora_weight_path)\n\n    if len(weight_path) &gt; 0:\n        inst.from_pretrained(\n            weight_path=weight_path,\n        )\n\n    return inst\n</code></pre>"},{"location":"cli/models/peft/#unitorch.cli.models.peft.LlamaLoraForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the Llama model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/peft/modeling_llama.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/peft/lora/llama\")\n@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the Llama model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/pegasus/","title":"unitorch.cli.models.pegasus","text":""},{"location":"cli/models/pegasus/#pegasusprocessor","title":"PegasusProcessor","text":"<p>Tip</p> <p><code>core/process/pegasus</code> is the section for configuration of PegasusProcessor.</p> <p>             Bases: <code>PegasusProcessor</code></p> <p>Processor for the Pegasus model.</p> <p>Initialize the PegasusProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>Dictionary of special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generated sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/cli/models/pegasus/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize the PegasusProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Dict, optional): Dictionary of special input IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generated sequence length. Defaults to 48.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        special_input_ids=special_input_ids,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/pegasus/#unitorch.cli.models.pegasus.PegasusProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of PegasusProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PegasusProcessor</code> <p>The initialized PegasusProcessor instance.</p> Source code in <code>src/unitorch/cli/models/pegasus/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/pegasus\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of PegasusProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        PegasusProcessor: The initialized PegasusProcessor instance.\n    \"\"\"\n    config.set_default_section(\"core/process/pegasus\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-pegasus\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_pegasus_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/pegasus/#pegasusforgeneration","title":"PegasusForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/pegasus</code> is the section for configuration of PegasusForGeneration.</p> <p>             Bases: <code>PegasusForGeneration</code></p> <p>Pegasus model for generation tasks.</p> <p>Initialize the PegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/pegasus/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the PegasusForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path, gradient_checkpointing=gradient_checkpointing\n    )\n</code></pre>"},{"location":"cli/models/pegasus/#unitorch.cli.models.pegasus.PegasusForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/pegasus/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/pegasus/#unitorch.cli.models.pegasus.PegasusForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of PegasusForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>PegasusForGeneration</code> <p>The initialized PegasusForGeneration instance.</p> Source code in <code>src/unitorch/cli/models/pegasus/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/pegasus\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of PegasusForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        PegasusForGeneration: The initialized PegasusForGeneration instance.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/pegasus\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-pegasus\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_pegasus_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_pegasus_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/pegasus/#unitorch.cli.models.pegasus.PegasusForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the Pegasus model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/pegasus/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/pegasus\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the Pegasus model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/roberta/","title":"unitorch.cli.models.roberta","text":""},{"location":"cli/models/roberta/#robertaprocessor","title":"RobertaProcessor","text":"<p>Tip</p> <p><code>core/process/roberta</code> is the section for configuration of RobertaProcessor.</p> <p>             Bases: <code>RobertaProcessor</code></p> <p>Processor for the Roberta model.</p> <p>Initialize the RobertaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the merge file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>The source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>The target type ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/cli/models/roberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    merge_path,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initialize the RobertaProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the merge file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        source_type_id (int, optional): The source type ID. Defaults to 0.\n        target_type_id (int, optional): The target type ID. Defaults to 0.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        merge_path=merge_path,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n    )\n</code></pre>"},{"location":"cli/models/roberta/#unitorch.cli.models.roberta.RobertaProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of RobertaProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RobertaProcessor</code> <p>The initialized RobertaProcessor instance.</p> Source code in <code>src/unitorch/cli/models/roberta/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/roberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of RobertaProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RobertaProcessor: The initialized RobertaProcessor instance.\n    \"\"\"\n    config.set_default_section(\"core/process/roberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-roberta\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_roberta_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    merge_path = config.getoption(\"merge_path\", None)\n    merge_path = pop_value(\n        merge_path,\n        nested_dict_value(pretrained_roberta_infos, pretrained_name, \"merge\"),\n    )\n    merge_path = cached_path(merge_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n        \"merge_path\": merge_path,\n    }\n</code></pre>"},{"location":"cli/models/roberta/#robertaforclassification","title":"RobertaForClassification","text":"<p>Tip</p> <p><code>core/model/classification/roberta</code> is the section for configuration of RobertaForClassification.</p> <p>             Bases: <code>RobertaForClassification</code></p> <p>Roberta model for classification tasks.</p> <p>Initialize the RobertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of output classes. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the RobertaForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of output classes. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/roberta/#unitorch.cli.models.roberta.RobertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/roberta/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/roberta/#unitorch.cli.models.roberta.RobertaForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of RobertaForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>RobertaForClassification</code> <p>The initialized RobertaForClassification instance.</p> Source code in <code>src/unitorch/cli/models/roberta/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/roberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of RobertaForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        RobertaForClassification: The initialized RobertaForClassification instance.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/roberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-roberta\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_roberta_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_roberta_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/swin/","title":"unitorch.cli.models.swin","text":""},{"location":"cli/models/swin/#swinprocessor","title":"SwinProcessor","text":"<p>Tip</p> <p><code>core/process/swin</code> is the section for configuration of SwinProcessor.</p> <p>             Bases: <code>SwinProcessor</code></p> <p>Swin Transformer processor for image tasks.</p> <p>Initialize the SwinProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>The path to the vision model configuration file.</p> required Source code in <code>src/unitorch/cli/models/swin/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initialize the SwinProcessor.\n\n    Args:\n        vision_config_path (str): The path to the vision model configuration file.\n    \"\"\"\n    super().__init__(\n        vision_config_path=vision_config_path,\n    )\n</code></pre>"},{"location":"cli/models/swin/#unitorch.cli.models.swin.SwinProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of SwinProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The SwinProcessor configuration.</p> Source code in <code>src/unitorch/cli/models/swin/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/swin\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of SwinProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The SwinProcessor configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/swin\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-swin\")\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(pretrained_swin_infos, pretrained_name, \"vision_config\"),\n    )\n\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/swin/#swinforimageclassification","title":"SwinForImageClassification","text":"<p>Tip</p> <p><code>core/model/classification/swin</code> is the section for configuration of SwinForImageClassification.</p> <p>             Bases: <code>SwinForImageClassification</code></p> <p>Swin Transformer model for image classification.</p> <p>Initialize the SwinForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/cli/models/swin/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initialize the SwinForImageClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (Optional[int]): The number of classes for classification. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n</code></pre>"},{"location":"cli/models/swin/#unitorch.cli.models.swin.SwinForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Perform forward pass of the SwinForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input pixel values of shape (batch_size, channels, height, width).</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The model outputs.</p> Source code in <code>src/unitorch/cli/models/swin/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Perform forward pass of the SwinForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The input pixel values of shape (batch_size, channels, height, width).\n\n    Returns:\n        ClassificationOutputs: The model outputs.\n    \"\"\"\n    outputs = super().forward(pixel_values=pixel_values)\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/swin/#unitorch.cli.models.swin.SwinForImageClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of SwinForImageClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>SwinForImageClassification</code> <p>An instance of the SwinForImageClassification model.</p> Source code in <code>src/unitorch/cli/models/swin/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/swin\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of SwinForImageClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        SwinForImageClassification: An instance of the SwinForImageClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/swin\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-swin\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_swin_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_swin_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/t5/","title":"unitorch.cli.models.t5","text":""},{"location":"cli/models/t5/#t5processor","title":"T5Processor","text":"<p>Tip</p> <p><code>core/process/t5</code> is the section for configuration of T5Processor.</p> <p>             Bases: <code>T5Processor</code></p> <p>T5 Processor for text generation.</p> <p>Initialize the T5Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Optional[Dict]</code> <p>A dictionary containing special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum generated sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/cli/models/t5/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize the T5Processor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Optional[Dict]): A dictionary containing special input IDs. Defaults to an empty dictionary.\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (Optional[int]): The maximum generated sequence length. Defaults to 48.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        special_input_ids=special_input_ids,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/t5/#unitorch.cli.models.t5.T5Processor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of T5Processor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the processor's configuration.</p> Source code in <code>src/unitorch/cli/models/t5/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/t5\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of T5Processor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the processor's configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/t5\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-t5\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_t5_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/t5/#t5forgeneration","title":"T5ForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/t5</code> is the section for configuration of T5ForGeneration.</p> <p>             Bases: <code>T5ForGeneration</code></p> <p>T5 model for text generation.</p> <p>Initialize the T5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/t5/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the T5ForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/t5/#unitorch.cli.models.t5.T5ForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/t5/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/t5/#unitorch.cli.models.t5.T5ForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of T5ForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>T5ForGeneration</code> <p>An instance of the T5ForGeneration model.</p> Source code in <code>src/unitorch/cli/models/t5/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/t5\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of T5ForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        T5ForGeneration: An instance of the T5ForGeneration model.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/t5\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-t5\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_t5_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_t5_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/t5/#unitorch.cli.models.t5.T5ForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the T5 model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/t5/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/t5\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the T5 model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"cli/models/visualbert/","title":"unitorch.cli.models.visualbert","text":""},{"location":"cli/models/visualbert/#visualbertprocessor","title":"VisualBertProcessor","text":"<p>Tip</p> <p><code>core/process/visualbert</code> is the section for configuration of VisualBertProcessor.</p> <p>             Bases: <code>VisualBertProcessor</code></p> <p>VisualBERT Processor for text and image inputs.</p> <p>Initialize the VisualBertProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>special_input_ids</code> <code>Optional[Dict]</code> <p>A dictionary containing special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>do_lower_case</code> <code>Optional[bool]</code> <p>Whether to convert the text to lowercase. Defaults to True.</p> <code>True</code> <code>do_basic_tokenize</code> <code>Optional[bool]</code> <p>Whether to perform basic tokenization. Defaults to True.</p> <code>True</code> <code>do_whole_word_mask</code> <code>Optional[bool]</code> <p>Whether to use whole-word masking. Defaults to True.</p> <code>True</code> <code>masked_lm_prob</code> <code>Optional[float]</code> <p>The probability of masked language model masking. Defaults to 0.15.</p> <code>0.15</code> <code>max_predictions_per_seq</code> <code>Optional[int]</code> <p>The maximum number of masked language model predictions per sequence. Defaults to 20.</p> <code>20</code> Source code in <code>src/unitorch/cli/models/visualbert/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    max_seq_length: Optional[int] = 128,\n    special_input_ids: Optional[Dict] = dict(),\n    do_lower_case: Optional[bool] = True,\n    do_basic_tokenize: Optional[bool] = True,\n    do_whole_word_mask: Optional[bool] = True,\n    masked_lm_prob: Optional[float] = 0.15,\n    max_predictions_per_seq: Optional[int] = 20,\n):\n    \"\"\"\n    Initialize the VisualBertProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to 128.\n        special_input_ids (Optional[Dict]): A dictionary containing special input IDs. Defaults to an empty dictionary.\n        do_lower_case (Optional[bool]): Whether to convert the text to lowercase. Defaults to True.\n        do_basic_tokenize (Optional[bool]): Whether to perform basic tokenization. Defaults to True.\n        do_whole_word_mask (Optional[bool]): Whether to use whole-word masking. Defaults to True.\n        masked_lm_prob (Optional[float]): The probability of masked language model masking. Defaults to 0.15.\n        max_predictions_per_seq (Optional[int]): The maximum number of masked language model predictions per sequence. Defaults to 20.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        special_input_ids=special_input_ids,\n        do_lower_case=do_lower_case,\n        do_basic_tokenize=do_basic_tokenize,\n        do_whole_word_mask=do_whole_word_mask,\n        masked_lm_prob=masked_lm_prob,\n        max_predictions_per_seq=max_predictions_per_seq,\n    )\n</code></pre>"},{"location":"cli/models/visualbert/#unitorch.cli.models.visualbert.VisualBertProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of VisualBertProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing the processor's configuration.</p> Source code in <code>src/unitorch/cli/models/visualbert/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/visualbert\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of VisualBertProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: A dictionary containing the processor's configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/visualbert\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-visualbert\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_visualbert_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/visualbert/#visualbertforclassification","title":"VisualBertForClassification","text":"<p>Tip</p> <p><code>core/model/classification/visualbert</code> is the section for configuration of VisualBertForClassification.</p> <p>             Bases: <code>VisualBertForClassification</code></p> <p>VisualBERT for Classification model.</p> <p>Initialize VisualBertForClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model's configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing to save memory during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize VisualBertForClassification.\n\n    Args:\n        config_path (str): The path to the model's configuration file.\n        num_classes (Optional[int]): The number of classes for classification.\n            Defaults to 1.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing\n            to save memory during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/visualbert/#unitorch.cli.models.visualbert.VisualBertForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the VisualBertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>token_type_ids</code> <code>Tensor</code> <p>The token type IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> required <code>visual_embeds</code> <code>Tensor</code> <p>The visual embeddings.</p> required <code>visual_attention_mask</code> <code>Tensor</code> <p>The visual attention mask.</p> required <code>visual_token_type_ids</code> <code>Tensor</code> <p>The visual token type IDs.</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output logits of the model.</p> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the VisualBertForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        token_type_ids (torch.Tensor): The token type IDs.\n        position_ids (torch.Tensor): The position IDs.\n        visual_embeds (torch.Tensor): The visual embeddings.\n        visual_attention_mask (torch.Tensor): The visual attention mask.\n        visual_token_type_ids (torch.Tensor): The visual token type IDs.\n\n    Returns:\n        ClassificationOutputs: The output logits of the model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        visual_embeds=visual_embeds,\n        visual_attention_mask=visual_attention_mask,\n        visual_token_type_ids=visual_token_type_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/visualbert/#unitorch.cli.models.visualbert.VisualBertForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of VisualBertForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VisualBertForClassification</code> <p>The initialized VisualBertForClassification instance.</p> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/visualbert\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of VisualBertForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        VisualBertForClassification: The initialized VisualBertForClassification instance.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/visualbert\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-visualbert\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_visualbert_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_visualbert_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/visualbert/#visualbertforpretrain","title":"VisualBertForPretrain","text":"<p>Tip</p> <p><code>core/model/pretrain/visualbert</code> is the section for configuration of VisualBertForPretrain.</p> <p>             Bases: <code>VisualBertForPretrain</code></p> <p>VisualBERT for Pretraining model.</p> <p>Initialize VisualBertForPretrain.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model's configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing to save memory during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize VisualBertForPretrain.\n\n    Args:\n        config_path (str): The path to the model's configuration file.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing\n            to save memory during training. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/visualbert/#unitorch.cli.models.visualbert.VisualBertForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n    nsp_label: torch.Tensor,\n    mlm_label: torch.Tensor,\n    mlm_label_mask: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the VisualBertForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>token_type_ids</code> <code>Tensor</code> <p>The token type IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> required <code>visual_embeds</code> <code>Tensor</code> <p>The visual embeddings.</p> required <code>visual_attention_mask</code> <code>Tensor</code> <p>The visual attention mask.</p> required <code>visual_token_type_ids</code> <code>Tensor</code> <p>The visual token type IDs.</p> required <code>nsp_label</code> <code>Tensor</code> <p>The next sentence prediction label.</p> required <code>mlm_label</code> <code>Tensor</code> <p>The masked language model label.</p> required <code>mlm_label_mask</code> <code>Tensor</code> <p>The masked language model label mask.</p> required <p>Returns:</p> Name Type Description <code>LossOutputs</code> <p>The output loss of the model.</p> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n    nsp_label: torch.Tensor,\n    mlm_label: torch.Tensor,\n    mlm_label_mask: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the VisualBertForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        token_type_ids (torch.Tensor): The token type IDs.\n        position_ids (torch.Tensor): The position IDs.\n        visual_embeds (torch.Tensor): The visual embeddings.\n        visual_attention_mask (torch.Tensor): The visual attention mask.\n        visual_token_type_ids (torch.Tensor): The visual token type IDs.\n        nsp_label (torch.Tensor): The next sentence prediction label.\n        mlm_label (torch.Tensor): The masked language model label.\n        mlm_label_mask (torch.Tensor): The masked language model label mask.\n\n    Returns:\n        LossOutputs: The output loss of the model.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        visual_embeds=visual_embeds,\n        visual_attention_mask=visual_attention_mask,\n        visual_token_type_ids=visual_token_type_ids,\n        nsp_label=nsp_label,\n        mlm_label=mlm_label,\n        mlm_label_mask=mlm_label_mask,\n    )\n    return LossOutputs(loss=outputs)\n</code></pre>"},{"location":"cli/models/visualbert/#unitorch.cli.models.visualbert.VisualBertForPretrain.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of VisualBertForPretrain from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VisualBertForPretrain</code> <p>The initialized VisualBertForPretrain instance.</p> Source code in <code>src/unitorch/cli/models/visualbert/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/pretrain/visualbert\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of VisualBertForPretrain from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        VisualBertForPretrain: The initialized VisualBertForPretrain instance.\n    \"\"\"\n    config.set_default_section(\"core/model/pretrain/visualbert\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-visualbert\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_visualbert_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_visualbert_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/vit/","title":"unitorch.cli.models.vit","text":""},{"location":"cli/models/vit/#vitprocessor","title":"ViTProcessor","text":"<p>Tip</p> <p><code>core/process/vit</code> is the section for configuration of ViTProcessor.</p> <p>             Bases: <code>ViTProcessor</code></p> <p>Vision Transformer (ViT) Processor for handling image processing tasks.</p> <p>Initialize ViTProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>The path to the vision config file.</p> required Source code in <code>src/unitorch/cli/models/vit/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initialize ViTProcessor.\n\n    Args:\n        vision_config_path (str): The path to the vision config file.\n    \"\"\"\n    super().__init__(\n        vision_config_path=vision_config_path,\n    )\n</code></pre>"},{"location":"cli/models/vit/#unitorch.cli.models.vit.ViTProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ViTProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed arguments for initializing the processor.</p> Source code in <code>src/unitorch/cli/models/vit/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/vit\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ViTProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The processed arguments for initializing the processor.\n    \"\"\"\n    config.set_default_section(\"core/process/vit\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-vit\")\n    vision_config_path = config.getoption(\"vision_config_path\", None)\n    vision_config_path = pop_value(\n        vision_config_path,\n        nested_dict_value(pretrained_vit_infos, pretrained_name, \"vision_config\"),\n    )\n\n    vision_config_path = cached_path(vision_config_path)\n\n    return {\n        \"vision_config_path\": vision_config_path,\n    }\n</code></pre>"},{"location":"cli/models/vit/#vitforimageclassification","title":"ViTForImageClassification","text":"<p>Tip</p> <p><code>core/model/classification/vit</code> is the section for configuration of ViTForImageClassification.</p> <p>             Bases: <code>ViTForImageClassification</code></p> <p>Vision Transformer (ViT) for Image Classification model.</p> <p>Initialize ViTForImageClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model's configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes for image classification. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/cli/models/vit/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initialize ViTForImageClassification.\n\n    Args:\n        config_path (str): The path to the model's configuration file.\n        num_classes (Optional[int]): The number of classes for image classification.\n            Defaults to 1.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n</code></pre>"},{"location":"cli/models/vit/#unitorch.cli.models.vit.ViTForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the ViTForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input pixel values of the image.</p> required <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The output logits of the model.</p> Source code in <code>src/unitorch/cli/models/vit/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the ViTForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The input pixel values of the image.\n\n    Returns:\n        ClassificationOutputs: The output logits of the model.\n    \"\"\"\n    outputs = super().forward(pixel_values=pixel_values)\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/vit/#unitorch.cli.models.vit.ViTForImageClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of ViTForImageClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ViTForImageClassification</code> <p>The initialized ViTForImageClassification instance.</p> Source code in <code>src/unitorch/cli/models/vit/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/vit\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of ViTForImageClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        ViTForImageClassification: The initialized ViTForImageClassification instance.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/vit\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-vit\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_vit_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(\n        config_path=config_path,\n        num_classes=num_classes,\n    )\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_vit_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/xlm_roberta/","title":"unitorch.cli.models.xlm_roberta","text":""},{"location":"cli/models/xlm_roberta/#xlmrobertaprocessor","title":"XLMRobertaProcessor","text":"<p>Tip</p> <p><code>core/process/xlm_roberta</code> is the section for configuration of XLMRobertaProcessor.</p> <p>             Bases: <code>XLMRobertaProcessor</code></p> <p>XLM-RoBERTa Processor for handling text processing tasks.</p> <p>Initialize XLMRobertaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>The source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>The target type ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/cli/models/xlm_roberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initialize XLMRobertaProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        source_type_id (int, optional): The source type ID. Defaults to 0.\n        target_type_id (int, optional): The target type ID. Defaults to 0.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n    )\n</code></pre>"},{"location":"cli/models/xlm_roberta/#unitorch.cli.models.xlm_roberta.XLMRobertaProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of XLMRobertaProcessor from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>The processed arguments for initializing the processor.</p> Source code in <code>src/unitorch/cli/models/xlm_roberta/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/xlm_roberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of XLMRobertaProcessor from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        dict: The processed arguments for initializing the processor.\n    \"\"\"\n    config.set_default_section(\"core/process/xlm_roberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-xlm-roberta\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_xlm_roberta_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/xlm_roberta/#xlmrobertaforclassification","title":"XLMRobertaForClassification","text":"<p>Tip</p> <p><code>core/model/classification/xlm_roberta</code> is the section for configuration of XLMRobertaForClassification.</p> <p>             Bases: <code>XLMRobertaForClassification</code></p> <p>XLM-RoBERTa model for classification tasks.</p> <p>Initialize XLMRobertaForClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize XLMRobertaForClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/xlm_roberta/#unitorch.cli.models.xlm_roberta.XLMRobertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/xlm_roberta/#unitorch.cli.models.xlm_roberta.XLMRobertaForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of XLMRobertaForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>XLMRobertaForClassification</code> <p>The instantiated XLMRobertaForClassification model.</p> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/xlm_roberta\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of XLMRobertaForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        XLMRobertaForClassification: The instantiated XLMRobertaForClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/xlm_roberta\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-xlm-roberta\")\n    config_path = config.getoption(\"config_path\", None)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_xlm_roberta_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_xlm_roberta_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/xlm_roberta/#xlmrobertaxlforclassification","title":"XLMRobertaXLForClassification","text":"<p>Tip</p> <p><code>core/model/classification/xlm_roberta_xl</code> is the section for configuration of XLMRobertaXLForClassification.</p> <p>             Bases: <code>XLMRobertaXLForClassification</code></p> <p>XLM-RoBERTa XL model for classification tasks.</p> <p>Initialize XLMRobertaXLForClassification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize XLMRobertaXLForClassification.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        num_classes=num_classes,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/xlm_roberta/#unitorch.cli.models.xlm_roberta.XLMRobertaXLForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ClassificationOutputs</code> <p>The classification outputs.</p> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>@autocast()\ndef forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor, optional): Attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs. Defaults to None.\n\n    Returns:\n        ClassificationOutputs: The classification outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    return ClassificationOutputs(outputs=outputs)\n</code></pre>"},{"location":"cli/models/xlm_roberta/#unitorch.cli.models.xlm_roberta.XLMRobertaXLForClassification.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of XLMRobertaXLForClassification from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>XLMRobertaXLForClassification</code> <p>The instantiated XLMRobertaXLForClassification model.</p> Source code in <code>src/unitorch/cli/models/xlm_roberta/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/classification/xlm_roberta_xl\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of XLMRobertaXLForClassification from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        XLMRobertaXLForClassification: The instantiated XLMRobertaXLForClassification model.\n    \"\"\"\n    config.set_default_section(\"core/model/classification/xlm_roberta_xl\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"xlm-roberta-xl\")\n    config_path = config.getoption(\"config_path\", None)\n\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_xlm_roberta_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n    num_classes = config.getoption(\"num_classes\", 1)\n\n    inst = cls(config_path, num_classes, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_xlm_roberta_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/xpegasus/","title":"unitorch.cli.models.xpegasus","text":""},{"location":"cli/models/xpegasus/#xpegasusprocessor","title":"XPegasusProcessor","text":"<p>Tip</p> <p><code>core/process/xpegasus</code> is the section for configuration of XPegasusProcessor.</p> <p>             Bases: <code>XPegasusProcessor</code></p> <p>XPegasus processor for preprocessing and postprocessing.</p> <p>Initialize the XPegasusProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>A dictionary of special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for input text. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum sequence length for generated text. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/cli/models/xpegasus/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize the XPegasusProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Dict, optional): A dictionary of special input IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum sequence length for input text. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum sequence length for generated text. Defaults to 48.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        special_input_ids=special_input_ids,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"cli/models/xpegasus/#unitorch.cli.models.xpegasus.XPegasusProcessor.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of XPegasusProcessor from the core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The core configuration object.</p> required <p>Returns:</p> Name Type Description <code>XPegasusProcessor</code> <p>An instance of XPegasusProcessor initialized with the provided configuration.</p> Source code in <code>src/unitorch/cli/models/xpegasus/processing.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/process/xpegasus\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of XPegasusProcessor from the core configuration.\n\n    Args:\n        config (Config): The core configuration object.\n\n    Returns:\n        XPegasusProcessor: An instance of XPegasusProcessor initialized with the provided configuration.\n    \"\"\"\n    config.set_default_section(\"core/process/xpegasus\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-xpegasus\")\n    vocab_path = config.getoption(\"vocab_path\", None)\n    vocab_path = pop_value(\n        vocab_path,\n        nested_dict_value(pretrained_xpegasus_infos, pretrained_name, \"vocab\"),\n    )\n    vocab_path = cached_path(vocab_path)\n\n    return {\n        \"vocab_path\": vocab_path,\n    }\n</code></pre>"},{"location":"cli/models/xpegasus/#xpegasusforgeneration","title":"XPegasusForGeneration","text":"<p>Tip</p> <p><code>core/model/generation/xpegasus</code> is the section for configuration of XPegasusForGeneration.</p> <p>             Bases: <code>XPegasusForGeneration</code></p> <p>XPegasus model for generation tasks.</p> <p>Initialize XPegasusForGeneration.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/cli/models/xpegasus/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize XPegasusForGeneration.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__(\n        config_path=config_path,\n        gradient_checkpointing=gradient_checkpointing,\n    )\n</code></pre>"},{"location":"cli/models/xpegasus/#unitorch.cli.models.xpegasus.XPegasusForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Perform a forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Decoder attention mask.</p> required <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/xpegasus/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Perform a forward pass through the model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask.\n        decoder_input_ids (torch.Tensor): Decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Decoder attention mask.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().forward(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    return GenerationOutputs(sequences=outputs)\n</code></pre>"},{"location":"cli/models/xpegasus/#unitorch.cli.models.xpegasus.XPegasusForGeneration.from_core_configure","title":"from_core_configure  <code>classmethod</code>","text":"<pre><code>from_core_configure(config, **kwargs)\n</code></pre> <p>Create an instance of XPegasusForGeneration from a core configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>The core configuration.</p> required <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>XPegasusForGeneration</code> <p>The instantiated XPegasusForGeneration model.</p> Source code in <code>src/unitorch/cli/models/xpegasus/modeling.py</code> <pre><code>@classmethod\n@add_default_section_for_init(\"core/model/generation/xpegasus\")\ndef from_core_configure(cls, config, **kwargs):\n    \"\"\"\n    Create an instance of XPegasusForGeneration from a core configuration.\n\n    Args:\n        config: The core configuration.\n        **kwargs: Additional keyword arguments.\n\n    Returns:\n        XPegasusForGeneration: The instantiated XPegasusForGeneration model.\n    \"\"\"\n    config.set_default_section(\"core/model/generation/xpegasus\")\n    pretrained_name = config.getoption(\"pretrained_name\", \"default-xpegasus\")\n    config_path = config.getoption(\"config_path\", None)\n    config_path = pop_value(\n        config_path,\n        nested_dict_value(pretrained_xpegasus_infos, pretrained_name, \"config\"),\n    )\n\n    config_path = cached_path(config_path)\n    gradient_checkpointing = config.getoption(\"gradient_checkpointing\", False)\n\n    inst = cls(config_path, gradient_checkpointing)\n    pretrained_weight_path = config.getoption(\"pretrained_weight_path\", None)\n    weight_path = pop_value(\n        pretrained_weight_path,\n        nested_dict_value(pretrained_xpegasus_infos, pretrained_name, \"weight\"),\n        check_none=False,\n    )\n    if weight_path is not None:\n        weight_path = cached_path(weight_path)\n        inst.from_pretrained(weight_path)\n\n    return inst\n</code></pre>"},{"location":"cli/models/xpegasus/#unitorch.cli.models.xpegasus.XPegasusForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate sequences using the XPegasus model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>Decoder start token ID. Defaults to 0.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 1.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum generation sequence length. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to perform early stopping. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k sampling parameter. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p sampling parameter. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenerationOutputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/cli/models/xpegasus/modeling.py</code> <pre><code>@add_default_section_for_function(\"core/model/generation/xpegasus\")\n@torch.no_grad()\n@autocast()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate sequences using the XPegasus model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): Decoder start token ID. Defaults to 0.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 1.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum generation sequence length. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to perform early stopping. Defaults to True.\n        length_penalty (float, optional): Length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k sampling parameter. Defaults to 50.\n        top_p (float, optional): Top-p sampling parameter. Defaults to 1.0.\n\n    Returns:\n        GenerationOutputs: The generation outputs.\n    \"\"\"\n    outputs = super().generate(\n        input_ids,\n        num_beams=num_beams,\n        decoder_start_token_id=decoder_start_token_id,\n        decoder_end_token_id=decoder_end_token_id,\n        num_return_sequences=num_return_sequences,\n        min_gen_seq_length=min_gen_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n        repetition_penalty=repetition_penalty,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        do_sample=do_sample,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n    )\n\n    return GenerationOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"examples/caption/blip/","title":"Blip Image Caption Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will learn how to perform image caption using the Blip model. Blip is a transformer-based model that has been pre-trained on a large corpus of text/image and can be fine-tuned for specific downstream tasks like multi-modal classification &amp; image caption.</p>"},{"location":"examples/caption/blip/#step-1-prepare-dataset-files","title":"Step 1: Prepare Dataset Files","text":"<p>\u00a0\u00a0\u00a0\u00a0Prepare a TSV (Tab-Separated Values) file as an example. You can also use JSON, PARQUET or HUB dataset formats. The TSV file should have three columns with the following dataset settings. The \"caption\" column is the model target. The \"image\" column contains the image path string in the zip files. You need to start an image service first.</p>"},{"location":"examples/caption/blip/#step-2-prepare-configini-file","title":"Step 2: Prepare config.ini File","text":"<p>Take this config as a template.</p>"},{"location":"examples/caption/blip/#task-settings","title":"Task Settings","text":"<pre><code># optim\n[core/optim/adamw]\nlearning_rate = 0.0001\n\n# scheduler\n[core/scheduler/linear_warmup]\nnum_warmup_rate = 0.001\n\n# task\n[core/task/supervised]\nmodel = core/model/caption/blip\noptim = core/optim/adamw\nscheduler = core/scheduler/linear_warmup\ndataset = core/dataset/ast\nloss_fn = core/loss/lm\nscore_fn = core/score/bleu\nmonitor_fns = ['core/score/bleu', 'core/score/rouge1', 'core/score/rouge2', 'core/score/rougel']\noutput_header = ['image']\npostprocess_fn = core/postprocess/blip/detokenize\nwriter = core/writer/csv\n\nfrom_ckpt_dir = ${core/cli:from_ckpt_dir}\nto_ckpt_dir = ${core/cli:cache_dir}\noutput_path = ${core/cli:cache_dir}/output.txt\ntrain_batch_size = 4\ndev_batch_size = 8\ntest_batch_size = 8\n</code></pre> <p>Note</p> <ul> <li><code>model</code>: model to be used in the task.</li> <li><code>optim</code>: optim to be used in the task.</li> <li><code>scheduler</code>: scheduler to be used in the task.</li> <li><code>loss_fn</code>: loss to be used in the task.</li> <li><code>score_fn</code>: metric to be used for saving checkpoints.</li> <li><code>monitor_fns</code>: metrics for logging.</li> <li><code>output_header</code>: save these fields for inference.</li> <li><code>postprocess_fn</code>, <code>writer</code>: post-process functions and writer for inference.</li> <li><code>{train, dev, test}_batch_size</code>: batch size settings for train/eval/inference.</li> </ul>"},{"location":"examples/caption/blip/#model-settings","title":"Model Settings","text":"<pre><code>[core/model/caption/blip]\npretrained_name = blip-image-captioning-base\nno_repeat_ngram_size = 3\nmax_gen_seq_length = 15\n</code></pre> <p>Note</p> <p>The options in [core/model/caption/blip] are settings for the Blip model.</p>"},{"location":"examples/caption/blip/#dataset-settings","title":"Dataset Settings","text":"<pre><code>[core/dataset/ast]\nnames = ['caption', 'image']\n\n[core/dataset/ast/train]\ndata_files = ${core/cli:train_file}\npreprocess_functions = [\n    'core/process/blip/generation(caption, core/process/image/read(image))',\n  ]\n\n[core/dataset/ast/dev]\ndata_files = ${core/cli:dev_file}\npreprocess_functions = [\n    'core/process/blip/image_classification(core/process/image/read(image))',\n    'core/process/blip/generation/labels(caption)',\n  ]\n\n[core/dataset/ast/test]\nnames = ['text', 'image']\ndata_files = ${core/cli:test_file}\npreprocess_functions = [\n    'core/process/blip/image_classification(core/process/image/read(image))'\n  ]\n</code></pre> <p>Note</p> <ul> <li>The options in [core/dataset/ast/train] are settings for the training data.</li> <li><code>names</code> specifies the fields in the dataset. For TSV files, this should be the header.</li> <li><code>preprocess_functions</code> are the preprocess functions used to convert the raw data into tensors as model inputs.</li> <li>These options can be set independently for train, dev, and test.</li> </ul>"},{"location":"examples/caption/blip/#processing-settings","title":"Processing Settings","text":"<pre><code>[core/process/blip]\npretrained_name = blip-image-captioning-base\nmax_gen_seq_length = 15\n\n[core/process/image]\nhttp_url = http://0.0.0.0:11230/?image={0}\n</code></pre> <p>Note</p> <ul> <li>The options in [core/process/blip] are settings for the Blip processor used in dataset.</li> <li>The options in [core/process/image] are settings for the Image processor to read image from the local service.</li> </ul>"},{"location":"examples/caption/blip/#step-3-run-training-command","title":"Step 3: Run Training Command","text":"<p>Start local image service first:</p> <pre><code>unitorch-service start path/to/zip/image/service.ini --zip_folder path/to/zip/folder\n</code></pre> <p>Use the following command to run the training:</p> <pre><code>unitorch-train path/to/config.ini --train_file path/to/train.tsv --dev_file path/to/dev.tsv --core/task/supervised@train_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@train_batch_size 128 part of the command overrides the parameter setting in the config file.</p> <p>Use the following command to run the inference:</p> <pre><code>unitorch-infer path/to/config.ini --test_file path/to/test.tsv --from_ckpt_dir path/to/ckpt/folder --core/task/supervised@test_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@test_batch_size 128 part of the command overrides the parameter setting in the config file.</p>"},{"location":"examples/classification/clip/","title":"Clip Multi-Modal Classification Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will learn how to perform multi-modal classification using the Clip model. Clip is a transformer-based model that has been pre-trained on a large corpus of text/image and can be fine-tuned for specific downstream tasks like multi-modal classification.</p>"},{"location":"examples/classification/clip/#step-1-prepare-dataset-files","title":"Step 1: Prepare Dataset Files","text":"<p>\u00a0\u00a0\u00a0\u00a0Prepare a TSV (Tab-Separated Values) file as an example. You can also use JSON, PARQUET or HUB dataset formats. The TSV file should have three columns with the following dataset settings. The \"image\" column contains the image paths as strings.</p>"},{"location":"examples/classification/clip/#step-2-prepare-configini-file","title":"Step 2: Prepare config.ini File","text":"<p>Take this config as a template.</p>"},{"location":"examples/classification/clip/#task-settings","title":"Task Settings","text":"<pre><code># optim\n[core/optim/adamw]\nlearning_rate = 0.0001\n\n# scheduler\n[core/scheduler/linear_warmup]\nnum_warmup_rate = 0.001\n\n# task\n[core/task/supervised]\nmodel = core/model/classification/clip\noptim = core/optim/adamw\nscheduler = core/scheduler/linear_warmup\ndataset = core/dataset/ast\nloss_fn = core/loss/ce\nscore_fn = core/score/acc\nmonitor_fns = ['core/score/acc']\noutput_header = ['text', 'image']\npostprocess_fn = core/postprocess/classification/score\nwriter = core/writer/csv\n\nfrom_ckpt_dir = ${core/cli:from_ckpt_dir}\nto_ckpt_dir = ${core/cli:cache_dir}\noutput_path = ${core/cli:cache_dir}/output.txt\ntrain_batch_size = 8\ndev_batch_size = 16\ntest_batch_size = 16\n</code></pre> <p>Note</p> <ul> <li><code>model</code>: model to be used in the task.</li> <li><code>optim</code>: optim to be used in the task.</li> <li><code>scheduler</code>: scheduler to be used in the task.</li> <li><code>loss_fn</code>: loss to be used in the task.</li> <li><code>score_fn</code>: metric to be used for saving checkpoints.</li> <li><code>monitor_fns</code>: metrics for logging.</li> <li><code>output_header</code>: save these fields for inference.</li> <li><code>postprocess_fn</code>, <code>writer</code>: post-process functions and writer for inference.</li> <li><code>{train, dev, test}_batch_size</code>: batch size settings for train/eval/inference.</li> </ul>"},{"location":"examples/classification/clip/#model-settings","title":"Model Settings","text":"<pre><code>[core/model/classification/clip]\npretrained_name = clip-vit-base-patch16\nnum_classes = 2\n</code></pre> <p>Note</p> <p>The options in [core/model/classification/clip] are settings for the Clip model.</p>"},{"location":"examples/classification/clip/#dataset-settings","title":"Dataset Settings","text":"<pre><code>[core/dataset/ast]\nnames = ['text', 'image', 'label']\n\n[core/dataset/ast/train]\ndata_files = ${core/cli:train_file}\npreprocess_functions = [\n    'core/process/clip/classification(text, core/process/image/read(image))',\n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/dev]\ndata_files = ${core/cli:dev_file}\npreprocess_functions = [\n    'core/process/clip/classification(text, core/process/image/read(image))', \n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/test]\nnames = ['text', 'image']\ndata_files = ${core/cli:test_file}\npreprocess_functions = [\n    'core/process/clip/classification(text, core/process/image/read(image))'\n  ]\n</code></pre> <p>Note</p> <ul> <li>The options in [core/dataset/ast/train] are settings for the training data.</li> <li><code>names</code> specifies the fields in the dataset. For TSV files, this should be the header.</li> <li><code>preprocess_functions</code> are the preprocess functions used to convert the raw data into tensors as model inputs.</li> <li>These options can be set independently for train, dev, and test.</li> </ul>"},{"location":"examples/classification/clip/#processing-settings","title":"Processing Settings","text":"<pre><code>[core/process/clip]\npretrained_name = clip-vit-base-patch16\nmax_seq_length = 36\n</code></pre> <p>Note</p> <p>The options in [core/process/clip] are settings for the Clip processor used in dataset.</p>"},{"location":"examples/classification/clip/#step-3-run-training-command","title":"Step 3: Run Training Command","text":"<p>Use the following command to run the training:</p> <pre><code>unitorch-train path/to/config.ini --train_file path/to/train.tsv --dev_file path/to/dev.tsv --core/task/supervised@train_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@train_batch_size 128 part of the command overrides the parameter setting in the config file.</p> <p>Use the following command to run the inference:</p> <pre><code>unitorch-infer path/to/config.ini --test_file path/to/test.tsv --from_ckpt_dir path/to/ckpt/folder --core/task/supervised@test_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@test_batch_size 128 part of the command overrides the parameter setting in the config file.</p>"},{"location":"examples/classification/roberta/","title":"Roberta Text Classification Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will learn how to perform text classification using the RoBERTa model. RoBERTa is a transformer-based model that has been pre-trained on a large corpus of text and can be fine-tuned for specific downstream tasks like text classification.</p>"},{"location":"examples/classification/roberta/#step-1-prepare-dataset-files","title":"Step 1: Prepare Dataset Files","text":"<p>\u00a0\u00a0\u00a0\u00a0Prepare a TSV (Tab-Separated Values) file as an example. You can also use JSON, PARQUET or HUB dataset formats. The TSV file should have three columns with the following dataset settings. The \"image\" column contains the image paths as strings.</p>"},{"location":"examples/classification/roberta/#step-2-prepare-configini-file","title":"Step 2: Prepare config.ini File","text":"<p>Take this config as a template.</p>"},{"location":"examples/classification/roberta/#task-settings","title":"Task Settings","text":"<pre><code># optim\n[core/optim/adamw]\nlearning_rate = 0.0001\n\n# scheduler\n[core/scheduler/linear_warmup]\nnum_warmup_rate = 0.001\n\n# task\n[core/task/supervised]\nmodel = core/model/classification/roberta\noptim = core/optim/adamw\nscheduler = core/scheduler/linear_warmup\ndataset = core/dataset/ast\nloss_fn = core/loss/ce\nscore_fn = core/score/acc\nmonitor_fns = ['core/score/acc']\noutput_header = ['query', 'doc']\npostprocess_fn = core/postprocess/classification/score\nwriter = core/writer/csv\n\nfrom_ckpt_dir = ${core/cli:from_ckpt_dir}\nto_ckpt_dir = ${core/cli:cache_dir}\noutput_path = ${core/cli:cache_dir}/output.txt\ntrain_batch_size = 64\ndev_batch_size = 64\ntest_batch_size = 256\n</code></pre> <p>Note</p> <ul> <li><code>model</code>: model to be used in the task.</li> <li><code>optim</code>: optim to be used in the task.</li> <li><code>scheduler</code>: scheduler to be used in the task.</li> <li><code>loss_fn</code>: loss to be used in the task.</li> <li><code>score_fn</code>: metric to be used for saving checkpoints.</li> <li><code>monitor_fns</code>: metrics for logging.</li> <li><code>output_header</code>: save these fields for inference.</li> <li><code>postprocess_fn</code>, <code>writer</code>: post-process functions and writer for inference.</li> <li><code>{train, dev, test}_batch_size</code>: batch size settings for train/eval/inference.</li> </ul>"},{"location":"examples/classification/roberta/#model-settings","title":"Model Settings","text":"<pre><code>[core/model/classification/roberta]\npretrained_name = roberta-base\nnum_classes = 2\n</code></pre> <p>Note</p> <p>The options in [core/model/classification/roberta] are settings for the Roberta model.</p>"},{"location":"examples/classification/roberta/#dataset-settings","title":"Dataset Settings","text":"<pre><code>[core/dataset/ast]\nnames = ['query', 'doc', 'label']\n\n[core/dataset/ast/train]\ndata_files = ${core/cli:train_file}\npreprocess_functions = [\n    'core/process/roberta/classification(query, doc))',\n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/dev]\ndata_files = ${core/cli:dev_file}\npreprocess_functions = [\n    'core/process/roberta/classification(query, doc))', \n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/test]\nnames = ['text', 'image']\ndata_files = ${core/cli:test_file}\npreprocess_functions = [\n    'core/process/roberta/classification(query, doc))'\n  ]\n</code></pre> <p>Note</p> <ul> <li>The options in [core/dataset/ast/train] are settings for the training data.</li> <li><code>names</code> specifies the fields in the dataset. For TSV files, this should be the header.</li> <li><code>preprocess_functions</code> are the preprocess functions used to convert the raw data into tensors as model inputs.</li> <li>These options can be set independently for train, dev, and test.</li> </ul>"},{"location":"examples/classification/roberta/#processing-settings","title":"Processing Settings","text":"<pre><code>[core/process/roberta]\npretrained_name = roberta-base\nmax_seq_length = 24\n</code></pre> <p>Note</p> <p>The options in [core/process/roberta] are settings for the Roberta processor used in dataset.</p>"},{"location":"examples/classification/roberta/#step-3-run-training-command","title":"Step 3: Run Training Command","text":"<p>Use the following command to run the training:</p> <pre><code>unitorch-train path/to/config.ini --train_file path/to/train.tsv --dev_file path/to/dev.tsv --core/task/supervised@train_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@train_batch_size 128 part of the command overrides the parameter setting in the config file.</p> <p>Use the following command to run the inference:</p> <pre><code>unitorch-infer path/to/config.ini --test_file path/to/test.tsv --from_ckpt_dir path/to/ckpt/folder --core/task/supervised@test_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@test_batch_size 128 part of the command overrides the parameter setting in the config file.</p>"},{"location":"examples/classification/swin/","title":"Swin Image Classification Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will learn how to perform image classification using the Swin model. Swin is a transformer-based model that has been pre-trained on a large corpus of image and can be fine-tuned for specific downstream tasks like image classification.</p>"},{"location":"examples/classification/swin/#step-1-prepare-dataset-files","title":"Step 1: Prepare Dataset Files","text":"<p>\u00a0\u00a0\u00a0\u00a0Prepare a TSV (Tab-Separated Values) file as an example. You can also use JSON, PARQUET or HUB dataset formats. The TSV file should have three columns with the following dataset settings. The \"image\" column contains the image path string in the zip files. You need to start an image service first.</p>"},{"location":"examples/classification/swin/#step-2-prepare-configini-file","title":"Step 2: Prepare config.ini File","text":"<p>Take this config as a template.</p>"},{"location":"examples/classification/swin/#task-settings","title":"Task Settings","text":"<pre><code># optim\n[core/optim/adamw]\nlearning_rate = 0.0001\n\n# scheduler\n[core/scheduler/linear_warmup]\nnum_warmup_rate = 0.001\n\n# task\n[core/task/supervised]\nmodel = core/model/classification/swin\noptim = core/optim/adamw\nscheduler = core/scheduler/linear_warmup\ndataset = core/dataset/ast\nloss_fn = core/loss/ce\nscore_fn = core/score/acc\nmonitor_fns = ['core/score/acc']\noutput_header = ['image']\npostprocess_fn = core/postprocess/classification/score\nwriter = core/writer/csv\n\nfrom_ckpt_dir = ${core/cli:from_ckpt_dir}\nto_ckpt_dir = ${core/cli:cache_dir}\noutput_path = ${core/cli:cache_dir}/output.txt\ntrain_batch_size = 8\ndev_batch_size = 32\ntest_batch_size = 32\n</code></pre> <p>Note</p> <ul> <li><code>model</code>: model to be used in the task.</li> <li><code>optim</code>: optim to be used in the task.</li> <li><code>scheduler</code>: scheduler to be used in the task.</li> <li><code>loss_fn</code>: loss to be used in the task.</li> <li><code>score_fn</code>: metric to be used for saving checkpoints.</li> <li><code>monitor_fns</code>: metrics for logging.</li> <li><code>output_header</code>: save these fields for inference.</li> <li><code>postprocess_fn</code>, <code>writer</code>: post-process functions and writer for inference.</li> <li><code>{train, dev, test}_batch_size</code>: batch size settings for train/eval/inference.</li> </ul>"},{"location":"examples/classification/swin/#model-settings","title":"Model Settings","text":"<pre><code>[core/model/classification/swin]\npretrained_name = swin-base-patch4-window7-224\nnum_classes = 2\n</code></pre> <p>Note</p> <p>The options in [core/model/classification/swin] are settings for the Swin model.</p>"},{"location":"examples/classification/swin/#dataset-settings","title":"Dataset Settings","text":"<pre><code>[core/dataset/ast]\nnames = ['image', 'label']\n\n[core/dataset/ast/train]\ndata_files = ${core/cli:train_file}\npreprocess_functions = [\n    'core/process/swin/image_classification(core/process/image/read(image))', \n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/dev]\ndata_files = ${core/cli:dev_file}\npreprocess_functions = [\n    'core/process/swin/image_classification(core/process/image/read(image))',\n    'core/process/label(label)'\n  ]\n\n[core/dataset/ast/test]\nnames = ['image']\ndata_files = ${core/cli:test_file}\npreprocess_functions = [\n    'core/process/swin/image_classification(core/process/image/read(image))'\n  ]\n</code></pre> <p>Note</p> <ul> <li>The options in [core/dataset/ast/train] are settings for the training data.</li> <li><code>names</code> specifies the fields in the dataset. For TSV files, this should be the header.</li> <li><code>preprocess_functions</code> are the preprocess functions used to convert the raw data into tensors as model inputs.</li> <li>These options can be set independently for train, dev, and test.</li> </ul>"},{"location":"examples/classification/swin/#processing-settings","title":"Processing Settings","text":"<pre><code>[core/process/swin]\npretrained_name = swin-base-patch4-window7-224\n\n[core/process/image]\nhttp_url = http://0.0.0.0:11230/?image={0}\n</code></pre> <p>Note</p> <ul> <li>The options in [core/process/swin] are settings for the Swin processor used in dataset.</li> <li>The options in [core/process/image] are settings for the Image processor to read image from the local service.</li> </ul>"},{"location":"examples/classification/swin/#step-3-run-training-command","title":"Step 3: Run Training Command","text":"<p>Start local image service first:</p> <pre><code>unitorch-service start path/to/zip/image/service.ini --zip_folder path/to/zip/folder\n</code></pre> <p>Use the following command to run the training:</p> <pre><code>unitorch-train path/to/config.ini --train_file path/to/train.tsv --dev_file path/to/dev.tsv --core/task/supervised@train_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@train_batch_size 128 part of the command overrides the parameter setting in the config file.</p> <p>Use the following command to run the inference:</p> <pre><code>unitorch-infer path/to/config.ini --test_file path/to/test.tsv --from_ckpt_dir path/to/ckpt/folder --core/task/supervised@test_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@test_batch_size 128 part of the command overrides the parameter setting in the config file.</p>"},{"location":"examples/diffusion/controlnet/","title":"ControlNet Diffusion Tutorial","text":""},{"location":"examples/generation/bart/","title":"Bart Text Generation Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will learn how to perform text generation using the Bart model. Bart is a transformer-based model that has been pre-trained on a large corpus of text and can be fine-tuned for specific downstream tasks like text generation.</p>"},{"location":"examples/generation/bart/#step-1-prepare-dataset-files","title":"Step 1: Prepare Dataset Files","text":"<p>\u00a0\u00a0\u00a0\u00a0Prepare a TSV (Tab-Separated Values) file as an example. You can also use JSON, PARQUET or HUB dataset formats. The TSV file should have three columns with the following dataset settings. The \"decode\" column is the model target. The \"encode\" is the model input.</p>"},{"location":"examples/generation/bart/#step-2-prepare-configini-file","title":"Step 2: Prepare config.ini File","text":"<p>Take this config as a template.</p>"},{"location":"examples/generation/bart/#task-settings","title":"Task Settings","text":"<pre><code># optim\n[core/optim/adamw]\nlearning_rate = 0.0001\n\n# scheduler\n[core/scheduler/linear_warmup]\nnum_warmup_rate = 0.001\n\n# task\n[core/task/supervised]\nmodel = core/model/generation/bart\noptim = core/optim/adamw\nscheduler = core/scheduler/linear_warmup\ndataset = core/dataset/ast\nloss_fn = core/loss/lm\nscore_fn = core/score/bleu\nmonitor_fns = ['core/score/bleu', 'core/score/rouge1', 'core/score/rouge2', 'core/score/rougel']\noutput_header = ['encode']\npostprocess_fn = core/postprocess/bart/detokenize\nwriter = core/writer/csv\n\nfrom_ckpt_dir = ${core/cli:from_ckpt_dir}\nto_ckpt_dir = ${core/cli:cache_dir}\noutput_path = ${core/cli:cache_dir}/output.txt\ntrain_batch_size = 4\ndev_batch_size = 8\ntest_batch_size = 8\n</code></pre> <p>Note</p> <ul> <li><code>model</code>: model to be used in the task.</li> <li><code>optim</code>: optim to be used in the task.</li> <li><code>scheduler</code>: scheduler to be used in the task.</li> <li><code>loss_fn</code>: loss to be used in the task.</li> <li><code>score_fn</code>: metric to be used for saving checkpoints.</li> <li><code>monitor_fns</code>: metrics for logging.</li> <li><code>output_header</code>: save these fields for inference.</li> <li><code>postprocess_fn</code>, <code>writer</code>: post-process functions and writer for inference.</li> <li><code>{train, dev, test}_batch_size</code>: batch size settings for train/eval/inference.</li> </ul>"},{"location":"examples/generation/bart/#model-settings","title":"Model Settings","text":"<pre><code>[core/model/generation/bart]\npretrained_name = bart-base\nno_repeat_ngram_size = 3\nmax_gen_seq_length = 15\n</code></pre> <p>Note</p> <p>The options in [core/model/generation/bart] are settings for the Bart model.</p>"},{"location":"examples/generation/bart/#dataset-settings","title":"Dataset Settings","text":"<pre><code>[core/dataset/ast]\nnames = ['encode', 'decode']\n\n[core/dataset/ast/train]\ndata_files = ${core/cli:train_file}\npreprocess_functions = [\n    'core/process/bart/generation(encode, decode)'\n  ]\n\n[core/dataset/ast/dev]\ndata_files = ${core/cli:dev_file}\npreprocess_functions = [\n    'core/process/bart/generation/inputs(encode)',\n    'core/process/bart/generation/labels(decode)'\n  ]\n\n[core/dataset/ast/test]\nnames = ['encode']\ndata_files = ${core/cli:test_file}\npreprocess_functions = [\n    'core/process/bart/generation/inputs(encode)'\n  ]\n</code></pre> <p>Note</p> <ul> <li>The options in [core/dataset/ast/train] are settings for the training data.</li> <li><code>names</code> specifies the fields in the dataset. For TSV files, this should be the header.</li> <li><code>preprocess_functions</code> are the preprocess functions used to convert the raw data into tensors as model inputs.</li> <li>These options can be set independently for train, dev, and test.</li> </ul>"},{"location":"examples/generation/bart/#processing-settings","title":"Processing Settings","text":"<pre><code>[core/process/bart]\npretrained_name = bart-base\nmax_seq_length = 24\nmax_gen_seq_length = 15\n</code></pre> <p>Note</p> <ul> <li>The options in [core/process/bart] are settings for the Blip processor used in dataset.</li> </ul>"},{"location":"examples/generation/bart/#step-3-run-training-command","title":"Step 3: Run Training Command","text":"<p>Use the following command to run the training:</p> <pre><code>unitorch-train path/to/config.ini --train_file path/to/train.tsv --dev_file path/to/dev.tsv --core/task/supervised@train_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@train_batch_size 128 part of the command overrides the parameter setting in the config file.</p> <p>Use the following command to run the inference:</p> <pre><code>unitorch-infer path/to/config.ini --test_file path/to/test.tsv --from_ckpt_dir path/to/ckpt/folder --core/task/supervised@test_batch_size 128\n</code></pre> <p>Note</p> <p>The --core/task/supervised@test_batch_size 128 part of the command overrides the parameter setting in the config file.</p>"},{"location":"examples/service/zip_image/","title":"Zip Image Service Tutorial","text":"<p>\u00a0\u00a0\u00a0\u00a0In this tutorial, we will explore the usage of an image service for training and inference with image-related models. This service is specifically designed to handle large-scale image datasets, containing over 300 million images. The image service operates in the backend and offers an HTTP API that allows Users &amp; GPU processes to access image bytes by using an image ID.</p> <p></p>"},{"location":"examples/service/zip_image/#configuration-files","title":"Configuration files","text":""},{"location":"examples/service/zip_image/#single-zip-folder","title":"Single zip folder","text":"<pre><code>[core/cli]\nzip_folder = zip_folder/\nservice_name = core/service/zip_image\n\n[core/service/zip_image]\nzip_folder = ${core/cli:zip_folder}\nzip_extension = .zip\n</code></pre>"},{"location":"examples/service/zip_image/#multiple-zip-folders","title":"Multiple zip folders","text":"<pre><code>[core/cli]\nzip_folder1 = zip_folder1/\nzip_folder2 = zip_folder2/\nzip_folder3 = zip_folder3/\nservice_name = core/service/zip_image\n\n[core/service/zip_image]\nzip_folder = [\n    \"${core/cli:zip_folder1}\",\n    \"${core/cli:zip_folder2}\",\n    \"${core/cli:zip_folder3}\"\n  ]\nzip_extension = .zip\n</code></pre>"},{"location":"examples/service/zip_image/#start-service","title":"Start Service","text":"<pre><code>unitorch-service start path/to/zip/image/service.ini \\\n    --zip_folder path/to/zip/folder\n</code></pre>"},{"location":"examples/service/zip_image/#stop-service","title":"Stop Service","text":"<pre><code>unitorch-service stop path/to/zip/image/service.ini \\\n    --zip_folder path/to/zip/folder\n</code></pre>"},{"location":"examples/service/zip_image/#restart-service","title":"Restart Service","text":"<pre><code>unitorch-service restart path/to/zip/image/service.ini \\\n    --zip_folder path/to/zip/folder\n</code></pre>"},{"location":"models/","title":"unitorch.models","text":""},{"location":"models/#checkpointmixin","title":"CheckpointMixin","text":""},{"location":"models/#unitorch.models.CheckpointMixin.from_checkpoint","title":"from_checkpoint","text":"<pre><code>from_checkpoint(\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Load model weights from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory path of the checkpoint.</p> required <code>weight_name</code> <code>str</code> <p>Name of the weight file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def from_checkpoint(\n    self,\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Load model weights from a checkpoint.\n\n    Args:\n        ckpt_dir (str): Directory path of the checkpoint.\n        weight_name (str): Name of the weight file.\n\n    Returns:\n        None\n    \"\"\"\n    if weight_name is None:\n        weight_name = self.checkpoint_name\n    weight_path = os.path.join(ckpt_dir, weight_name)\n    if not os.path.exists(weight_path):\n        return\n    state_dict = torch.load(weight_path, map_location=\"cpu\")\n    self.load_state_dict(state_dict)\n    logging.info(\n        f\"{type(self).__name__} model load weight from checkpoint {weight_path}\"\n    )\n</code></pre>"},{"location":"models/#unitorch.models.CheckpointMixin.from_pretrained","title":"from_pretrained","text":"<pre><code>from_pretrained(\n    weight_path: Union[str, List[str]] = None,\n    state_dict: Union[Dict, List[Dict]] = None,\n    replace_keys: Optional[Dict] = dict(),\n    prefix_keys: Optional[Dict] = dict(),\n)\n</code></pre> <p>Load pretrained weights into the model.</p> <p>Parameters:</p> Name Type Description Default <code>weight_path</code> <code>str or List[str]</code> <p>Path(s) to the pretrained weight file(s).</p> <code>None</code> <code>state_dict</code> <code>Dict or List[Dict]</code> <p>Pretrained state_dict(s) to load weights from.</p> <code>None</code> <code>replace_keys</code> <code>Dict</code> <p>Dictionary mapping keys in the pretrained state_dict to the model's keys.</p> <code>dict()</code> <code>prefix_keys</code> <code>Dict</code> <p>Dictionary prefix keys in the pretrained state_dict to the model's keys.</p> <code>dict()</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def from_pretrained(\n    self,\n    weight_path: Union[str, List[str]] = None,\n    state_dict: Union[Dict, List[Dict]] = None,\n    replace_keys: Optional[Dict] = dict(),\n    prefix_keys: Optional[Dict] = dict(),\n):\n    \"\"\"\n    Load pretrained weights into the model.\n\n    Args:\n        weight_path (str or List[str]): Path(s) to the pretrained weight file(s).\n        state_dict (Dict or List[Dict]): Pretrained state_dict(s) to load weights from.\n        replace_keys (Dict): Dictionary mapping keys in the pretrained state_dict to the model's keys.\n        prefix_keys (Dict): Dictionary prefix keys in the pretrained state_dict to the model's keys.\n\n    Returns:\n        None\n    \"\"\"\n    assert weight_path or state_dict, \"weight_path or state_dict must be set\"\n\n    # Load state_dict(s) based on the provided weight_path or state_dict\n    state_dicts = []\n    if weight_path:\n        if isinstance(weight_path, str):\n            weight_path = [weight_path]\n        for path in weight_path:\n            logging.debug(f\"Loading weights from {path}\")\n        state_dicts += [\n            torch.load(hf_cached_path(path), map_location=\"cpu\")\n            for path in weight_path\n        ]\n\n    if state_dict:\n        state_dicts += state_dict if isinstance(state_dict, list) else [state_dict]\n\n    self_state_dict = self.state_dict()  # Get the current state_dict of the model\n    load_keys = []  # Keep track of the keys loaded from the state_dict(s)\n    non_load_keys = []  # Keep track of the keys not loaded from the state_dict(s)\n\n    if isinstance(self.replace_keys_in_state_dict, dict):\n        replace_keys = {**self.replace_keys_in_state_dict, **replace_keys}\n\n    if isinstance(self.prefix_keys_in_state_dict, dict):\n        prefix_keys = {**self.prefix_keys_in_state_dict, **prefix_keys}\n\n    # Iterate over the state_dict(s) and load the matching keys into the model's state_dict\n    for _state_dict in state_dicts:\n        if not _state_dict:\n            continue\n        for key, value in list(_state_dict.items()):\n            for rkey, prefix in prefix_keys.items():\n                if re.match(rkey, key):\n                    key = prefix + key\n                    break\n\n            for rkey, nkey in replace_keys.items():\n                key = re.sub(rkey, nkey, key)\n            if key in self_state_dict and value.shape == self_state_dict[key].shape:\n                self_state_dict[key] = value\n                if key not in load_keys:\n                    load_keys.append(key)\n            else:\n                non_load_keys.append(key)\n\n    self.load_state_dict(self_state_dict, False)\n    load_percent = (\n        len(load_keys) / len(self_state_dict) * 100\n    )  # Calculate the percentage of loaded keys\n    logging.debug(f\"Non load keys in pretrain weights: {list(non_load_keys)}\")\n    logging.debug(\n        f\"{type(self).__name__} missed keys: {list(self_state_dict.keys() - load_keys)}\"\n    )\n    logging.info(f\"{type(self).__name__} loaded weights ({int(load_percent)}%)\")\n</code></pre>"},{"location":"models/#unitorch.models.CheckpointMixin.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Save the model's current state as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory path to save the checkpoint.</p> required <code>weight_name</code> <code>str</code> <p>Name of the weight file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def save_checkpoint(\n    self,\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Save the model's current state as a checkpoint.\n\n    Args:\n        ckpt_dir (str): Directory path to save the checkpoint.\n        weight_name (str): Name of the weight file.\n\n    Returns:\n        None\n    \"\"\"\n    if weight_name is None:\n        weight_name = self.checkpoint_name\n    state_dict = self.state_dict()\n    weight_path = os.path.join(ckpt_dir, weight_name)\n    torch.save(state_dict, weight_path)\n    logging.info(f\"{type(self).__name__} model save checkpoint to {weight_path}\")\n</code></pre>"},{"location":"models/#genericmodel","title":"GenericModel","text":"<p>             Bases: <code>Module</code>, <code>CheckpointMixin</code></p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"models/#unitorch.models.GenericModel.device","title":"device  <code>property</code>","text":"<pre><code>device\n</code></pre> <p>Returns the device of the model's parameters.</p> <p>Returns:</p> Type Description <p>torch.device: The device of the model's parameters.</p>"},{"location":"models/#unitorch.models.GenericModel.dtype","title":"dtype  <code>property</code>","text":"<pre><code>dtype: torch.dtype\n</code></pre> <p>Returns the data type of the model's parameters.</p> <p>Returns:</p> Type Description <code>dtype</code> <p>torch.dtype: The data type of the model's parameters.</p>"},{"location":"models/#unitorch.models.GenericModel.init_weights","title":"init_weights","text":"<pre><code>init_weights()\n</code></pre> <p>Initialize the weights of the model.</p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def init_weights(self):\n    \"\"\"\n    Initialize the weights of the model.\n    \"\"\"\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"models/#hftextgenerationprocessor","title":"HfTextGenerationProcessor","text":"<p>Processor for text generation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer to use for text generation.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generated sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    self.tokenizer = tokenizer\n    self.max_seq_length = max_seq_length\n    self.max_gen_seq_length = max_gen_seq_length\n    self.pad_token = self.tokenizer.pad_token\n    self.bos_token = self.tokenizer.bos_token\n    self.eos_token = self.tokenizer.eos_token\n    self.mask_token = self.tokenizer.mask_token\n    self.pad_token_id = self.tokenizer.pad_token_id\n    self.vocab_size = len(self.tokenizer.get_vocab())\n</code></pre>"},{"location":"models/#unitorch.models.HfTextGenerationProcessor.detokenize","title":"detokenize","text":"<pre><code>detokenize(\n    sequences: torch.Tensor,\n    skip_special_tokens: Optional[bool] = True,\n)\n</code></pre> <p>Detokenize the sequences.</p> <p>Parameters:</p> Name Type Description Default <code>sequences</code> <code>Tensor</code> <p>The sequences to detokenize.</p> required <code>skip_special_tokens</code> <code>bool</code> <p>Whether to skip special tokens. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>list</code> <p>The detokenized sequences.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def detokenize(\n    self,\n    sequences: torch.Tensor,\n    skip_special_tokens: Optional[bool] = True,\n):\n    \"\"\"\n    Detokenize the sequences.\n\n    Args:\n        sequences (torch.Tensor): The sequences to detokenize.\n        skip_special_tokens (bool, optional): Whether to skip special tokens. Defaults to True.\n\n    Returns:\n        list: The detokenized sequences.\n    \"\"\"\n    if sequences.dim() == 3:\n        _, num_return_sequences, sequences_length = sequences.size()\n        sequences = sequences.reshape(-1, sequences_length).clamp_max(\n            self.vocab_size\n        )\n        sequences = sequences.clamp_min(0)\n        sequences[sequences == self.vocab_size] = self.pad_token_id\n        decode_tokens = self.tokenizer.batch_decode(\n            sequences,\n            skip_special_tokens=skip_special_tokens,\n        )\n        decode_tokens = [\n            decode_tokens[i : i + num_return_sequences]\n            for i in range(0, len(decode_tokens), num_return_sequences)\n        ]\n    elif sequences.dim() == 2:\n        sequences = sequences.clamp_min(0).clamp_max(self.vocab_size)\n        sequences[sequences == self.vocab_size] = self.pad_token_id\n        decode_tokens = self.tokenizer.batch_decode(\n            sequences,\n            skip_special_tokens=skip_special_tokens,\n        )\n    else:\n        raise ValueError(f\"Can't decode the tensor with shape {sequences.shape}\")\n\n    return decode_tokens\n</code></pre>"},{"location":"models/#unitorch.models.HfTextGenerationProcessor.generation","title":"generation","text":"<pre><code>generation(\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Generate inputs, labels, and tokens for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>text_pair</code> <code>str</code> <p>The paired text.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generated sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated input tokens, attention masks, label tokens, and attention masks.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def generation(\n    self,\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Generate inputs, labels, and tokens for text generation.\n\n    Args:\n        text (str): The input text.\n        text_pair (str): The paired text.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n        max_gen_seq_length (int, optional): Maximum generated sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated input tokens, attention masks, label tokens, and attention masks.\n    \"\"\"\n    max_seq_length = pop_value(max_seq_length, self.max_seq_length)\n    max_gen_seq_length = pop_value(max_gen_seq_length, self.max_gen_seq_length)\n\n    tokens = self.generation_inputs(text, max_seq_length)\n    tokens_pair = self.generation_inputs(text_pair, max_gen_seq_length)\n    labels = self.generation_labels(text_pair, max_gen_seq_length)\n\n    return GenericOutputs(\n        input_ids=tokens.input_ids,\n        attention_mask=tokens.attention_mask,\n        input_ids_pair=tokens_pair.input_ids,\n        attention_mask_pair=tokens_pair.attention_mask,\n        input_ids_label=labels.input_ids,\n        attention_mask_label=labels.attention_mask,\n    )\n</code></pre>"},{"location":"models/#unitorch.models.HfTextGenerationProcessor.generation_inputs","title":"generation_inputs","text":"<pre><code>generation_inputs(\n    text: str, max_seq_length: Optional[int] = None\n)\n</code></pre> <p>Generate inputs for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated input tokens and attention mask.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def generation_inputs(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Generate inputs for text generation.\n\n    Args:\n        text (str): The input text.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated input tokens and attention mask.\n    \"\"\"\n    max_seq_length = pop_value(max_seq_length, self.max_seq_length)\n    tokens = self.tokenizer.tokenize(str(text))\n    tokens = tokens[: max_seq_length - 2]\n    tokens = [self.bos_token] + tokens + [self.eos_token]\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = input_ids[:max_seq_length]\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += [self.pad_token_id] * len(padding)\n    attention_mask += padding\n\n    assert len(input_ids) == max_seq_length\n    assert len(attention_mask) == max_seq_length\n\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/#unitorch.models.HfTextGenerationProcessor.generation_labels","title":"generation_labels","text":"<pre><code>generation_labels(\n    text: str, max_gen_seq_length: Optional[int] = None\n)\n</code></pre> <p>Generate labels for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generated sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated label tokens and attention mask.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def generation_labels(\n    self,\n    text: str,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Generate labels for text generation.\n\n    Args:\n        text (str): The input text.\n        max_gen_seq_length (int, optional): Maximum generated sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated label tokens and attention mask.\n    \"\"\"\n    max_gen_seq_length = pop_value(max_gen_seq_length, self.max_gen_seq_length)\n    tokens = self.tokenizer.tokenize(str(text))\n    tokens = tokens[: max_gen_seq_length - 2]\n    tokens = [self.bos_token] + tokens + [self.eos_token]\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = input_ids[1:max_gen_seq_length]\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_gen_seq_length - len(input_ids))\n    input_ids += [self.pad_token_id] * len(padding)\n    attention_mask += padding\n\n    assert len(input_ids) == max_gen_seq_length\n\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/#hftextclassificationprocessor","title":"HfTextClassificationProcessor","text":"<p>Processor for text classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer to use for text classification.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>Source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>Target type ID. Defaults to 1.</p> <code>1</code> <code>position_start_id</code> <code>int</code> <p>Start position ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def __init__(\n    self,\n    tokenizer: PreTrainedTokenizer,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 1,\n    position_start_id: Optional[int] = 0,\n):\n    self.tokenizer = tokenizer\n    self.max_seq_length = max_seq_length\n    self.pad_token = self.tokenizer.pad_token\n    self.sep_token = self.tokenizer.sep_token\n    self.cls_token = self.tokenizer.cls_token\n    self.mask_token = self.tokenizer.mask_token\n    self.pad_token_id = self.tokenizer.pad_token_id\n    self.source_type_id = source_type_id\n    self.target_type_id = target_type_id\n    self.position_start_id = position_start_id\n</code></pre>"},{"location":"models/#unitorch.models.HfTextClassificationProcessor.classification","title":"classification","text":"<pre><code>classification(\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Generate inputs for text classification.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>text_pair</code> <code>str</code> <p>The paired text. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated input tokens, token type IDs, attention mask, and position IDs.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def classification(\n    self,\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Generate inputs for text classification.\n\n    Args:\n        text (str): The input text.\n        text_pair (str, optional): The paired text. Defaults to None.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated input tokens, token type IDs, attention mask, and position IDs.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n\n    tokens = self.tokenizer.tokenize(str(text))\n    if text_pair is None:\n        tokens = tokens[: max_seq_length - 2]\n        tokens = [self.cls_token] + tokens + [self.sep_token]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        token_type_ids = [self.source_type_id] * len(input_ids)\n        attention_mask = [1] * len(input_ids)\n    else:\n        tokens_pair = self.tokenizer.tokenize(str(text_pair))\n        truncate_sequence_pair(tokens, tokens_pair, max_seq_length - 3)\n        token_type_ids = (\n            [self.source_type_id]\n            + [self.source_type_id] * len(tokens)\n            + [self.source_type_id]\n            + [self.target_type_id] * len(tokens_pair)\n            + [self.target_type_id]\n        )\n        tokens = (\n            [self.cls_token]\n            + tokens\n            + [self.sep_token]\n            + tokens_pair\n            + [self.sep_token]\n        )\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += len(padding) * [self.pad_token_id]\n    attention_mask += padding\n    token_type_ids += len(padding) * [self.target_type_id]\n\n    assert len(input_ids) == max_seq_length\n    assert len(attention_mask) == max_seq_length\n    assert len(token_type_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        token_type_ids=torch.tensor(token_type_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n        position_ids=torch.tensor(\n            list(\n                range(\n                    self.position_start_id,\n                    self.position_start_id + max_seq_length,\n                )\n            ),\n            dtype=torch.long,\n        ),\n    )\n</code></pre>"},{"location":"models/#hfimageclassificationprocessor","title":"HfImageClassificationProcessor","text":"<p>Processor for image classification tasks.</p> <p>Initialize the HfImageClassificationProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_processor</code> <code>BaseImageProcessor</code> <p>The vision processor object used for image transformations.</p> required Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def __init__(\n    self,\n    vision_processor: BaseImageProcessor,\n):\n    \"\"\"\n    Initialize the HfImageClassificationProcessor.\n\n    Args:\n        vision_processor (BaseImageProcessor): The vision processor object used for image transformations.\n    \"\"\"\n    self.vision_processor = vision_processor\n\n    self.size = getattr(self.vision_processor, \"size\", None)\n\n    self.resample = getattr(self.vision_processor, \"resample\", None)\n\n    self.crop_size = getattr(self.vision_processor, \"crop_size\", None)\n    self.pad_size = getattr(self.vision_processor, \"pad_size\", None)\n\n    self.rescale_factor = getattr(self.vision_processor, \"rescale_factor\", None)\n\n    self.image_mean = getattr(self.vision_processor, \"image_mean\", None)\n    self.image_std = getattr(self.vision_processor, \"image_std\", None)\n</code></pre>"},{"location":"models/#unitorch.models.HfImageClassificationProcessor.classification","title":"classification","text":"<pre><code>classification(image: Union[Image.Image, str])\n</code></pre> <p>Perform image classification on the given image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image.</p> required <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The output of the image classification, including pixel values.</p> Source code in <code>src/unitorch/models/processing_utils.py</code> <pre><code>def classification(\n    self,\n    image: Union[Image.Image, str],\n):\n    \"\"\"\n    Perform image classification on the given image.\n\n    Args:\n        image (Image.Image): The input image.\n\n    Returns:\n        GenericOutputs: The output of the image classification, including pixel values.\n    \"\"\"\n    if isinstance(image, str):\n        image = Image.open(image)\n\n    if self.size is not None:\n        image = self.vision_processor.resize(\n            image=to_numpy_array(image.convert(\"RGB\")),\n            size=self.size,\n            resample=self.resample,\n        )\n\n    if self.crop_size is not None:\n        image = self.vision_processor.center_crop(\n            image,\n            size=self.crop_size,\n        )\n\n    if self.rescale_factor is not None:\n        image = self.vision_processor.rescale(\n            image,\n            scale=self.rescale_factor,\n        )\n\n    if self.image_mean is not None and self.image_std is not None:\n        image = self.vision_processor.normalize(\n            image=image,\n            mean=self.image_mean,\n            std=self.image_std,\n        )\n\n    if self.pad_size is not None:\n        image = self.vision_processor.pad_image(\n            image,\n            size=self.pad_size,\n        )\n\n    image = to_channel_dimension_format(image, ChannelDimension.FIRST)\n\n    return GenericOutputs(\n        pixel_values=torch.tensor(image),\n    )\n</code></pre>"},{"location":"models/#exponentialmovingaverage","title":"ExponentialMovingAverage","text":"<p>             Bases: <code>Module</code></p> <p>Exponential Moving Average (EMA) for model parameters.</p> <p>Initializes the ExponentialMovingAverage.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to apply EMA to.</p> required <code>decay</code> <code>float</code> <p>Decay rate for the EMA. Defaults to 0.9999.</p> <code>0.9999</code> <code>tau</code> <code>int</code> <p>Time constant for the EMA. Defaults to 2000.</p> <code>2000</code> <code>num_steps</code> <code>int</code> <p>Number of steps taken for the EMA. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/modeling_ema.py</code> <pre><code>def __init__(\n    self,\n    model,\n    decay: Optional[float] = 0.9999,\n    tau: Optional[int] = 2000,\n    num_steps: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes the ExponentialMovingAverage.\n\n    Args:\n        model (nn.Module): The model to apply EMA to.\n        decay (float, optional): Decay rate for the EMA. Defaults to 0.9999.\n        tau (int, optional): Time constant for the EMA. Defaults to 2000.\n        num_steps (int, optional): Number of steps taken for the EMA. Defaults to 0.\n    \"\"\"\n    super().__init__()\n    self.model = deepcopy(model)\n    self.num_steps = num_steps\n    self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n\n    for p in self.model.parameters():\n        p.requires_grad = False\n</code></pre>"},{"location":"models/#unitorch.models.ExponentialMovingAverage.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> <p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <p>The output of the model.</p> Source code in <code>src/unitorch/models/modeling_ema.py</code> <pre><code>def forward(self, *args, **kwargs):\n    \"\"\"\n    Forward pass through the model.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        The output of the model.\n    \"\"\"\n    return self.model(*args, **kwargs)\n</code></pre>"},{"location":"models/#unitorch.models.ExponentialMovingAverage.from_checkpoint","title":"from_checkpoint","text":"<pre><code>from_checkpoint(\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Load model weights from a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory path of the checkpoint.</p> required <code>weight_name</code> <code>str</code> <p>Name of the weight file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/unitorch/models/modeling_ema.py</code> <pre><code>def from_checkpoint(\n    self,\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Load model weights from a checkpoint.\n\n    Args:\n        ckpt_dir (str): Directory path of the checkpoint.\n        weight_name (str): Name of the weight file.\n\n    Returns:\n        None\n    \"\"\"\n    if weight_name is None:\n        weight_name = self.checkpoint_name\n    weight_path = os.path.join(ckpt_dir, weight_name)\n    if not os.path.exists(weight_path):\n        return\n    state_dict = torch.load(weight_path, map_location=\"cpu\")\n    self.model.load_state_dict(state_dict)\n    logging.info(\n        f\"{type(self).__name__} model load weight from checkpoint {weight_path}\"\n    )\n</code></pre>"},{"location":"models/#unitorch.models.ExponentialMovingAverage.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs\n)\n</code></pre> <p>Save the model's current state as a checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory path to save the checkpoint.</p> required <code>weight_name</code> <code>str</code> <p>Name of the weight file.</p> <code>None</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>src/unitorch/models/modeling_ema.py</code> <pre><code>def save_checkpoint(\n    self,\n    ckpt_dir: str,\n    weight_name: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Save the model's current state as a checkpoint.\n\n    Args:\n        ckpt_dir (str): Directory path to save the checkpoint.\n        weight_name (str): Name of the weight file.\n\n    Returns:\n        None\n    \"\"\"\n    if weight_name is None:\n        weight_name = self.checkpoint_name\n    state_dict = self.model.state_dict()\n    weight_path = os.path.join(ckpt_dir, weight_name)\n    torch.save(state_dict, weight_path)\n    logging.info(f\"{type(self).__name__} model save checkpoint to {weight_path}\")\n</code></pre>"},{"location":"models/#unitorch.models.ExponentialMovingAverage.step","title":"step","text":"<pre><code>step(model)\n</code></pre> <p>Performs a step of EMA.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to update the EMA with.</p> required Source code in <code>src/unitorch/models/modeling_ema.py</code> <pre><code>@torch.no_grad()\ndef step(self, model):\n    \"\"\"\n    Performs a step of EMA.\n\n    Args:\n        model (nn.Module): The model to update the EMA with.\n    \"\"\"\n    self.num_steps += 1\n    rate = self.decay(self.num_steps)\n\n    new_state = model.state_dict()\n    for key, value in self.model.state_dict().items():\n        if not value.dtype.is_floating_point:\n            continue\n        value *= rate\n        value += (1 - rate) * new_state[key].detach()\n</code></pre>"},{"location":"models/bart/","title":"unitorch.models.bart","text":""},{"location":"models/bart/#bartprocessor","title":"BartProcessor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> <p>Processor for BART model. Inherits from HfTextGenerationProcessor.</p> <p>Initializes the BartProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>Path to the BPE merge file.</p> required <code>special_input_ids</code> <code>Optional[Dict]</code> <p>Optional dictionary of special input IDs.</p> <code>dict()</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length.</p> <code>128</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>Maximum generation sequence length.</p> <code>48</code> Source code in <code>src/unitorch/models/bart/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes the BartProcessor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        merge_path (str): Path to the BPE merge file.\n        special_input_ids (Optional[Dict]): Optional dictionary of special input IDs.\n        max_seq_length (Optional[int]): Maximum sequence length.\n        max_gen_seq_length (Optional[int]): Maximum generation sequence length.\n    \"\"\"\n    tokenizer = get_bart_tokenizer(\n        vocab_path,\n        merge_path,\n        special_input_ids=special_input_ids,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/bart/#bartforgeneration","title":"BartForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>BART model for text generation.</p> <p>Initializes a BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the BART model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing during training. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/bart/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a BartForGeneration model.\n\n    Args:\n        config_path (str): Path to the BART model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing during training. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BartConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.config.forced_bos_token_id = None\n    self.model = BartForConditionalGeneration(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/bart/#unitorch.models.bart.BartForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs forward pass of the BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor indicating which tokens should be attended to.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor indicating which decoder tokens should be attended to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits of the model.</p> Source code in <code>src/unitorch/models/bart/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs forward pass of the BartForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor indicating which tokens should be attended to.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor indicating which decoder tokens should be attended to.\n\n    Returns:\n        (torch.Tensor): Output logits of the model.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/bart/#unitorch.models.bart.BartForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates text using the BartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 2.</p> <code>2</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Repetition penalty for generated sequences. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to avoid repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Length penalty for generated sequences. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Diversity penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling during generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Value for top-p sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Generated sequences and their scores.</p> Source code in <code>src/unitorch/models/bart/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates text using the BartForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): Number of sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): Repetition penalty for generated sequences. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to avoid repetition. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): Length penalty for generated sequences. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Diversity penalty for diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling during generation. Defaults to False.\n        temperature (float, optional): Temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): Value for top-k sampling. Defaults to 50.\n        top_p (float, optional): Value for top-p sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: Generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences, sequences_scores=outputs.sequences_scores\n    )\n</code></pre>"},{"location":"models/beit/","title":"unitorch.models.beit","text":""},{"location":"models/beit/#beitprocessor","title":"BeitProcessor","text":"<p>             Bases: <code>HfImageClassificationProcessor</code></p> <p>Initializes the BeitProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required Source code in <code>src/unitorch/models/beit/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initializes the BeitProcessor.\n\n    Args:\n        vision_config_path (str): The path to the vision configuration file.\n    \"\"\"\n    vision_processor = BeitImageProcessor.from_json_file(vision_config_path)\n    super().__init__(\n        vision_processor=vision_processor,\n    )\n</code></pre>"},{"location":"models/beit/#beitforimageclassification","title":"BeitForImageClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BeitForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/models/beit/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initializes the BeitForImageClassification model.\n\n    Args:\n        config_path (str): The path to the configuration file.\n        num_classes (Optional[int], optional): The number of classes for classification. Defaults to 1.\n    \"\"\"\n    super().__init__()\n    config = BeitConfig.from_json_file(config_path)\n\n    self.beit = BeitModel(config, add_pooling_layer=True)\n    self.classifier = nn.Linear(config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/beit/#unitorch.models.beit.BeitForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the BeitForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input tensor of pixel values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The logits of the model output.</p> Source code in <code>src/unitorch/models/beit/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BeitForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The input tensor of pixel values.\n\n    Returns:\n        (torch.Tensor):The logits of the model output.\n    \"\"\"\n    outputs = self.beit(\n        pixel_values=pixel_values,\n    )\n\n    pooled_output = outputs[1]\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/bert/","title":"unitorch.models.bert","text":""},{"location":"models/bert/#bertprocessor","title":"BertProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> <p>Initializes the BertProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>special_input_ids</code> <code>Optional[Dict]</code> <p>Special input IDs mapping. Defaults to an empty dictionary.</p> <code>dict()</code> <code>do_lower_case</code> <code>Optional[bool]</code> <p>Whether to perform lowercase tokenization. Defaults to True.</p> <code>True</code> <code>do_basic_tokenize</code> <code>Optional[bool]</code> <p>Whether to perform basic tokenization. Defaults to True.</p> <code>True</code> <code>do_whole_word_mask</code> <code>Optional[bool]</code> <p>Whether to perform whole word masking. Defaults to True.</p> <code>True</code> <code>masked_lm_prob</code> <code>Optional[float]</code> <p>The probability of masking a token for pretraining. Defaults to 0.15.</p> <code>0.15</code> <code>max_predictions_per_seq</code> <code>Optional[int]</code> <p>The maximum number of masked tokens per sequence for pretraining. Defaults to 20.</p> <code>20</code> Source code in <code>src/unitorch/models/bert/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    special_input_ids: Optional[Dict] = dict(),\n    do_lower_case: Optional[bool] = True,\n    do_basic_tokenize: Optional[bool] = True,\n    do_whole_word_mask: Optional[bool] = True,\n    masked_lm_prob: Optional[float] = 0.15,\n    max_predictions_per_seq: Optional[int] = 20,\n):\n    \"\"\"\n    Initializes the BertProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (Optional[int], optional): The maximum sequence length. Defaults to 128.\n        special_input_ids (Optional[Dict], optional): Special input IDs mapping. Defaults to an empty dictionary.\n        do_lower_case (Optional[bool], optional): Whether to perform lowercase tokenization. Defaults to True.\n        do_basic_tokenize (Optional[bool], optional): Whether to perform basic tokenization. Defaults to True.\n        do_whole_word_mask (Optional[bool], optional): Whether to perform whole word masking. Defaults to True.\n        masked_lm_prob (Optional[float], optional): The probability of masking a token for pretraining. Defaults to 0.15.\n        max_predictions_per_seq (Optional[int], optional): The maximum number of masked tokens per sequence for pretraining. Defaults to 20.\n    \"\"\"\n    tokenizer = get_bert_tokenizer(\n        vocab_path,\n        do_lower_case=do_lower_case,\n        do_basic_tokenize=do_basic_tokenize,\n        special_input_ids=special_input_ids,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n    )\n    self.do_whole_word_mask = do_whole_word_mask\n    self.masked_lm_prob = masked_lm_prob\n    self.max_predictions_per_seq = max_predictions_per_seq\n    self.vocab_words = list(self.tokenizer.vocab.keys())\n</code></pre>"},{"location":"models/bert/#unitorch.models.bert.BertProcessor.pretrain","title":"pretrain","text":"<pre><code>pretrain(\n    text: str,\n    text_pair: str,\n    nsp_label: int,\n    max_seq_length: Optional[int] = None,\n    masked_lm_prob: Optional[float] = None,\n    do_whole_word_mask: Optional[bool] = None,\n    max_predictions_per_seq: Optional[int] = None,\n)\n</code></pre> <p>The Bert pretrain processor on the given text and text pair.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>text_pair</code> <code>str</code> <p>The input text pair.</p> required <code>nsp_label</code> <code>int</code> <p>The next sentence prediction label.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to None.</p> <code>None</code> <code>masked_lm_prob</code> <code>Optional[float]</code> <p>The probability of masking a token for pretraining. Defaults to None.</p> <code>None</code> <code>do_whole_word_mask</code> <code>Optional[bool]</code> <p>Whether to perform whole word masking. Defaults to None.</p> <code>None</code> <code>max_predictions_per_seq</code> <code>Optional[int]</code> <p>The maximum number of masked tokens per sequence for pretraining. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>pretrain processing outputs.</p> Source code in <code>src/unitorch/models/bert/processing.py</code> <pre><code>def pretrain(\n    self,\n    text: str,\n    text_pair: str,\n    nsp_label: int,\n    max_seq_length: Optional[int] = None,\n    masked_lm_prob: Optional[float] = None,\n    do_whole_word_mask: Optional[bool] = None,\n    max_predictions_per_seq: Optional[int] = None,\n):\n    \"\"\"\n    The Bert pretrain processor on the given text and text pair.\n\n    Args:\n        text (str): The input text.\n        text_pair (str): The input text pair.\n        nsp_label (int): The next sentence prediction label.\n        max_seq_length (Optional[int], optional): The maximum sequence length. Defaults to None.\n        masked_lm_prob (Optional[float], optional): The probability of masking a token for pretraining. Defaults to None.\n        do_whole_word_mask (Optional[bool], optional): Whether to perform whole word masking. Defaults to None.\n        max_predictions_per_seq (Optional[int], optional): The maximum number of masked tokens per sequence for pretraining. Defaults to None.\n\n    Returns:\n        GenericOutputs: pretrain processing outputs.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n\n    masked_lm_prob = pop_value(\n        masked_lm_prob,\n        self.masked_lm_prob,\n    )\n\n    do_whole_word_mask = pop_value(\n        do_whole_word_mask,\n        self.do_whole_word_mask,\n    )\n\n    max_predictions_per_seq = pop_value(\n        max_predictions_per_seq,\n        self.max_predictions_per_seq,\n    )\n\n    _tokens = self.tokenizer.tokenize(str(text))\n    tokens_pair = self.tokenizer.tokenize(str(text_pair))\n    truncate_sequence_pair(_tokens, tokens_pair, max_seq_length - 3)\n    tokens = (\n        [self.cls_token]\n        + _tokens\n        + [self.sep_token]\n        + tokens_pair\n        + [self.sep_token]\n    )\n\n    covered_indexes = get_random_mask_indexes(\n        tokens,\n        masked_lm_prob,\n        do_whole_word_mask,\n        max_predictions_per_seq,\n        special_tokens=[self.cls_token, self.sep_token],\n    )\n    label = [\n        tokens[pos] if pos in covered_indexes else self.pad_token\n        for pos in range(max_seq_length)\n    ]\n    label_mask = [\n        1 if pos in covered_indexes else 0 for pos in range(max_seq_length)\n    ]\n    label = self.tokenizer.convert_tokens_to_ids(label)\n\n    for index in covered_indexes:\n        mask_token = None\n        if random.random() &lt; 0.8:\n            mask_token = self.mask_token\n        else:\n            mask_token = (\n                tokens[index]\n                if random.random() &lt; 0.5\n                else get_random_word(self.vocab_words)\n            )\n        tokens[index] = mask_token\n\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    token_type_ids = [0] + [0] * len(_tokens) + [0] + [1] * len(tokens_pair) + [1]\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    input_ids += len(padding) * [self.pad_token_id]\n    attention_mask += padding\n    token_type_ids += len(padding) * [1]\n\n    assert len(input_ids) == max_seq_length\n    assert len(attention_mask) == max_seq_length\n    assert len(token_type_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        token_type_ids=torch.tensor(token_type_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n        position_ids=torch.tensor(list(range(max_seq_length)), dtype=torch.long),\n        nsp_label=torch.tensor(int(nsp_label), dtype=torch.long),\n        mlm_label=torch.tensor(label, dtype=torch.long),\n        mlm_label_mask=torch.tensor(label_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bert/#bertforclassification","title":"BertForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/bert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the BertForClassification model.\n\n    Args:\n        config_path (str): The path to the configuration file.\n        num_classes (Optional[int], optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BertConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.bert = BertModel(self.config)\n    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/bert/#unitorch.models.bert.BertForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the BertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor of token indices.</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Tensor]</code> <p>The token type IDs tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Optional[Tensor]</code> <p>The position IDs tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The logits of the model output.</p> Source code in <code>src/unitorch/models/bert/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the BertForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor of token indices.\n        attention_mask (torch.Tensor optional): The attention mask tensor. Defaults to None.\n        token_type_ids (torch.Tensor optional): The token type IDs tensor. Defaults to None.\n        position_ids (torch.Tensor optional): The position IDs tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The logits of the model output.\n    \"\"\"\n    outputs = self.bert(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = outputs[1]\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/blip/","title":"unitorch.models.blip","text":""},{"location":"models/blip/#blipprocessor","title":"BlipProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code>, <code>HfTextGenerationProcessor</code>, <code>HfImageClassificationProcessor</code></p> <p>Initializes the BlipProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for text inputs. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for generated outputs. Defaults to 48.</p> <code>48</code> <code>position_start_id</code> <code>Optional[int]</code> <p>The position start ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    vision_config_path: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n    position_start_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes the BlipProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        vision_config_path (str): The path to the vision configuration file.\n        max_seq_length (Optional[int]): The maximum sequence length for text inputs. Defaults to 128.\n        max_gen_seq_length (Optional[int]): The maximum sequence length for generated outputs. Defaults to 48.\n        position_start_id (Optional[int]): The position start ID. Defaults to 0.\n    \"\"\"\n    vision_processor = BlipImageProcessor.from_json_file(vision_config_path)\n    HfImageClassificationProcessor.__init__(self, vision_processor=vision_processor)\n\n    tokenizer = BertTokenizer(\n        vocab_file=vocab_path,\n    )\n    tokenizer.bos_token = tokenizer.cls_token\n    tokenizer.bos_token_id = tokenizer.cls_token_id\n    tokenizer.eos_token = tokenizer.sep_token\n    tokenizer.eos_token_id = tokenizer.sep_token_id\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=0,\n        target_type_id=0,\n        position_start_id=position_start_id,\n    )\n\n    HfTextGenerationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.classification","title":"classification","text":"<pre><code>classification(\n    text: str,\n    image: Union[Image.Image, str],\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs\n</code></pre> <p>Performs classification using both text and image inputs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to classify.</p> required <code>image</code> <code>Image</code> <p>The input image to classify.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for the text. If None, the default value from initialization is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The outputs of the classification.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def classification(\n    self,\n    text: str,\n    image: Union[Image.Image, str],\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Performs classification using both text and image inputs.\n\n    Args:\n        text (str): The input text to classify.\n        image (PIL.Image.Image): The input image to classify.\n        max_seq_length (Optional[int]): The maximum sequence length for the text. If None, the default value from initialization is used.\n\n    Returns:\n        GenericOutputs: The outputs of the classification.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n\n    text_outputs = self.text_classification(text, max_seq_length)\n    pixel_outputs = self.image_classification(image)\n\n    return GenericOutputs(\n        input_ids=text_outputs.input_ids,\n        attention_mask=text_outputs.attention_mask,\n        position_ids=text_outputs.position_ids,\n        pixel_values=pixel_outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.generation","title":"generation","text":"<pre><code>generation(\n    text: str,\n    image: Union[Image.Image, str],\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs\n</code></pre> <p>Generate inputs, labels, and tokens for image to text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>image</code> <code>Image</code> <p>The input image to caption.</p> required <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generated sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The generated input tokens, attention masks, label tokens, and attention masks.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def generation(\n    self,\n    text: str,\n    image: Union[Image.Image, str],\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Generate inputs, labels, and tokens for image to text generation.\n\n    Args:\n        text (str): The input text.\n        image (Image.Image): The input image to caption.\n        max_gen_seq_length (int, optional): Maximum generated sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated input tokens, attention masks, label tokens, and attention masks.\n    \"\"\"\n\n    max_gen_seq_length = pop_value(max_gen_seq_length, self.max_gen_seq_length)\n\n    tokens = self.generation_inputs(text, max_gen_seq_length)\n    pixels = self.image_classification(image)\n    labels = self.generation_labels(text, max_gen_seq_length)\n\n    return GenericOutputs(\n        input_ids=tokens.input_ids,\n        attention_mask=tokens.attention_mask,\n        pixel_values=pixels.pixel_values,\n        input_ids_label=labels.input_ids,\n        attention_mask_label=labels.attention_mask,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.generation_inputs","title":"generation_inputs","text":"<pre><code>generation_inputs(\n    text: str, max_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Generate inputs for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The generated input tokens and attention mask.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def generation_inputs(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Generate inputs for text generation.\n\n    Args:\n        text (str): The input text.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The generated input tokens and attention mask.\n    \"\"\"\n    outputs = HfTextGenerationProcessor.generation_inputs(\n        self,\n        text=text,\n        max_seq_length=max_seq_length,\n    )\n    return GenericOutputs(\n        input_ids=outputs.input_ids,\n        attention_mask=outputs.attention_mask,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.generation_labels","title":"generation_labels","text":"<pre><code>generation_labels(\n    text: str, max_gen_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Generates labels for text generation based on the given input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for generating labels.</p> required <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for the generated labels. If None, the default value from initialization is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The generated labels.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def generation_labels(\n    self,\n    text: str,\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Generates labels for text generation based on the given input text.\n\n    Args:\n        text (str): The input text for generating labels.\n        max_gen_seq_length (Optional[int]): The maximum sequence length for the generated labels. If None, the default value from initialization is used.\n\n    Returns:\n        GenericOutputs: The generated labels.\n    \"\"\"\n    outputs = HfTextGenerationProcessor.generation_labels(\n        self,\n        text=text,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n    return GenericOutputs(\n        input_ids=outputs.input_ids,\n        attention_mask=outputs.attention_mask,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.image_classification","title":"image_classification","text":"<pre><code>image_classification(\n    image: Union[Image.Image, str]\n) -&gt; GenericOutputs\n</code></pre> <p>Performs image classification on the given input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image to classify.</p> required <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The outputs of the image classification.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def image_classification(\n    self,\n    image: Union[Image.Image, str],\n) -&gt; GenericOutputs:\n    \"\"\"\n    Performs image classification on the given input image.\n\n    Args:\n        image (PIL.Image.Image): The input image to classify.\n\n    Returns:\n        GenericOutputs: The outputs of the image classification.\n    \"\"\"\n    outputs = HfImageClassificationProcessor.classification(\n        self,\n        image=image,\n    )\n\n    return GenericOutputs(\n        pixel_values=outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipProcessor.text_classification","title":"text_classification","text":"<pre><code>text_classification(\n    text: str, max_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Performs text classification on the given input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to classify.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for the text. If None, the default value from initialization is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The outputs of the text classification.</p> Source code in <code>src/unitorch/models/blip/processing.py</code> <pre><code>def text_classification(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Performs text classification on the given input text.\n\n    Args:\n        text (str): The input text to classify.\n        max_seq_length (Optional[int]): The maximum sequence length for the text. If None, the default value from initialization is used.\n\n    Returns:\n        GenericOutputs: The outputs of the text classification.\n    \"\"\"\n    outputs = HfTextClassificationProcessor.classification(\n        self,\n        text=text,\n        max_seq_length=max_seq_length,\n    )\n    return GenericOutputs(\n        input_ids=outputs.input_ids,\n        attention_mask=outputs.attention_mask,\n        position_ids=outputs.position_ids,\n    )\n</code></pre>"},{"location":"models/blip/#blipforpretrain","title":"BlipForPretrain","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BlipForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>projection_dim</code> <code>Optional[int]</code> <p>Dimension of the projection. Defaults to 512.</p> <code>512</code> <code>freeze_base_model</code> <code>Optional[bool]</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> <code>use_all_gather</code> <code>Optional[bool]</code> <p>Whether to use all_gather operation for distributed training. Defaults to True.</p> <code>True</code> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n    use_all_gather: Optional[bool] = True,\n):\n    \"\"\"\n    Initializes the BlipForPretrain model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        projection_dim (Optional[int], optional): Dimension of the projection. Defaults to 512.\n        freeze_base_model (Optional[bool], optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n        use_all_gather (Optional[bool], optional): Whether to use all_gather operation for distributed training. Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    config = BlipConfig.from_json_file(config_path)\n    text_config = config.text_config\n    vision_config = config.vision_config\n    text_config.gradient_checkpointing = gradient_checkpointing\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.use_all_gather = use_all_gather\n\n    self.text_embed_dim = text_config.hidden_size\n    self.vision_embed_dim = vision_config.hidden_size\n\n    self.text_model = BlipTextModel(text_config)\n    self.vision_model = BlipVisionModel(vision_config)\n\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.logit_scale = nn.Parameter(torch.ones([]) * config.logit_scale_init_value)\n\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the BlipForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>Pixel values of the images.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask for the input.</p> required <code>position_ids</code> <code>Tensor</code> <p>Position IDs for the input.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output loss for the pretraining task.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BlipForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        pixel_values (torch.Tensor): Pixel values of the images.\n        attention_mask (torch.Tensor): Attention mask for the input.\n        position_ids (torch.Tensor): Position IDs for the input.\n\n    Returns:\n        (torch.Tensor):Output loss for the pretraining task.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    image_embeds = vision_outputs[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    # normalized features\n    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n\n    logit_scale = self.logit_scale.exp()\n    if self.use_all_gather and dist.is_initialized():\n        text_embeds = self._all_gather(text_embeds)\n        image_embeds = self._all_gather(image_embeds)\n    logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n    return _blip_loss(logits_per_text)\n</code></pre>"},{"location":"models/blip/#blipforclassification","title":"BlipForClassification","text":"<p>             Bases: <code>Module</code></p> <p>Initializes the BlipForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>projection_dim</code> <code>Optional[int]</code> <p>Dimension of the projection. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>Optional[bool]</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the BlipForClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        projection_dim (Optional[int], optional): Dimension of the projection. Defaults to 512.\n        num_classes (Optional[int], optional): Number of classes for classification. Defaults to 1.\n        freeze_base_model (Optional[bool], optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    # Load the BLIP model configuration\n    config = BlipConfig.from_json_file(config_path)\n    text_config = config.text_config\n    vision_config = config.vision_config\n\n    # Set gradient checkpointing option\n    text_config.gradient_checkpointing = gradient_checkpointing\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n\n    self.text_embed_dim = text_config.hidden_size\n    self.vision_embed_dim = vision_config.hidden_size\n\n    # Initialize the text and vision models\n    self.text_model = BlipTextModel(text_config)\n    self.vision_model = BlipVisionModel(vision_config)\n\n    # Projection layers for text and vision embeddings\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n\n    # Classifier layer\n    self.classifier = nn.Linear(self.projection_dim * 2, num_classes)\n\n    # Initialize the weights of the model\n    self.init_weights()\n\n    if freeze_base_model:\n        # Freeze the parameters of the base models if specified\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    # Set gradient checkpointing option for encoders\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the BlipForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>pixel_values</code> <code>Tensor</code> <p>Pixel values of the images.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask for the input.</p> required <code>position_ids</code> <code>Tensor</code> <p>Position IDs for the input.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits for classification.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BlipForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        pixel_values (torch.Tensor): Pixel values of the images.\n        attention_mask (torch.Tensor): Attention mask for the input.\n        position_ids (torch.Tensor): Position IDs for the input.\n\n    Returns:\n        (torch.Tensor):Output logits for classification.\n    \"\"\"\n    # Process the vision modality\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    # Process the text modality\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    # Project vision embeddings to the specified dimensionality\n    image_embeds = vision_outputs[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # Project text embeddings to the specified dimensionality\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    # Concatenate and classify the projected embeddings\n    return self.classifier(F.relu(torch.cat([image_embeds, text_embeds], axis=1)))\n</code></pre>"},{"location":"models/blip/#blipfortextclassification","title":"BlipForTextClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BlipForTextClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>projection_dim</code> <code>Optional[int]</code> <p>Dimension of the projection. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>Optional[bool]</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the BlipForTextClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        projection_dim (Optional[int], optional): Dimension of the projection. Defaults to 512.\n        num_classes (Optional[int], optional): Number of classes for classification. Defaults to 1.\n        freeze_base_model (Optional[bool], optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    # Load the BLIP model configuration\n    config = BlipConfig.from_json_file(config_path)\n    text_config = config.text_config\n    text_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.text_embed_dim = text_config.hidden_size\n\n    # Initialize the BLIP text model\n    self.text_model = BlipTextModel(text_config)\n\n    # Project text embeddings to the desired dimension\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n\n    # Classifier layer for classification task\n    self.classifier = nn.Linear(self.projection_dim, num_classes)\n\n    # Initialize the model weights\n    self.init_weights()\n\n    # Freeze the base model if specified\n    if freeze_base_model:\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n    # Set gradient checkpointing for the encoder\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForTextClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the BlipForTextClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask for the input.</p> required <code>position_ids</code> <code>Tensor</code> <p>Position IDs for the input.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits for classification.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BlipForTextClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Input token IDs.\n        attention_mask (torch.Tensor): Attention mask for the input.\n        position_ids (torch.Tensor): Position IDs for the input.\n\n    Returns:\n        (torch.Tensor):Output logits for classification.\n    \"\"\"\n    # Pass the input through the BLIP text model\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    # Extract text embeddings and project to desired dimension\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    # Apply ReLU activation and pass through the classifier layer\n    return self.classifier(nn.functional.relu(text_embeds))\n</code></pre>"},{"location":"models/blip/#blipforimageclassification","title":"BlipForImageClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BlipForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>projection_dim</code> <code>Optional[int]</code> <p>Dimension of the projection. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>Optional[bool]</code> <p>Whether to freeze the base model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the BlipForImageClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        projection_dim (Optional[int], optional): Dimension of the projection. Defaults to 512.\n        num_classes (Optional[int], optional): Number of classes for classification. Defaults to 1.\n        freeze_base_model (Optional[bool], optional): Whether to freeze the base model. Defaults to True.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    # Load the BLIP model configuration\n    config = BlipConfig.from_json_file(config_path)\n    vision_config = config.vision_config\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.vision_embed_dim = vision_config.hidden_size\n\n    self.vision_model = BlipVisionModel(vision_config)\n\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n\n    self.classifier = nn.Linear(self.projection_dim, num_classes)\n\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the BlipForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits for classification.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the BlipForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): Input pixel values.\n\n    Returns:\n        (torch.Tensor):Output logits for classification.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    image_embeds = vision_outputs[1]\n\n    image_embeds = self.visual_projection(image_embeds)\n\n    image_embeds = F.relu(image_embeds)\n\n    return self.classifier(image_embeds)\n</code></pre>"},{"location":"models/blip/#blipforimagecaption","title":"BlipForImageCaption","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the BlipForImageCaption model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the BlipForImageCaption model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        gradient_checkpointing (Optional[bool], optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BlipConfig.from_json_file(config_path)\n    self.config.vision_config.gradient_checkpointing = gradient_checkpointing\n    self.config.text_config.gradient_checkpointing = gradient_checkpointing\n    self.model = BlipForConditionalGeneration(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForImageCaption.forward","title":"forward","text":"<pre><code>forward(\n    pixel_values: torch.Tensor,\n    input_ids: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the BlipForImageCaption model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <code>input_ids</code> <code>Optional[Tensor]</code> <p>Input token IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Logits for caption generation.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n    input_ids: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the BlipForImageCaption model.\n\n    Args:\n        pixel_values (torch.Tensor): Input pixel values.\n        input_ids (torch.Tensor optional): Input token IDs. Defaults to None.\n        attention_mask (torch.Tensor optional): Attention mask. Defaults to None.\n\n    Returns:\n        (torch.Tensor):Logits for caption generation.\n    \"\"\"\n    outputs = self.model(\n        pixel_values=pixel_values,\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.decoder_logits\n    return logits\n</code></pre>"},{"location":"models/blip/#unitorch.models.blip.BlipForImageCaption.generate","title":"generate","text":"<pre><code>generate(\n    pixel_values: torch.Tensor,\n    input_ids: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 101,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 102,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates captions for the given input images.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input pixel values.</p> required <code>input_ids</code> <code>Optional[Tensor]</code> <p>Input token IDs. Defaults to None.</p> <code>None</code> <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Attention mask. Defaults to None.</p> <code>None</code> <code>num_beams</code> <code>Optional[int]</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>Optional[int]</code> <p>ID of the start token for decoding. Defaults to 30522.</p> <code>101</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>ID of the end token for decoding. Defaults to 2.</p> <code>102</code> <code>num_return_sequences</code> <code>Optional[int]</code> <p>Number of caption sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>Optional[int]</code> <p>Minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>Maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>Repetition penalty value. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>Optional[int]</code> <p>Size of n-grams to avoid repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>Optional[bool]</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>Optional[float]</code> <p>Length penalty value. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>Optional[int]</code> <p>Number of groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>Optional[float]</code> <p>Diversity penalty value. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>Optional[bool]</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>Temperature value for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>Optional[int]</code> <p>Value of k for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>Optional[float]</code> <p>Value of p for top-p sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Generated caption sequences and their scores.</p> Source code in <code>src/unitorch/models/blip/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    pixel_values: torch.Tensor,\n    input_ids: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 101,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 102,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates captions for the given input images.\n\n    Args:\n        pixel_values (torch.Tensor): Input pixel values.\n        input_ids (torch.Tensor optional): Input token IDs. Defaults to None.\n        attention_mask (torch.Tensor optional): Attention mask. Defaults to None.\n        num_beams (Optional[int], optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (Optional[int], optional): ID of the start token for decoding. Defaults to 30522.\n        decoder_end_token_id (int or List[int], optional): ID of the end token for decoding. Defaults to 2.\n        num_return_sequences (Optional[int], optional): Number of caption sequences to return. Defaults to 1.\n        min_gen_seq_length (Optional[int], optional): Minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (Optional[int], optional): Maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (Optional[float], optional): Repetition penalty value. Defaults to 1.0.\n        no_repeat_ngram_size (Optional[int], optional): Size of n-grams to avoid repetition. Defaults to 0.\n        early_stopping (Optional[bool], optional): Whether to stop generation early. Defaults to True.\n        length_penalty (Optional[float], optional): Length penalty value. Defaults to 1.0.\n        num_beam_groups (Optional[int], optional): Number of groups for diverse beam search. Defaults to 1.\n        diversity_penalty (Optional[float], optional): Diversity penalty value. Defaults to 0.0.\n        do_sample (Optional[bool], optional): Whether to use sampling for generation. Defaults to False.\n        temperature (Optional[float], optional): Temperature value for sampling. Defaults to 1.0.\n        top_k (Optional[int], optional): Value of k for top-k sampling. Defaults to 50.\n        top_p (Optional[float], optional): Value of p for top-p sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: Generated caption sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        pixel_values=pixel_values,\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences, sequences_scores=outputs.sequences_scores\n    )\n</code></pre>"},{"location":"models/bloom/","title":"unitorch.models.bloom","text":""},{"location":"models/bloom/#bloomprocessor","title":"BloomProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code>, <code>HfTextGenerationProcessor</code></p> <p>Processor for the Bloom model that combines text classification and text generation functionality.</p> <p>Initializes a new instance of the BloomProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_file</code> <code>str</code> <p>The path to the tokenizer file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for classification. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for generation. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def __init__(\n    self,\n    tokenizer_file: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes a new instance of the BloomProcessor.\n\n    Args:\n        tokenizer_file (str): The path to the tokenizer file.\n        max_seq_length (Optional[int]): The maximum sequence length for classification. Defaults to 128.\n        max_gen_seq_length (Optional[int]): The maximum sequence length for generation. Defaults to 48.\n    \"\"\"\n    tokenizer = BloomTokenizerFast(tokenizer_file=tokenizer_file)\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.cls_token_id = tokenizer.bos_token_id\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n    )\n    HfTextGenerationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomProcessor.classification","title":"classification","text":"<pre><code>classification(\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs\n</code></pre> <p>Preprocesses text for classification.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text to classify.</p> required <code>text_pair</code> <code>Optional[str]</code> <p>The second input text for sequence classification. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The processed input IDs and attention mask tensors.</p> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def classification(\n    self,\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Preprocesses text for classification.\n\n    Args:\n        text (str): The input text to classify.\n        text_pair (Optional[str]): The second input text for sequence classification. Defaults to None.\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The processed input IDs and attention mask tensors.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n\n    tokens = self.tokenizer.tokenize(str(text))\n    if text_pair is None:\n        tokens = tokens[:max_seq_length]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    else:\n        tokens_pair = self.tokenizer.tokenize(str(text_pair))\n        truncate_sequence_pair(tokens, tokens_pair, max_seq_length)\n        tokens = tokens + tokens_pair\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    attention_mask = [0] * len(padding) + [1] * len(input_ids)\n    input_ids = len(padding) * [self.pad_token_id] + input_ids\n\n    assert len(input_ids) == max_seq_length\n    assert len(attention_mask) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomProcessor.generation","title":"generation","text":"<pre><code>generation(\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs\n</code></pre> <p>Preprocesses text for generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for generation.</p> required <code>text_pair</code> <code>str</code> <p>The second input text for generation.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length for classification. Defaults to None.</p> <code>None</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum generation sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The processed input IDs and attention mask tensors.</p> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def generation(\n    self,\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Preprocesses text for generation.\n\n    Args:\n        text (str): The input text for generation.\n        text_pair (str): The second input text for generation.\n        max_seq_length (Optional[int]): The maximum sequence length for classification. Defaults to None.\n        max_gen_seq_length (Optional[int]): The maximum generation sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The processed input IDs and attention mask tensors.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n\n    tokens = self.tokenizer.tokenize(str(text))[-max_seq_length:]\n    tokens_pair = self.tokenizer.tokenize(str(text_pair))[\n        : max_gen_seq_length - 1\n    ] + [self.eos_token]\n    padding_a = [self.pad_token] * (max_seq_length - len(tokens))\n    padding_b = [self.pad_token] * (max_gen_seq_length - len(tokens_pair))\n    attention_mask = (\n        [0] * len(padding_a)\n        + [1] * (len(tokens) + len(tokens_pair))\n        + [0] * len(padding_b)\n    )\n    _tokens = padding_a + tokens + tokens_pair + padding_b\n    input_ids = self.tokenizer.convert_tokens_to_ids(_tokens)\n\n    tokens_label = tokens_pair + [self.pad_token] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    input_ids_label = self.tokenizer.convert_tokens_to_ids(tokens_label)\n    input_ids_label = [0] * (max_seq_length - 1) + input_ids_label\n    attention_mask_label = [1] * len(tokens_pair) + [0] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    attention_mask_label = [0] * (max_seq_length - 1) + attention_mask_label\n\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n        input_ids_label=torch.tensor(input_ids_label, dtype=torch.long),\n        attention_mask_label=torch.tensor(attention_mask_label, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomProcessor.generation_inputs","title":"generation_inputs","text":"<pre><code>generation_inputs(\n    text: str, max_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Preprocesses text as generation inputs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for generation.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The processed input IDs tensor.</p> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def generation_inputs(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Preprocesses text as generation inputs.\n\n    Args:\n        text (str): The input text for generation.\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The processed input IDs tensor.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    tokens = self.tokenizer.tokenize(str(text))[-max_seq_length:]\n    padding = [self.pad_token] * (max_seq_length - len(tokens))\n    tokens = padding + tokens\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    assert len(input_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomProcessor.generation_labels","title":"generation_labels","text":"<pre><code>generation_labels(\n    text: str, max_gen_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Preprocesses text as generation labels.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for generation labels.</p> required <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum generation sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The processed input IDs and attention mask tensors.</p> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def generation_labels(\n    self,\n    text: str,\n    max_gen_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Preprocesses text as generation labels.\n\n    Args:\n        text (str): The input text for generation labels.\n        max_gen_seq_length (Optional[int]): The maximum generation sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The processed input IDs and attention mask tensors.\n    \"\"\"\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n    tokens = self.tokenizer.tokenize(str(text))[: max_gen_seq_length - 1] + [\n        self.eos_token\n    ]\n    padding = [self.pad_token] * (max_gen_seq_length - len(tokens))\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_gen_seq_length - len(input_ids))\n    input_ids += [self.pad_token_id] * len(padding)\n    attention_mask += padding\n\n    assert len(input_ids) == max_gen_seq_length\n    assert len(attention_mask) == max_gen_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomProcessor.prompt","title":"prompt","text":"<pre><code>prompt(\n    text: str, max_seq_length: Optional[int] = None\n) -&gt; GenericOutputs\n</code></pre> <p>Preprocesses text as a prompt for text generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text prompt.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>The maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The processed input IDs tensor.</p> Source code in <code>src/unitorch/models/bloom/processing.py</code> <pre><code>def prompt(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Preprocesses text as a prompt for text generation.\n\n    Args:\n        text (str): The input text prompt.\n        max_seq_length (Optional[int]): The maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: The processed input IDs tensor.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    tokens = self.tokenizer.tokenize(str(text))[-max_seq_length:]\n    padding = [self.pad_token] * (max_seq_length - len(tokens))\n    tokens = padding + tokens\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    assert len(input_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/bloom/#bloomforclassification","title":"BloomForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>A classification model based on the Bloom architecture.</p> <p>Initializes a new instance of the BloomForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the configuration file for the Bloom model.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>The number of output classes for classification. Defaults to 1.</p> <code>1</code> <code>hidden_dropout_prob</code> <code>Optional[float]</code> <p>The dropout probability for the hidden layers. Defaults to 0.1.</p> <code>0.1</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    hidden_dropout_prob: Optional[float] = 0.1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a new instance of the BloomForClassification model.\n\n    Args:\n        config_path (str): The path to the configuration file for the Bloom model.\n        num_classes (Optional[int]): The number of output classes for classification. Defaults to 1.\n        hidden_dropout_prob (Optional[float]): The dropout probability for the hidden layers. Defaults to 0.1.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BloomConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.transformer = BloomModel(self.config)\n    self.dropout = nn.Dropout(hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the BloomForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits for classification.</p> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the BloomForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (Optional[torch.Tensor]): The attention mask tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The output logits for classification.\n    \"\"\"\n    outputs = self.transformer(\n        input_ids,\n        attention_mask=attention_mask,\n    )[0]\n    pooled_output = outputs[:, -1]\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/bloom/#bloomforpretrain","title":"BloomForPretrain","text":"<p>             Bases: <code>GenericModel</code></p> <p>A pretraining model based on the Bloom architecture.</p> <p>Initializes a new instance of the BloomForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the configuration file for the Bloom model.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a new instance of the BloomForPretrain model.\n\n    Args:\n        config_path (str): The path to the configuration file for the Bloom model.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BloomConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.transformer = BloomModel(self.config)\n    self.lm_head = nn.Linear(\n        self.config.hidden_size, self.config.vocab_size, bias=False\n    )\n    self.init_weights()\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    input_ids_label: torch.Tensor,\n    attention_mask_label: torch.Tensor,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the BloomForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor.</p> required <code>input_ids_label</code> <code>Tensor</code> <p>The input token IDs for the labeled data.</p> required <code>attention_mask_label</code> <code>Tensor</code> <p>The attention mask tensor for the labeled data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed loss value.</p> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    input_ids_label: torch.Tensor,\n    attention_mask_label: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the BloomForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask tensor.\n        input_ids_label (torch.Tensor): The input token IDs for the labeled data.\n        attention_mask_label (torch.Tensor): The attention mask tensor for the labeled data.\n\n    Returns:\n        (torch.Tensor):The computed loss value.\n    \"\"\"\n    outputs = self.transformer(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n    )\n    predict_logits = self.lm_head(outputs[0])\n    batch_size, seq_len, num_classes = predict_logits.size()\n    logits = predict_logits.contiguous().view(batch_size * seq_len, num_classes)\n    targets = input_ids_label.contiguous().view(-1).long()\n    masks = attention_mask_label.contiguous().view(-1)\n    loss = nn.CrossEntropyLoss(reduction=\"none\")(logits, targets)\n    loss = loss * masks.float()\n    loss = loss.contiguous().view(batch_size, seq_len).sum(1) / torch.max(\n        masks.contiguous().view(batch_size, seq_len).float().sum(1),\n        torch.ones(batch_size).to(masks.device),\n    )\n    loss = torch.mean(loss)\n    return loss\n</code></pre>"},{"location":"models/bloom/#bloomforgeneration","title":"BloomForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>A generation model based on the Bloom architecture.</p> <p>Initializes a new instance of the BloomForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the configuration file for the Bloom model.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a new instance of the BloomForGeneration model.\n\n    Args:\n        config_path (str): The path to the configuration file for the Bloom model.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BloomConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = BloomForCausalLM(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Forward pass of the BloomForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits.</p> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the BloomForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (Optional[torch.Tensor]): The attention mask tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The output logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/bloom/#unitorch.models.bloom.BloomForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n) -&gt; GenericOutputs\n</code></pre> <p>Generate sequences using the BloomForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>Optional[int]</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>Optional[int]</code> <p>The ID of the start token for decoding. Defaults to 1.</p> <code>1</code> <code>decoder_end_token_id</code> <code>Optional[int]</code> <p>The ID of the end token for decoding. Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>Optional[int]</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>Optional[int]</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>Optional[float]</code> <p>The penalty for repeated n-grams. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>Optional[int]</code> <p>The size of n-grams to prevent repetition. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>Optional[bool]</code> <p>Whether to stop generation early based on specified conditions. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>Optional[float]</code> <p>The penalty for longer sequences. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>Optional[int]</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>Optional[float]</code> <p>The penalty for diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>Optional[bool]</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Optional[float]</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>Optional[int]</code> <p>The number of top-k tokens to consider for sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>Optional[float]</code> <p>The cumulative probability for top-p sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/bloom/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n) -&gt; GenericOutputs:\n    \"\"\"\n    Generate sequences using the BloomForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        num_beams (Optional[int]): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (Optional[int]): The ID of the start token for decoding. Defaults to 1.\n        decoder_end_token_id (Optional[int]): The ID of the end token for decoding. Defaults to 2.\n        num_return_sequences (Optional[int]): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (Optional[int]): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (Optional[int]): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (Optional[float]): The penalty for repeated n-grams. Defaults to 1.0.\n        no_repeat_ngram_size (Optional[int]): The size of n-grams to prevent repetition. Defaults to 0.\n        early_stopping (Optional[bool]): Whether to stop generation early based on specified conditions. Defaults to True.\n        length_penalty (Optional[float]): The penalty for longer sequences. Defaults to 1.0.\n        num_beam_groups (Optional[int]): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (Optional[float]): The penalty for diverse beam search. Defaults to 0.0.\n        do_sample (Optional[bool]): Whether to use sampling for generation. Defaults to False.\n        temperature (Optional[float]): The temperature for sampling. Defaults to 1.0.\n        top_k (Optional[int]): The number of top-k tokens to consider for sampling. Defaults to 50.\n        top_p (Optional[float]): The cumulative probability for top-p sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    input_seq_length = input_ids.size(1)\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length + input_seq_length,\n        min_length=min_gen_seq_length + input_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1) - input_seq_length].copy_(\n        sequences[:, :, input_seq_length : sequences.size(-1)]\n    )\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences.long(),\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/clip/","title":"unitorch.models.clip","text":""},{"location":"models/clip/#clipprocessor","title":"ClipProcessor","text":"<p>             Bases: <code>HfImageClassificationProcessor</code>, <code>HfTextClassificationProcessor</code></p> <p>Initializes the ClipProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the merge file.</p> required <code>vision_config_path</code> <code>str</code> <p>The path to the vision configuration file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for text inputs. Defaults to 128.</p> <code>128</code> <code>position_start_id</code> <code>int</code> <p>The starting position ID for positional embeddings. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/clip/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vision_config_path: str,\n    max_seq_length: Optional[int] = 128,\n    position_start_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes the ClipProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the merge file.\n        vision_config_path (str): The path to the vision configuration file.\n        max_seq_length (int, optional): The maximum sequence length for text inputs. Defaults to 128.\n        position_start_id (int, optional): The starting position ID for positional embeddings. Defaults to 0.\n    \"\"\"\n    vision_processor = CLIPImageProcessor.from_json_file(vision_config_path)\n    HfImageClassificationProcessor.__init__(\n        self,\n        vision_processor=vision_processor,\n    )\n\n    tokenizer = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=0,\n        target_type_id=0,\n        position_start_id=position_start_id,\n    )\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipProcessor.classification","title":"classification","text":"<pre><code>classification(\n    text: str,\n    image: Union[Image.Image, str],\n    max_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Performs classification using text and image inputs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for text inputs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>An object containing the processed inputs.</p> Source code in <code>src/unitorch/models/clip/processing.py</code> <pre><code>def classification(\n    self,\n    text: str,\n    image: Union[Image.Image, str],\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Performs classification using text and image inputs.\n\n    Args:\n        text (str): The input text.\n        image (PIL.Image.Image): The input image.\n        max_seq_length (int, optional): The maximum sequence length for text inputs. Defaults to None.\n\n    Returns:\n        GenericOutputs: An object containing the processed inputs.\n    \"\"\"\n    text_outputs = self.text_classification(\n        text=text,\n        max_seq_length=max_seq_length,\n    )\n    pixel_outputs = self.image_classification(\n        image=image,\n    )\n\n    return GenericOutputs(\n        input_ids=text_outputs.input_ids,\n        attention_mask=text_outputs.attention_mask,\n        position_ids=text_outputs.position_ids,\n        pixel_values=pixel_outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipProcessor.image_classification","title":"image_classification","text":"<pre><code>image_classification(image: Union[Image.Image, str])\n</code></pre> <p>Performs image classification.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The input image.</p> required <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>An object containing the processed inputs.</p> Source code in <code>src/unitorch/models/clip/processing.py</code> <pre><code>def image_classification(\n    self,\n    image: Union[Image.Image, str],\n):\n    \"\"\"\n    Performs image classification.\n\n    Args:\n        image (PIL.Image.Image): The input image.\n\n    Returns:\n        GenericOutputs: An object containing the processed inputs.\n    \"\"\"\n    outputs = HfImageClassificationProcessor.classification(\n        self,\n        image=image,\n    )\n\n    return GenericOutputs(\n        pixel_values=outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipProcessor.text_classification","title":"text_classification","text":"<pre><code>text_classification(\n    text: str, max_seq_length: Optional[int] = None\n)\n</code></pre> <p>Performs text classification.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length for text inputs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>An object containing the processed inputs.</p> Source code in <code>src/unitorch/models/clip/processing.py</code> <pre><code>def text_classification(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Performs text classification.\n\n    Args:\n        text (str): The input text.\n        max_seq_length (int, optional): The maximum sequence length for text inputs. Defaults to None.\n\n    Returns:\n        GenericOutputs: An object containing the processed inputs.\n    \"\"\"\n    outputs = HfTextClassificationProcessor.classification(\n        self,\n        text=text,\n        max_seq_length=max_seq_length,\n    )\n    return GenericOutputs(\n        input_ids=outputs.input_ids,\n        attention_mask=outputs.attention_mask,\n        position_ids=outputs.position_ids,\n    )\n</code></pre>"},{"location":"models/clip/#clipforpretrain","title":"ClipForPretrain","text":"<p>             Bases: <code>GenericModel</code></p> <p>Clip model for pretraining.</p> <p>Initializes the ClipForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>Dimension of the projected embeddings. Defaults to 512.</p> <code>512</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> <code>use_all_gather</code> <code>bool</code> <p>Whether to use all-gather operation. Defaults to True.</p> <code>True</code> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n    use_all_gather: Optional[bool] = True,\n):\n    \"\"\"\n    Initializes the ClipForPretrain model.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        projection_dim (int, optional): Dimension of the projected embeddings. Defaults to 512.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n        use_all_gather (bool, optional): Whether to use all-gather operation. Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    config = CLIPConfig.from_json_file(config_path)\n    text_config = config.text_config\n    vision_config = config.vision_config\n    text_config.gradient_checkpointing = gradient_checkpointing\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.use_all_gather = use_all_gather\n\n    self.text_embed_dim = text_config.hidden_size\n    self.vision_embed_dim = vision_config.hidden_size\n\n    self.text_model = CLIPTextTransformer(text_config)\n    self.vision_model = CLIPVisionTransformer(vision_config)\n\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.logit_scale = nn.Parameter(torch.ones([]) * config.logit_scale_init_value)\n\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the Clip model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input text token IDs. Defaults to None.</p> required <code>pixel_values</code> <code>Tensor</code> <p>Input image pixel values. Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask for the input. Defaults to None.</p> required <code>position_ids</code> <code>Tensor</code> <p>Position IDs for the input tokens. Defaults to None.</p> required <code>output_attentions</code> <code>bool</code> <p>Whether to output attentions. Defaults to None.</p> required <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states. Defaults to None.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Logits per text.</p> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the Clip model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input text token IDs. Defaults to None.\n        pixel_values (torch.Tensor, optional): Input image pixel values. Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask for the input. Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs for the input tokens. Defaults to None.\n        output_attentions (bool, optional): Whether to output attentions. Defaults to None.\n        output_hidden_states (bool, optional): Whether to output hidden states. Defaults to None.\n\n    Returns:\n        (torch.Tensor):Logits per text.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    image_embeds = vision_outputs[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    # normalized features\n    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n\n    logit_scale = self.logit_scale.exp()\n    if self.use_all_gather and dist.is_initialized():\n        text_embeds = self._all_gather(text_embeds)\n        image_embeds = self._all_gather(image_embeds)\n    logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n    return _clip_loss(logits_per_text)\n</code></pre>"},{"location":"models/clip/#clipforclassification","title":"ClipForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Clip model for classification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Config file path to Clip model.</p> required <code>projection_dim</code> <code>int</code> <p>Dimension for image/text output embedding.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>Number of classes for classification.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model.</p> <code>True</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to enable gradient_checkpointing.</p> <code>False</code> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Clip model for classification.\n\n    Args:\n        config_path (str): Config file path to Clip model.\n        projection_dim (int): Dimension for image/text output embedding.\n        num_classes (int): Number of classes for classification.\n        freeze_base_model (bool): Whether to freeze the base model.\n        gradient_checkpointing (Optional[bool]): Whether to enable gradient_checkpointing.\n    \"\"\"\n    super().__init__()\n    config = CLIPConfig.from_json_file(config_path)\n    text_config = config.text_config\n    vision_config = config.vision_config\n    text_config.gradient_checkpointing = gradient_checkpointing\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n\n    self.text_embed_dim = text_config.hidden_size\n    self.vision_embed_dim = vision_config.hidden_size\n\n    self.text_model = CLIPTextTransformer(text_config)\n    self.vision_model = CLIPVisionTransformer(vision_config)\n\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n\n    self.classifier = nn.Linear(self.projection_dim * 2, num_classes)\n\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the Clip model for classification.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>tensor</code> <p>Tokens of text.</p> required <code>pixel_values</code> <code>tensor</code> <p>Pixels of image.</p> required <code>attention_mask</code> <code>tensor</code> <p>Attention mask of tokens.</p> required <code>position_ids</code> <code>tensor</code> <p>Position IDs.</p> required <code>output_attentions</code> <code>bool</code> <p>Whether to output attentions.</p> required <code>output_hidden_states</code> <code>bool</code> <p>Whether to output hidden states.</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>Output tensor from the classifier.</p> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    pixel_values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the Clip model for classification.\n\n    Args:\n        input_ids (tensor): Tokens of text.\n        pixel_values (tensor): Pixels of image.\n        attention_mask (tensor): Attention mask of tokens.\n        position_ids (tensor): Position IDs.\n        output_attentions (bool): Whether to output attentions.\n        output_hidden_states (bool): Whether to output hidden states.\n\n    Returns:\n        tensor: Output tensor from the classifier.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n\n    image_embeds = vision_outputs[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    return self.classifier(F.relu(torch.cat([image_embeds, text_embeds], axis=1)))\n</code></pre>"},{"location":"models/clip/#clipfortextclassification","title":"ClipForTextClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the Clip model for text classification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the CLIP configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection layer. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the Clip model for text classification.\n\n    Args:\n        config_path (str): The path to the CLIP configuration file.\n        projection_dim (int, optional): The dimension of the projection layer. Defaults to 512.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    config = CLIPConfig.from_json_file(config_path)\n    text_config = config.text_config\n    text_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.text_embed_dim = text_config.hidden_size\n\n    self.text_model = CLIPTextTransformer(text_config)\n\n    self.text_projection = nn.Linear(\n        self.text_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n\n    self.classifier = nn.Linear(self.projection_dim, num_classes)\n\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.text_model.parameters():\n            p.requires_grad = False\n\n    self.text_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipForTextClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the Clip model for text classification.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits.</p> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    position_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the Clip model for text classification.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        position_ids (torch.Tensor): The position IDs.\n\n    Returns:\n        (torch.Tensor):The output logits.\n    \"\"\"\n    text_outputs = self.text_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    text_embeds = text_outputs[1]\n    text_embeds = self.text_projection(text_embeds)\n\n    # normalized features\n    # text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n\n    return self.classifier(F.relu(text_embeds))\n</code></pre>"},{"location":"models/clip/#clipforimageclassification","title":"ClipForImageClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the Clip model for image classification.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the CLIP configuration file.</p> required <code>projection_dim</code> <code>int</code> <p>The dimension of the projection layer. Defaults to 512.</p> <code>512</code> <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>freeze_base_model</code> <code>bool</code> <p>Whether to freeze the base model parameters. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    projection_dim: Optional[int] = 512,\n    num_classes: Optional[int] = 1,\n    freeze_base_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the Clip model for image classification.\n\n    Args:\n        config_path (str): The path to the CLIP configuration file.\n        projection_dim (int, optional): The dimension of the projection layer. Defaults to 512.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        freeze_base_model (bool, optional): Whether to freeze the base model parameters. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    config = CLIPConfig.from_json_file(config_path)\n    vision_config = config.vision_config\n    vision_config.gradient_checkpointing = gradient_checkpointing\n\n    self.projection_dim = projection_dim\n    self.vision_embed_dim = vision_config.hidden_size\n    self.vision_model = CLIPVisionTransformer(vision_config)\n    self.visual_projection = nn.Linear(\n        self.vision_embed_dim,\n        self.projection_dim,\n        bias=False,\n    )\n    self.classifier = nn.Linear(self.projection_dim, num_classes)\n    self.init_weights()\n\n    if freeze_base_model:\n        for p in self.vision_model.parameters():\n            p.requires_grad = False\n\n    self.vision_model.encoder.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"models/clip/#unitorch.models.clip.ClipForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the Clip model for image classification.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input pixel values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits.</p> Source code in <code>src/unitorch/models/clip/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the Clip model for image classification.\n\n    Args:\n        pixel_values (torch.Tensor): The input pixel values.\n\n    Returns:\n        (torch.Tensor):The output logits.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n\n    image_embeds = vision_outputs[1]\n    image_embeds = self.visual_projection(image_embeds)\n\n    # normalized features\n    # image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n\n    return self.classifier(F.relu(image_embeds))\n</code></pre>"},{"location":"models/deberta/","title":"unitorch.models.deberta","text":""},{"location":"models/deberta/#debertaprocessor","title":"DebertaProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> <p>Processor for DeBERTa-based text classification models.</p> <p>Initializes the DebertaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>Path to the merge file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>Optional[int]</code> <p>Source type ID for token types. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>Optional[int]</code> <p>Target type ID for token types. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/deberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes the DebertaProcessor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        merge_path (str): Path to the merge file.\n        max_seq_length (Optional[int]): Maximum sequence length. Defaults to 128.\n        source_type_id (Optional[int]): Source type ID for token types. Defaults to 0.\n        target_type_id (Optional[int]): Target type ID for token types. Defaults to 0.\n    \"\"\"\n    tokenizer = get_deberta_tokenizer(\n        vocab_path,\n        merge_path,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n        position_start_id=tokenizer.pad_token_id + 1,\n    )\n</code></pre>"},{"location":"models/deberta/#debertaforclassification","title":"DebertaForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the DebertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the Deberta model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing for memory optimization. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/deberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the DebertaForClassification model.\n\n    Args:\n        config_path (str): The path to the Deberta model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing for memory optimization. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = DebertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.deberta = DebertaModel(self.config)\n    self.pooler = ContextPooler(self.config)\n    self.dropout = StableDropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/deberta/#unitorch.models.deberta.DebertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs forward pass of the DebertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing token ids.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>The token type ids tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position ids tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits tensor.</p> Source code in <code>src/unitorch/models/deberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs forward pass of the DebertaForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing token ids.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        token_type_ids (torch.Tensor, optional): The token type ids tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position ids tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The output logits tensor.\n    \"\"\"\n    outputs = self.deberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = self.pooler(outputs[0])\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/deberta/#debertaformasklm","title":"DebertaForMaskLM","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the DebertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the Deberta model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing for memory optimization. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/deberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the DebertaForMaskLM model.\n\n    Args:\n        config_path (str): The path to the Deberta model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing for memory optimization. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = DebertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.deberta = DebertaModel(self.config)\n    self.cls = DebertaOnlyMLMHead(self.config)\n    self.cls.predictions.decoder.weight = (\n        self.deberta.embeddings.word_embeddings.weight\n    )\n    self.init_weights()\n</code></pre>"},{"location":"models/deberta/#unitorch.models.deberta.DebertaForMaskLM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs forward pass of the DebertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing token ids.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>The token type ids tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position ids tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits tensor.</p> Source code in <code>src/unitorch/models/deberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs forward pass of the DebertaForMaskLM model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing token ids.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        token_type_ids (torch.Tensor, optional): The token type ids tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position ids tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The output logits tensor.\n    \"\"\"\n    outputs = self.deberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    sequence_output = outputs[0]\n    logits = self.cls(sequence_output)\n    return logits\n</code></pre>"},{"location":"models/deberta/#debertav2processor","title":"DebertaV2Processor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> <p>Processor for DeBERTa-v2 based text classification models.</p> <p>Initializes the DebertaV2Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>Optional[int]</code> <p>Source type ID for token types. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>Optional[int]</code> <p>Target type ID for token types. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/models/deberta/processing_v2.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 1,\n):\n    \"\"\"\n    Initializes the DebertaV2Processor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        max_seq_length (Optional[int]): Maximum sequence length. Defaults to 128.\n        source_type_id (Optional[int]): Source type ID for token types. Defaults to 0.\n        target_type_id (Optional[int]): Target type ID for token types. Defaults to 1.\n    \"\"\"\n    tokenizer = get_deberta_v2_tokenizer(\n        vocab_path,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n        position_start_id=tokenizer.pad_token_id + 1,\n    )\n</code></pre>"},{"location":"models/deberta/#debertav2forclassification","title":"DebertaV2ForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes the DebertaV2ForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the DebertaV2 model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing for memory optimization. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/deberta/modeling_v2.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the DebertaV2ForClassification model.\n\n    Args:\n        config_path (str): The path to the DebertaV2 model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing for memory optimization. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = DebertaV2Config.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.deberta = DebertaV2Model(self.config)\n    self.pooler = ContextPooler(self.config)\n    self.dropout = StableDropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/deberta/#unitorch.models.deberta.DebertaV2ForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs forward pass of the DebertaV2ForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input tensor containing token ids.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask tensor. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>The token type ids tensor. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>The position ids tensor. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The output logits tensor.</p> Source code in <code>src/unitorch/models/deberta/modeling_v2.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs forward pass of the DebertaV2ForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input tensor containing token ids.\n        attention_mask (torch.Tensor, optional): The attention mask tensor. Defaults to None.\n        token_type_ids (torch.Tensor, optional): The token type ids tensor. Defaults to None.\n        position_ids (torch.Tensor, optional): The position ids tensor. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The output logits tensor.\n    \"\"\"\n    outputs = self.deberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = self.pooler(outputs[0])\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/diffusers/","title":"unitorch.models.diffusers","text":""},{"location":"models/diffusers/#stableprocessor","title":"StableProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> Source code in <code>src/unitorch/models/diffusers/processing_stable.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n    random_flip: Optional[bool] = False,\n):\n    tokenizer = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = pad_token\n\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    self.vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size) if center_crop else RandomCrop(image_size),\n            RandomHorizontalFlip() if random_flip else Lambda(lambda x: x),\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n\n    if vae_config_path is not None:\n        vae_config_dict = json.load(open(vae_config_path))\n        vae_scale_factor = 2 ** (\n            len(vae_config_dict.get(\"block_out_channels\", [])) - 1\n        )\n        self.vae_image_processor = VaeImageProcessor(\n            vae_scale_factor=vae_scale_factor\n        )\n    else:\n        self.vae_image_processor = None\n</code></pre>"},{"location":"models/diffusers/#stablefortext2imagegeneration","title":"StableForText2ImageGeneration","text":"<p>             Bases: <code>GenericStableModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stableforimage2imagegeneration","title":"StableForImage2ImageGeneration","text":"<p>             Bases: <code>GenericStableModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionImg2ImgPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stableforimageinpainting","title":"StableForImageInpainting","text":"<p>             Bases: <code>GenericStableModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionInpaintPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stableforimageresolution","title":"StableForImageResolution","text":"<p>             Bases: <code>GenericStableModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionUpscalePipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        low_res_scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stablexlprocessor","title":"StableXLProcessor","text":"Source code in <code>src/unitorch/models/diffusers/processing_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    vae_config_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n):\n    tokenizer1 = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer1.cls_token = tokenizer1.bos_token\n    tokenizer1.sep_token = tokenizer1.eos_token\n    tokenizer1.pad_token = pad_token\n\n    self.text_processor1 = HfTextClassificationProcessor(\n        tokenizer=tokenizer1,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    tokenizer2 = CLIPTokenizer(\n        vocab_file=vocab2_path,\n        merges_file=merge2_path,\n    )\n\n    tokenizer2.cls_token = tokenizer2.bos_token\n    tokenizer2.sep_token = tokenizer2.eos_token\n    tokenizer2.pad_token = pad_token2\n\n    self.text_processor2 = HfTextClassificationProcessor(\n        tokenizer=tokenizer2,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    self.image_size = image_size\n    self.center_crop = center_crop\n    self.vision_resize = Resize(image_size)\n    self.vision_crop = (\n        CenterCrop(image_size) if center_crop else RandomCrop(image_size)\n    )\n    self.vision_flip = RandomHorizontalFlip(p=1.0)\n    self.vision_processor = Compose(\n        [\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n\n    if vae_config_path is not None:\n        vae_config_dict = json.load(open(vae_config_path))\n        vae_scale_factor = 2 ** (\n            len(vae_config_dict.get(\"block_out_channels\", [])) - 1\n        )\n        self.vae_image_processor = VaeImageProcessor(\n            vae_scale_factor=vae_scale_factor\n        )\n</code></pre>"},{"location":"models/diffusers/#stablexlfortext2imagegeneration","title":"StableXLForText2ImageGeneration","text":"<p>             Bases: <code>GenericStableXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionXLPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stablexlforimage2imagegeneration","title":"StableXLForImage2ImageGeneration","text":"<p>             Bases: <code>GenericStableXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionXLImg2ImgPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stablexlforimageinpainting","title":"StableXLForImageInpainting","text":"<p>             Bases: <code>GenericStableXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        snr_gamma=snr_gamma,\n        lora_r=lora_r,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionXLInpaintPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stablexlrefinerprocessor","title":"StableXLRefinerProcessor","text":"Source code in <code>src/unitorch/models/diffusers/processing_stable_xl_refiner.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    refiner_vocab_path: Optional[str] = None,\n    refiner_merge_path: Optional[str] = None,\n    refiner_vocab2_path: Optional[str] = None,\n    refiner_merge2_path: Optional[str] = None,\n    vae_config_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n    refiner_pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    refiner_pad_token2: Optional[str] = \"!\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n):\n    tokenizer1 = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer1.cls_token = tokenizer1.bos_token\n    tokenizer1.sep_token = tokenizer1.eos_token\n    tokenizer1.pad_token = pad_token\n\n    self.text_processor1 = HfTextClassificationProcessor(\n        tokenizer=tokenizer1,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    tokenizer2 = CLIPTokenizer(\n        vocab_file=vocab2_path,\n        merges_file=merge2_path,\n    )\n\n    tokenizer2.cls_token = tokenizer2.bos_token\n    tokenizer2.sep_token = tokenizer2.eos_token\n    tokenizer2.pad_token = pad_token2\n\n    tokenizer2.pad_token = \"!\"\n    self.text_processor2 = HfTextClassificationProcessor(\n        tokenizer=tokenizer2,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    if refiner_vocab_path is not None and refiner_merge_path is not None:\n        refiner_tokenizer1 = CLIPTokenizer(\n            vocab_file=refiner_vocab_path,\n            merges_file=refiner_merge_path,\n        )\n\n        refiner_tokenizer1.cls_token = refiner_tokenizer1.bos_token\n        refiner_tokenizer1.sep_token = refiner_tokenizer1.eos_token\n        refiner_tokenizer1.pad_token = refiner_pad_token\n\n        self.refiner_text_processor1 = HfTextClassificationProcessor(\n            tokenizer=refiner_tokenizer1,\n            max_seq_length=max_seq_length,\n            position_start_id=position_start_id,\n        )\n    else:\n        self.refiner_text_processor1 = None\n\n    if refiner_vocab2_path is not None and refiner_merge2_path is not None:\n        refiner_tokenizer2 = CLIPTokenizer(\n            vocab_file=refiner_vocab2_path,\n            merges_file=refiner_merge2_path,\n        )\n\n        refiner_tokenizer2.cls_token = refiner_tokenizer2.bos_token\n        refiner_tokenizer2.sep_token = refiner_tokenizer2.eos_token\n        refiner_tokenizer2.pad_token = refiner_pad_token2\n\n        refiner_tokenizer2.pad_token = \"!\"\n        self.refiner_text_processor2 = HfTextClassificationProcessor(\n            tokenizer=refiner_tokenizer2,\n            max_seq_length=max_seq_length,\n            position_start_id=position_start_id,\n        )\n    else:\n        self.refiner_text_processor2 = None\n\n    self.image_size = image_size\n    self.center_crop = center_crop\n    self.vision_resize = Resize(image_size)\n    self.vision_crop = (\n        CenterCrop(image_size) if center_crop else RandomCrop(image_size)\n    )\n    self.vision_flip = RandomHorizontalFlip(p=1.0)\n    self.vision_processor = Compose(\n        [\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n\n    if vae_config_path is not None:\n        vae_config_dict = json.load(open(vae_config_path))\n        vae_scale_factor = 2 ** (\n            len(vae_config_dict.get(\"block_out_channels\", [])) - 1\n        )\n        self.vae_image_processor = VaeImageProcessor(\n            vae_scale_factor=vae_scale_factor\n        )\n</code></pre>"},{"location":"models/diffusers/#stablexlrefinerfortext2imagegeneration","title":"StableXLRefinerForText2ImageGeneration","text":"<p>             Bases: <code>GenericStableXLRefinerModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_stable_xl_refiner.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    refiner_config_path: Optional[str] = None,\n    refiner_text_config_path: Optional[str] = None,\n    refiner_text2_config_path: Optional[str] = None,\n    refiner_vae_config_path: Optional[str] = None,\n    refiner_scheduler_config_path: Optional[str] = None,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        scheduler_config_path=scheduler_config_path,\n        refiner_config_path=refiner_config_path,\n        refiner_text_config_path=refiner_text_config_path,\n        refiner_text2_config_path=refiner_text2_config_path,\n        refiner_vae_config_path=refiner_vae_config_path,\n        refiner_scheduler_config_path=refiner_scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        seed=seed,\n    )\n\n    self.pipeline = StableDiffusionXLPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n\n    self.refiner_pipeline = StableDiffusionXLImg2ImgPipeline(\n        vae=self.refiner_vae,\n        text_encoder=self.refiner_text,\n        text_encoder_2=self.refiner_text2,\n        unet=self.refiner_unet,\n        scheduler=self.refiner_scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n        requires_aesthetics_score=True,\n    )\n    self.refiner_pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#stablexlrefinerforimage2imagegeneration","title":"StableXLRefinerForImage2ImageGeneration","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"models/diffusers/#stablexlrefinerforimageinpainting","title":"StableXLRefinerForImageInpainting","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> Source code in <code>src/unitorch/models/__init__.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    pass\n</code></pre>"},{"location":"models/diffusers/#controlnetprocessor","title":"ControlNetProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> Source code in <code>src/unitorch/models/diffusers/processing_controlnet.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    image_size: Optional[int] = 512,\n):\n    tokenizer = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = pad_token\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n    self.vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n    self.condition_vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n        ]\n    )\n\n    vae_config_dict = json.load(open(vae_config_path))\n    vae_scale_factor = 2 ** (len(vae_config_dict.get(\"block_out_channels\", [])) - 1)\n    self.vae_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor, do_convert_rgb=True\n    )\n    self.vae_mask_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor\n    )\n    self.vae_condition_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor,\n        do_convert_rgb=True,\n        do_normalize=False,\n    )\n</code></pre>"},{"location":"models/diffusers/#controlnetfortext2imagegeneration","title":"ControlNetForText2ImageGeneration","text":"<p>             Bases: <code>GenericControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#controlnetforimage2imagegeneration","title":"ControlNetForImage2ImageGeneration","text":"<p>             Bases: <code>GenericControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetImg2ImgPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#controlnetforimageinpainting","title":"ControlNetForImageInpainting","text":"<p>             Bases: <code>GenericControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetInpaintPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#controlnetxlprocessor","title":"ControlNetXLProcessor","text":"Source code in <code>src/unitorch/models/diffusers/processing_controlnet_xl.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n    image_size: Optional[int] = 512,\n):\n    tokenizer1 = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer1.cls_token = tokenizer1.bos_token\n    tokenizer1.sep_token = tokenizer1.eos_token\n    tokenizer1.pad_token = pad_token\n\n    self.text_processor1 = HfTextClassificationProcessor(\n        tokenizer=tokenizer1,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    tokenizer2 = CLIPTokenizer(\n        vocab_file=vocab2_path,\n        merges_file=merge2_path,\n    )\n\n    tokenizer2.cls_token = tokenizer2.bos_token\n    tokenizer2.sep_token = tokenizer2.eos_token\n    tokenizer2.pad_token = pad_token2\n\n    self.text_processor2 = HfTextClassificationProcessor(\n        tokenizer=tokenizer2,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    self.vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n    self.condition_vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n        ]\n    )\n\n    vae_config_dict = json.load(open(vae_config_path))\n    vae_scale_factor = 2 ** (len(vae_config_dict.get(\"block_out_channels\", [])) - 1)\n    self.vae_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor, do_convert_rgb=True\n    )\n    self.vae_mask_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor\n    )\n    self.vae_condition_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor,\n        do_convert_rgb=True,\n        do_normalize=False,\n    )\n</code></pre>"},{"location":"models/diffusers/#controlnetxlfortext2imagegeneration","title":"ControlNetXLForText2ImageGeneration","text":"<p>             Bases: <code>GenericControlNetXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionXLControlNetPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#controlnetxlforimage2imagegeneration","title":"ControlNetXLForImage2ImageGeneration","text":"<p>             Bases: <code>GenericControlNetXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionXLControlNetImg2ImgPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#controlnetxlforimageinpainting","title":"ControlNetXLForImageInpainting","text":"<p>             Bases: <code>GenericControlNetXLModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_controlnet_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    text2_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        text2_config_path=text2_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionXLControlNetInpaintPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        text_encoder_2=self.text2,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        tokenizer_2=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#dreamboothprocessor","title":"DreamboothProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> Source code in <code>src/unitorch/models/diffusers/processing_dreambooth.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n):\n    tokenizer = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = pad_token\n\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    self.vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size) if center_crop else RandomCrop(image_size),\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n\n    if vae_config_path is not None:\n        vae_config_dict = json.load(open(vae_config_path))\n        vae_scale_factor = 2 ** (\n            len(vae_config_dict.get(\"block_out_channels\", [])) - 1\n        )\n        self.vae_image_processor = VaeImageProcessor(\n            vae_scale_factor=vae_scale_factor\n        )\n    else:\n        self.vae_image_processor = None\n</code></pre>"},{"location":"models/diffusers/#dreamboothfortext2imagegeneration","title":"DreamboothForText2ImageGeneration","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_dreambooth.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    prior_loss_weight: Optional[float] = 1.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__()\n    self.seed = seed\n    self.num_train_timesteps = num_train_timesteps\n    self.num_infer_timesteps = num_infer_timesteps\n    self.image_size = image_size\n    self.snr_gamma = snr_gamma\n    self.prior_loss_weight = prior_loss_weight\n\n    config_dict = json.load(open(config_path))\n    if image_size is not None:\n        config_dict.update({\"sample_size\": image_size})\n    if in_channels is not None:\n        config_dict.update({\"in_channels\": in_channels})\n    if out_channels is not None:\n        config_dict.update({\"out_channels\": out_channels})\n    self.unet = UNet2DConditionModel.from_config(config_dict)\n\n    text_config = CLIPTextConfig.from_json_file(text_config_path)\n    self.text = CLIPTextModel(text_config)\n\n    vae_config_dict = json.load(open(vae_config_path))\n    self.vae = AutoencoderKL.from_config(vae_config_dict)\n\n    scheduler_config_dict = json.load(open(scheduler_config_path))\n    scheduler_class_name = scheduler_config_dict.get(\"_class_name\", \"DDPMScheduler\")\n    assert hasattr(schedulers, scheduler_class_name)\n    scheduler_class = getattr(schedulers, scheduler_class_name)\n    assert issubclass(scheduler_class, SchedulerMixin)\n    scheduler_config_dict[\"num_train_timesteps\"] = num_train_timesteps\n    self.scheduler = scheduler_class.from_config(scheduler_config_dict)\n\n    self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n    if freeze_vae_encoder:\n        for param in self.vae.parameters():\n            param.requires_grad = False\n\n    if freeze_text_encoder:\n        for param in self.text.parameters():\n            param.requires_grad = False\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(self.quant_config, ignore_modules=[\"lm_head\", \"unet\", \"vae\"])\n\n    if lora_r is not None:\n        for param in self.unet.parameters():\n            param.requires_grad = False\n        self.enable_lora(lora_r=lora_r)\n</code></pre>"},{"location":"models/diffusers/#dreamboothxlprocessor","title":"DreamboothXLProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> Source code in <code>src/unitorch/models/diffusers/processing_dreambooth_xl.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vocab2_path: str,\n    merge2_path: str,\n    vae_config_path: Optional[str] = None,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    pad_token2: Optional[str] = \"!\",\n    image_size: Optional[int] = 512,\n    center_crop: Optional[bool] = False,\n):\n    tokenizer1 = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n\n    tokenizer1.cls_token = tokenizer1.bos_token\n    tokenizer1.sep_token = tokenizer1.eos_token\n    tokenizer1.pad_token = pad_token\n\n    self.text_processor1 = HfTextClassificationProcessor(\n        tokenizer=tokenizer1,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    tokenizer2 = CLIPTokenizer(\n        vocab_file=vocab2_path,\n        merges_file=merge2_path,\n    )\n\n    tokenizer2.cls_token = tokenizer2.bos_token\n    tokenizer2.sep_token = tokenizer2.eos_token\n    tokenizer2.pad_token = pad_token2\n\n    self.text_processor2 = HfTextClassificationProcessor(\n        tokenizer=tokenizer2,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n\n    self.image_size = image_size\n    self.center_crop = center_crop\n    self.vision_resize = Resize(image_size)\n    self.vision_crop = (\n        CenterCrop(image_size) if center_crop else RandomCrop(image_size)\n    )\n    self.vision_flip = RandomHorizontalFlip(p=1.0)\n    self.vision_processor = Compose(\n        [\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n\n    if vae_config_path is not None:\n        vae_config_dict = json.load(open(vae_config_path))\n        vae_scale_factor = 2 ** (\n            len(vae_config_dict.get(\"block_out_channels\", [])) - 1\n        )\n        self.vae_image_processor = VaeImageProcessor(\n            vae_scale_factor=vae_scale_factor\n        )\n</code></pre>"},{"location":"models/diffusers/#dreamboothxlfortext2imagegeneration","title":"DreamboothXLForText2ImageGeneration","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_dreambooth_xl.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    snr_gamma: Optional[float] = 5.0,\n    prior_loss_weight: Optional[float] = 1.0,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__()\n    self.seed = seed\n    self.num_train_timesteps = num_train_timesteps\n    self.num_infer_timesteps = num_infer_timesteps\n    self.image_size = image_size\n    self.snr_gamma = snr_gamma\n    self.prior_loss_weight = prior_loss_weight\n\n    config_dict = json.load(open(config_path))\n    if image_size is not None:\n        config_dict.update({\"sample_size\": image_size})\n    if in_channels is not None:\n        config_dict.update({\"in_channels\": in_channels})\n    if out_channels is not None:\n        config_dict.update({\"out_channels\": out_channels})\n    self.unet = UNet2DConditionModel.from_config(config_dict)\n\n    text_config = CLIPTextConfig.from_json_file(text_config_path)\n    self.text = CLIPTextModel(text_config)\n\n    vae_config_dict = json.load(open(vae_config_path))\n    self.vae = AutoencoderKL.from_config(vae_config_dict)\n\n    scheduler_config_dict = json.load(open(scheduler_config_path))\n    scheduler_class_name = scheduler_config_dict.get(\"_class_name\", \"DDPMScheduler\")\n    assert hasattr(schedulers, scheduler_class_name)\n    scheduler_class = getattr(schedulers, scheduler_class_name)\n    assert issubclass(scheduler_class, SchedulerMixin)\n    scheduler_config_dict[\"num_train_timesteps\"] = num_train_timesteps\n    self.scheduler = scheduler_class.from_config(scheduler_config_dict)\n\n    self.vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n\n    if freeze_vae_encoder:\n        for param in self.vae.parameters():\n            param.requires_grad = False\n\n    if freeze_text_encoder:\n        for param in self.text.parameters():\n            param.requires_grad = False\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(self.quant_config, ignore_modules=[\"lm_head\", \"unet\", \"vae\"])\n\n    if lora_r is not None:\n        for param in self.unet.parameters():\n            param.requires_grad = False\n        self.enable_lora(lora_r=lora_r)\n</code></pre>"},{"location":"models/diffusers/#multicontrolnetprocessor","title":"MultiControlNetProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> Source code in <code>src/unitorch/models/diffusers/processing_multicontrolnet.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    vae_config_path: str,\n    max_seq_length: Optional[int] = 77,\n    position_start_id: Optional[int] = 0,\n    pad_token: Optional[str] = \"&lt;|endoftext|&gt;\",\n    image_size: Optional[int] = 512,\n):\n    tokenizer = CLIPTokenizer(\n        vocab_file=vocab_path,\n        merges_file=merge_path,\n    )\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = pad_token\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        position_start_id=position_start_id,\n    )\n    self.vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n            Normalize([0.5], [0.5]),\n        ]\n    )\n    self.condition_vision_processor = Compose(\n        [\n            Resize(image_size),\n            CenterCrop(image_size),\n            ToTensor(),\n        ]\n    )\n\n    vae_config_dict = json.load(open(vae_config_path))\n    vae_scale_factor = 2 ** (len(vae_config_dict.get(\"block_out_channels\", [])) - 1)\n    self.vae_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor, do_convert_rgb=True\n    )\n    self.vae_mask_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor\n    )\n    self.vae_condition_image_processor = VaeImageProcessor(\n        vae_scale_factor=vae_scale_factor,\n        do_convert_rgb=True,\n        do_normalize=False,\n    )\n</code></pre>"},{"location":"models/diffusers/#multicontrolnetfortext2imagegeneration","title":"MultiControlNetForText2ImageGeneration","text":"<p>             Bases: <code>GenericMultiControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_multicontrolnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#multicontrolnetforimage2imagegeneration","title":"MultiControlNetForImage2ImageGeneration","text":"<p>             Bases: <code>GenericMultiControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_multicontrolnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetImg2ImgPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/diffusers/#multicontrolnetforimageinpainting","title":"MultiControlNetForImageInpainting","text":"<p>             Bases: <code>GenericMultiControlNetModel</code></p> Source code in <code>src/unitorch/models/diffusers/modeling_multicontrolnet.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    text_config_path: str,\n    vae_config_path: str,\n    controlnet_config_path: Union[str, List[str]],\n    scheduler_config_path: str,\n    quant_config_path: Optional[str] = None,\n    image_size: Optional[int] = None,\n    in_channels: Optional[int] = None,\n    out_channels: Optional[int] = None,\n    num_train_timesteps: Optional[int] = 1000,\n    num_infer_timesteps: Optional[int] = 50,\n    freeze_vae_encoder: Optional[bool] = True,\n    freeze_text_encoder: Optional[bool] = True,\n    freeze_unet_encoder: Optional[bool] = True,\n    lora_r: Optional[int] = None,\n    seed: Optional[int] = 1123,\n):\n    super().__init__(\n        config_path=config_path,\n        text_config_path=text_config_path,\n        vae_config_path=vae_config_path,\n        controlnet_config_path=controlnet_config_path,\n        scheduler_config_path=scheduler_config_path,\n        quant_config_path=quant_config_path,\n        image_size=image_size,\n        in_channels=in_channels,\n        out_channels=out_channels,\n        num_train_timesteps=num_train_timesteps,\n        num_infer_timesteps=num_infer_timesteps,\n        freeze_vae_encoder=freeze_vae_encoder,\n        freeze_text_encoder=freeze_text_encoder,\n        freeze_unet_encoder=freeze_unet_encoder,\n        lora_r=lora_r,\n        seed=seed,\n    )\n    self.pipeline = StableDiffusionControlNetInpaintPipeline(\n        vae=self.vae,\n        text_encoder=self.text,\n        unet=self.unet,\n        controlnet=self.controlnet,\n        scheduler=self.scheduler,\n        tokenizer=None,\n        safety_checker=None,\n        feature_extractor=None,\n    )\n    self.pipeline.set_progress_bar_config(disable=True)\n</code></pre>"},{"location":"models/llama/","title":"unitorch.models.llama","text":""},{"location":"models/llama/#llamaprocessor","title":"LlamaProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code>, <code>HfTextGenerationProcessor</code></p> <p>Initialize the LlamaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_file</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length for text classification. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum sequence length for text generation. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_file: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initialize the LlamaProcessor.\n\n    Args:\n        vocab_file (str): Path to the vocabulary file.\n        max_seq_length (int, optional): Maximum sequence length for text classification. Defaults to 128.\n        max_gen_seq_length (int, optional): Maximum sequence length for text generation. Defaults to 48.\n    \"\"\"\n    tokenizer = LlamaTokenizer(vocab_file=vocab_file)\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = tokenizer.unk_token\n    tokenizer.cls_token_id = tokenizer.bos_token_id\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    tokenizer.pad_token_id = tokenizer.unk_token_id\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n    )\n    HfTextGenerationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaProcessor.classification","title":"classification","text":"<pre><code>classification(\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Process text for classification.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>text_pair</code> <code>str</code> <p>Input text pair. Defaults to None.</p> <code>None</code> <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids and attention_mask tensors.</p> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def classification(\n    self,\n    text: str,\n    text_pair: Optional[str] = None,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for classification.\n\n    Args:\n        text (str): Input text.\n        text_pair (str, optional): Input text pair. Defaults to None.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids and attention_mask tensors.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n\n    tokens = self.tokenizer.tokenize(str(text))\n    if text_pair is None:\n        tokens = tokens[:max_seq_length]\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    else:\n        tokens_pair = self.tokenizer.tokenize(str(text_pair))\n        truncate_sequence_pair(tokens, tokens_pair, max_seq_length)\n        tokens = tokens + tokens_pair\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    padding = [0] * (max_seq_length - len(input_ids))\n    attention_mask = [0] * len(padding) + [1] * len(input_ids)\n    input_ids = len(padding) * [self.pad_token_id] + input_ids\n\n    assert len(input_ids) == max_seq_length\n    assert len(attention_mask) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaProcessor.generation","title":"generation","text":"<pre><code>generation(\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Process text for generation.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>text_pair</code> <code>str</code> <p>Input text pair.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids, attention_mask, input_ids_label, and attention_mask_label tensors.</p> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def generation(\n    self,\n    text: str,\n    text_pair: str,\n    max_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation.\n\n    Args:\n        text (str): Input text.\n        text_pair (str): Input text pair.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids, attention_mask, input_ids_label, and attention_mask_label tensors.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n\n    tokens = [self.bos_token] + self.tokenizer.tokenize(str(text))[\n        1 - max_seq_length :\n    ]\n    tokens_pair = self.tokenizer.tokenize(str(text_pair))[\n        : max_gen_seq_length - 1\n    ] + [self.eos_token]\n    padding_a = [self.pad_token] * (max_seq_length - len(tokens))\n    padding_b = [self.pad_token] * (max_gen_seq_length - len(tokens_pair))\n    attention_mask = (\n        [0] * len(padding_a)\n        + [1] * (len(tokens) + len(tokens_pair))\n        + [0] * len(padding_b)\n    )\n    _tokens = padding_a + tokens + tokens_pair + padding_b\n    input_ids = self.tokenizer.convert_tokens_to_ids(_tokens)\n\n    tokens_label = tokens_pair + [self.pad_token] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    input_ids_label = self.tokenizer.convert_tokens_to_ids(tokens_label)\n    input_ids_label = [0] * (max_seq_length - 1) + input_ids_label\n    attention_mask_label = [1] * len(tokens_pair) + [0] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    attention_mask_label = [0] * (max_seq_length - 1) + attention_mask_label\n\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n        input_ids_label=torch.tensor(input_ids_label, dtype=torch.long),\n        attention_mask_label=torch.tensor(attention_mask_label, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaProcessor.generation_inputs","title":"generation_inputs","text":"<pre><code>generation_inputs(\n    text: str, max_seq_length: Optional[int] = None\n)\n</code></pre> <p>Process text for generation inputs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids tensor.</p> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def generation_inputs(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation inputs.\n\n    Args:\n        text (str): Input text.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids tensor.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    tokens = [self.bos_token] + self.tokenizer.tokenize(str(text))[\n        1 - max_seq_length :\n    ]\n    padding = [self.pad_token] * (max_seq_length - len(tokens))\n    tokens = padding + tokens\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    assert len(input_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaProcessor.generation_labels","title":"generation_labels","text":"<pre><code>generation_labels(\n    text: str, max_gen_seq_length: Optional[int] = None\n)\n</code></pre> <p>Process text for generation labels.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids and attention_mask tensors.</p> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def generation_labels(\n    self,\n    text: str,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation labels.\n\n    Args:\n        text (str): Input text.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids and attention_mask tensors.\n    \"\"\"\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n    tokens = self.tokenizer.tokenize(str(text))[: max_gen_seq_length - 1] + [\n        self.eos_token\n    ]\n    padding = [self.pad_token] * (max_gen_seq_length - len(tokens))\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_gen_seq_length - len(input_ids))\n    input_ids += [self.pad_token_id] * len(padding)\n    attention_mask += padding\n\n    assert len(input_ids) == max_gen_seq_length\n    assert len(attention_mask) == max_gen_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaProcessor.prompt","title":"prompt","text":"<pre><code>prompt(text: str, max_seq_length: Optional[int] = None)\n</code></pre> <p>Process text as a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>max_seq_length</code> <code>int</code> <p>Maximum sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids tensor.</p> Source code in <code>src/unitorch/models/llama/processing.py</code> <pre><code>def prompt(\n    self,\n    text: str,\n    max_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text as a prompt.\n\n    Args:\n        text (str): Input text.\n        max_seq_length (int, optional): Maximum sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids tensor.\n    \"\"\"\n    max_seq_length = pop_value(\n        max_seq_length,\n        self.max_seq_length,\n    )\n    tokens = [self.bos_token] + self.tokenizer.tokenize(str(text))[\n        1 - max_seq_length :\n    ]\n    padding = [self.pad_token] * (max_seq_length - len(tokens))\n    tokens = padding + tokens\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n\n    assert len(input_ids) == max_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/llama/#llamaforclassification","title":"LlamaForClassification","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> <p>Llama model for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for classification. Defaults to 1.</p> <code>1</code> <code>hidden_dropout_prob</code> <code>float</code> <p>Dropout probability for hidden layers. Defaults to 0.1.</p> <code>0.1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    num_classes: Optional[int] = 1,\n    hidden_dropout_prob: Optional[float] = 0.1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Llama model for classification tasks.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        num_classes (int, optional): Number of classes for classification. Defaults to 1.\n        hidden_dropout_prob (float, optional): Dropout probability for hidden layers. Defaults to 0.1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = LlamaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = LlamaModel(self.config)\n    self.dropout = nn.Dropout(hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(self.quant_config, ignore_modules=[\"lm_head\"])\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the classification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, num_classes).</p> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the classification model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, num_classes).\n    \"\"\"\n    outputs = self.model(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )[0]\n    pooled_output = outputs[:, -1]\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/llama/#llamaforpretrain","title":"LlamaForPretrain","text":"<p>             Bases: <code>GenericModel</code></p> <p>Llama model for pretraining tasks.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Llama model for pretraining tasks.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = LlamaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = LlamaModel(self.config)\n    self.lm_head = nn.Linear(\n        self.config.hidden_size, self.config.vocab_size, bias=False\n    )\n    self.init_weights()\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(self.quant_config, ignore_modules=[\"lm_head\"])\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    input_ids_label: Optional[torch.Tensor] = None,\n    attention_mask_label: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the pretraining model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length). Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>input_ids_label</code> <code>Tensor</code> <p>Labeled input tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>attention_mask_label</code> <code>Tensor</code> <p>Labeled attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output loss.Tensor: Loss value.</p> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    input_ids_label: Optional[torch.Tensor] = None,\n    attention_mask_label: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the pretraining model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input tensor of shape (batch_size, sequence_length). Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n        input_ids_label (torch.Tensor, optional): Labeled input tensor of shape (batch_size, sequence_length). Defaults to None.\n        attention_mask_label (torch.Tensor, optional): Labeled attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output loss.Tensor: Loss value.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )\n    predict_logits = self.lm_head(outputs[0])\n    batch_size, seq_len, num_classes = predict_logits.size()\n    logits = predict_logits.contiguous().view(batch_size * seq_len, num_classes)\n    targets = input_ids_label.contiguous().view(-1).long()\n    masks = attention_mask_label.contiguous().view(-1)\n    loss = nn.CrossEntropyLoss(reduction=\"none\")(logits, targets)\n    loss = loss * masks.float()\n    loss = loss.contiguous().view(batch_size, seq_len).sum(1) / torch.max(\n        masks.contiguous().view(batch_size, seq_len).float().sum(1),\n        torch.ones(batch_size).to(masks.device),\n    )\n    loss = torch.mean(loss)\n    return loss\n</code></pre>"},{"location":"models/llama/#llamaforgeneration","title":"LlamaForGeneration","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> <p>Llama model for text generation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Llama model for text generation tasks.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = LlamaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = LlamaForCausalLM(self.config)\n    self.init_weights()\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(self.quant_config, ignore_modules=[\"lm_head\"])\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length). Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).</p> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the generation model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input tensor of shape (batch_size, sequence_length). Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/llama/#unitorch.models.llama.LlamaForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate text using the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 2.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Penalty for repeated tokens. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Penalty for longer sequences. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Penalty for diverse sequences in diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k value for sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p value for sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Generated sequences and their scores.</p> Source code in <code>src/unitorch/models/llama/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate text using the generation model.\n\n    Args:\n        input_ids: Input tensor of shape (batch_size, sequence_length).\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): Penalty for repeated tokens. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): Penalty for longer sequences. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Penalty for diverse sequences in diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k value for sampling. Defaults to 50.\n        top_p (float, optional): Top-p value for sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: Generated sequences and their scores.\n    \"\"\"\n    input_seq_length = input_ids.size(1)\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length + input_seq_length,\n        min_length=min_gen_seq_length + input_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1) - input_seq_length].copy_(\n        sequences[:, :, input_seq_length : sequences.size(-1)]\n    )\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences.long(),\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/mbart/","title":"unitorch.models.mbart","text":""},{"location":"models/mbart/#mbartprocessor","title":"MBartProcessor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> <p>Initializes an MBartProcessor with the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum generation sequence length. Defaults to 48.</p> <code>48</code> <code>special_input_ids</code> <code>Dict</code> <p>A dictionary of special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> Source code in <code>src/unitorch/models/mbart/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n    special_input_ids: Optional[Dict] = dict(),\n):\n    \"\"\"\n    Initializes an MBartProcessor with the provided parameters.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum generation sequence length. Defaults to 48.\n        special_input_ids (Dict, optional): A dictionary of special input IDs. Defaults to an empty dictionary.\n    \"\"\"\n    tokenizer = get_mbart_tokenizer(\n        vocab_path,\n        special_input_ids=special_input_ids,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/mbart/#mbartforgeneration","title":"MBartForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes an MBartForGeneration model with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>freeze_input_embedding</code> <code>bool</code> <p>Whether to freeze the input embeddings. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/mbart/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    freeze_input_embedding: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes an MBartForGeneration model with the provided configuration.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        freeze_input_embedding (bool, optional): Whether to freeze the input embeddings. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = MBartConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = MBartForConditionalGeneration(self.config)\n\n    if freeze_input_embedding:\n        for param in self.model.get_input_embeddings().parameters():\n            param.requires_grad = False\n\n    self.init_weights()\n</code></pre>"},{"location":"models/mbart/#unitorch.models.mbart.MBartForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs forward pass of the MBartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor of decoder attention mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/mbart/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs forward pass of the MBartForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor of attention mask.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor of decoder attention mask.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/mbart/#unitorch.models.mbart.MBartForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the MBartForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The decoder's start token ID. Defaults to 2.</p> <code>2</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The decoder's end token ID. Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/mbart/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 2,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the MBartForGeneration model.\n\n    Args:\n        input_ids: The input token IDs.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The decoder's start token ID. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The decoder's end token ID. Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/minigpt4/","title":"unitorch.models.minigpt4","text":""},{"location":"models/minigpt4/#minigpt4blip2llamaprocessor","title":"MiniGPT4Blip2LlamaProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code>, <code>HfImageClassificationProcessor</code>, <code>HfTextGenerationProcessor</code></p> <p>MiniGPT4Blip2LlamaProcessor is a class for processing inputs and outputs of the MiniGPT4 model with Blip2 and Llama. It inherits from the _MiniGPT4Blip2LlamaProcessor class.</p> <p>Initializes a MiniGPT4Blip2LlamaProcessor instance.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The file path to the vocabulary.</p> required <code>vision_config_path</code> <code>str</code> <p>The file path to the vision configuration.</p> required <code>max_prefix_seq_length</code> <code>int</code> <p>The maximum length of the prefix sequence. Defaults to 64.</p> <code>64</code> <code>max_suffix_seq_length</code> <code>int</code> <p>The maximum length of the suffix sequence. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequence. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/minigpt4/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_file: str,\n    vision_config_path: str,\n    max_prefix_seq_length: Optional[int] = 64,\n    max_suffix_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes a MiniGPT4Blip2LlamaProcessor instance.\n\n    Args:\n        vocab_path (str): The file path to the vocabulary.\n        vision_config_path (str): The file path to the vision configuration.\n        max_prefix_seq_length (int, optional): The maximum length of the prefix sequence. Defaults to 64.\n        max_suffix_seq_length (int, optional): The maximum length of the suffix sequence. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequence. Defaults to 48.\n    \"\"\"\n    tokenizer = LlamaTokenizer(vocab_file=vocab_file)\n    tokenizer.cls_token = tokenizer.bos_token\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.pad_token = tokenizer.unk_token\n    tokenizer.cls_token_id = tokenizer.bos_token_id\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    tokenizer.pad_token_id = tokenizer.unk_token_id\n    vision_processor = BlipImageProcessor.from_json_file(vision_config_path)\n    HfTextClassificationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_prefix_seq_length,\n    )\n    HfTextGenerationProcessor.__init__(\n        self,\n        tokenizer=tokenizer,\n        max_seq_length=max_prefix_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n    HfImageClassificationProcessor.__init__(self, vision_processor=vision_processor)\n    self.max_prefix_seq_length = max_prefix_seq_length\n    self.max_suffix_seq_length = max_suffix_seq_length\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaProcessor.generation","title":"generation","text":"<pre><code>generation(\n    prefix_text: str,\n    suffix_text: str,\n    text_pair: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Process text for generation.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_text</code> <code>str</code> <p>The prefix text.</p> required <code>suffix_text</code> <code>str</code> <p>The suffix text.</p> required <code>text_pair</code> <code>str</code> <p>The text pair.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <code>max_prefix_seq_length</code> <code>int</code> <p>The maximum length of the prefix sequence. Defaults to None.</p> <code>None</code> <code>max_suffix_seq_length</code> <code>int</code> <p>The maximum length of the suffix sequence. Defaults to None.</p> <code>None</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequence. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids, attention_mask, input_ids_label, and attention_mask_label tensors.</p> Source code in <code>src/unitorch/models/minigpt4/processing.py</code> <pre><code>def generation(\n    self,\n    prefix_text: str,\n    suffix_text: str,\n    text_pair: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation.\n\n    Args:\n        prefix_text (str): The prefix text.\n        suffix_text (str): The suffix text.\n        text_pair (str): The text pair.\n        image (PIL.Image.Image): The input image.\n        max_prefix_seq_length (int, optional): The maximum length of the prefix sequence. Defaults to None.\n        max_suffix_seq_length (int, optional): The maximum length of the suffix sequence. Defaults to None.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequence. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids, attention_mask, input_ids_label, and attention_mask_label tensors.\n    \"\"\"\n    max_prefix_seq_length = pop_value(\n        max_prefix_seq_length,\n        self.max_prefix_seq_length,\n    )\n    max_suffix_seq_length = pop_value(\n        max_suffix_seq_length,\n        self.max_suffix_seq_length,\n    )\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n    prefix_tokens = self.tokenizer.tokenize(str(prefix_text))\n\n    if len(prefix_tokens) &gt;= max_prefix_seq_length:\n        logging.warning(\n            f\"Input prefix text {prefix_text} has been truncated to {max_prefix_seq_length - 1} tokens.\"\n        )\n        prefix_tokens = prefix_tokens[: max_prefix_seq_length - 1]\n\n    suffix_tokens = self.tokenizer.tokenize(str(suffix_text))\n\n    if len(suffix_tokens) &gt; max_suffix_seq_length:\n        logging.warning(\n            f\"Input suffix text {suffix_text} has been truncated to {max_suffix_seq_length} tokens.\"\n        )\n        suffix_tokens = suffix_tokens[:max_suffix_seq_length]\n\n    prefix_tokens = [self.bos_token] + prefix_tokens\n    prefix_padding = [self.pad_token] * (max_prefix_seq_length - len(prefix_tokens))\n    prefix_attention_mask = [0] * len(prefix_padding) + [1] * len(prefix_tokens)\n    prefix_tokens = prefix_padding + prefix_tokens\n    prefix_input_ids = self.tokenizer.convert_tokens_to_ids(prefix_tokens)\n\n    suffix_padding = [self.pad_token] * (max_suffix_seq_length - len(suffix_tokens))\n    suffix_attention_mask = [0] * len(suffix_padding) + [1] * len(suffix_tokens)\n    suffix_tokens = suffix_padding + suffix_tokens\n    suffix_input_ids = self.tokenizer.convert_tokens_to_ids(suffix_tokens)\n\n    tokens_pair = self.tokenizer.tokenize(str(text_pair))[\n        : max_gen_seq_length - 1\n    ] + [self.eos_token]\n\n    padding_pair = [self.pad_token] * (max_gen_seq_length - len(tokens_pair))\n    input_ids_pair = self.tokenizer.convert_tokens_to_ids(\n        tokens_pair + padding_pair\n    )\n    attention_mask_pair = [1] * len(tokens_pair) + [0] * len(padding_pair)\n\n    tokens_label = tokens_pair + [self.pad_token] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    input_ids_label = self.tokenizer.convert_tokens_to_ids(tokens_label)\n    input_ids_label = [0] * (max_suffix_seq_length - 1) + input_ids_label\n    attention_mask_label = [1] * len(tokens_pair) + [0] * (\n        max_gen_seq_length - len(tokens_pair) + 1\n    )\n    attention_mask_label = [0] * (max_suffix_seq_length - 1) + attention_mask_label\n\n    outputs = HfImageClassificationProcessor.classification(\n        self,\n        image=image,\n    )\n\n    return GenericOutputs(\n        prefix_input_ids=torch.tensor(prefix_input_ids, dtype=torch.long),\n        prefix_attention_mask=torch.tensor(prefix_attention_mask, dtype=torch.long),\n        suffix_input_ids=torch.tensor(suffix_input_ids, dtype=torch.long),\n        suffix_attention_mask=torch.tensor(suffix_attention_mask, dtype=torch.long),\n        input_ids_pair=torch.tensor(input_ids_pair, dtype=torch.long),\n        attention_mask_pair=torch.tensor(attention_mask_pair, dtype=torch.long),\n        input_ids_label=torch.tensor(input_ids_label, dtype=torch.long),\n        attention_mask_label=torch.tensor(attention_mask_label, dtype=torch.long),\n        pixel_values=outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaProcessor.generation_inputs","title":"generation_inputs","text":"<pre><code>generation_inputs(\n    prefix_text: str,\n    suffix_text: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Process text for generation inputs.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_text</code> <code>str</code> <p>The prefix text.</p> required <code>suffix_text</code> <code>str</code> <p>The suffix text.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <code>max_prefix_seq_length</code> <code>int</code> <p>The maximum length of the prefix sequence. Defaults to None.</p> <code>None</code> <code>max_suffix_seq_length</code> <code>int</code> <p>The maximum length of the suffix sequence. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids tensor.</p> Source code in <code>src/unitorch/models/minigpt4/processing.py</code> <pre><code>def generation_inputs(\n    self,\n    prefix_text: str,\n    suffix_text: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation inputs.\n\n    Args:\n        prefix_text (str): The prefix text.\n        suffix_text (str): The suffix text.\n        image (PIL.Image.Image): The input image.\n        max_prefix_seq_length (int, optional): The maximum length of the prefix sequence. Defaults to None.\n        max_suffix_seq_length (int, optional): The maximum length of the suffix sequence. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids tensor.\n    \"\"\"\n    max_prefix_seq_length = pop_value(\n        max_prefix_seq_length,\n        self.max_prefix_seq_length,\n    )\n    max_suffix_seq_length = pop_value(\n        max_suffix_seq_length,\n        self.max_suffix_seq_length,\n    )\n    prefix_tokens = self.tokenizer.tokenize(str(prefix_text))\n\n    if len(prefix_tokens) &gt;= max_prefix_seq_length:\n        logging.warning(\n            f\"Input prefix text {prefix_text} has been truncated to {max_prefix_seq_length - 1} tokens.\"\n        )\n        prefix_tokens = prefix_tokens[: max_prefix_seq_length - 1]\n\n    suffix_tokens = self.tokenizer.tokenize(str(suffix_text))\n\n    if len(suffix_tokens) &gt; max_suffix_seq_length:\n        logging.warning(\n            f\"Input suffix text {suffix_text} has been truncated to {max_suffix_seq_length} tokens.\"\n        )\n        suffix_tokens = suffix_tokens[:max_suffix_seq_length]\n\n    prefix_tokens = [self.bos_token] + prefix_tokens\n    prefix_padding = [self.pad_token] * (max_prefix_seq_length - len(prefix_tokens))\n    prefix_tokens = prefix_padding + prefix_tokens\n    prefix_input_ids = self.tokenizer.convert_tokens_to_ids(prefix_tokens)\n\n    suffix_padding = [self.pad_token] * (max_suffix_seq_length - len(suffix_tokens))\n    suffix_tokens = suffix_padding + suffix_tokens\n    suffix_input_ids = self.tokenizer.convert_tokens_to_ids(suffix_tokens)\n\n    outputs = HfImageClassificationProcessor.classification(\n        self,\n        image=image,\n    )\n\n    return GenericOutputs(\n        prefix_input_ids=torch.tensor(prefix_input_ids, dtype=torch.long),\n        suffix_input_ids=torch.tensor(suffix_input_ids, dtype=torch.long),\n        pixel_values=outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaProcessor.generation_labels","title":"generation_labels","text":"<pre><code>generation_labels(\n    text: str, max_gen_seq_length: Optional[int] = None\n)\n</code></pre> <p>Process text for generation labels.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text.</p> required <code>max_gen_seq_length</code> <code>int</code> <p>Maximum generation sequence length. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids and attention_mask tensors.</p> Source code in <code>src/unitorch/models/minigpt4/processing.py</code> <pre><code>def generation_labels(\n    self,\n    text: str,\n    max_gen_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text for generation labels.\n\n    Args:\n        text (str): Input text.\n        max_gen_seq_length (int, optional): Maximum generation sequence length. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids and attention_mask tensors.\n    \"\"\"\n    max_gen_seq_length = pop_value(\n        max_gen_seq_length,\n        self.max_gen_seq_length,\n    )\n    tokens = self.tokenizer.tokenize(str(text))[: max_gen_seq_length - 1] + [\n        self.eos_token\n    ]\n    padding = [self.pad_token] * (max_gen_seq_length - len(tokens))\n    input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n    attention_mask = [1] * len(input_ids)\n\n    padding = [0] * (max_gen_seq_length - len(input_ids))\n    input_ids += [self.pad_token_id] * len(padding)\n    attention_mask += padding\n\n    assert len(input_ids) == max_gen_seq_length\n    assert len(attention_mask) == max_gen_seq_length\n    return GenericOutputs(\n        input_ids=torch.tensor(input_ids, dtype=torch.long),\n        attention_mask=torch.tensor(attention_mask, dtype=torch.long),\n    )\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaProcessor.prompt","title":"prompt","text":"<pre><code>prompt(\n    prefix_text: str,\n    suffix_text: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n)\n</code></pre> <p>Process text as a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prefix_text</code> <code>str</code> <p>The prefix text.</p> required <code>suffix_text</code> <code>str</code> <p>The suffix text.</p> required <code>image</code> <code>Image</code> <p>The input image.</p> required <code>max_prefix_seq_length</code> <code>int</code> <p>The maximum length of the prefix sequence. Defaults to None.</p> <code>None</code> <code>max_suffix_seq_length</code> <code>int</code> <p>The maximum length of the suffix sequence. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Processed input_ids tensor.</p> Source code in <code>src/unitorch/models/minigpt4/processing.py</code> <pre><code>def prompt(\n    self,\n    prefix_text: str,\n    suffix_text: str,\n    image: Union[Image.Image, str],\n    max_prefix_seq_length: Optional[int] = None,\n    max_suffix_seq_length: Optional[int] = None,\n):\n    \"\"\"\n    Process text as a prompt.\n\n    Args:\n        prefix_text (str): The prefix text.\n        suffix_text (str): The suffix text.\n        image (PIL.Image.Image): The input image.\n        max_prefix_seq_length (int, optional): The maximum length of the prefix sequence. Defaults to None.\n        max_suffix_seq_length (int, optional): The maximum length of the suffix sequence. Defaults to None.\n\n    Returns:\n        GenericOutputs: Processed input_ids tensor.\n    \"\"\"\n    max_prefix_seq_length = pop_value(\n        max_prefix_seq_length,\n        self.max_prefix_seq_length,\n    )\n    max_suffix_seq_length = pop_value(\n        max_suffix_seq_length,\n        self.max_suffix_seq_length,\n    )\n    prefix_tokens = self.tokenizer.tokenize(str(prefix_text))\n\n    if len(prefix_tokens) &gt;= max_prefix_seq_length:\n        logging.warning(\n            f\"Input prefix text {prefix_text} has been truncated to {max_prefix_seq_length - 1} tokens.\"\n        )\n        prefix_tokens = prefix_tokens[: max_prefix_seq_length - 1]\n\n    suffix_tokens = self.tokenizer.tokenize(str(suffix_text))\n\n    if len(suffix_tokens) &gt; max_suffix_seq_length:\n        logging.warning(\n            f\"Input suffix text {suffix_text} has been truncated to {max_suffix_seq_length} tokens.\"\n        )\n        suffix_tokens = suffix_tokens[:max_suffix_seq_length]\n\n    prefix_tokens = [self.bos_token] + prefix_tokens\n    prefix_padding = [self.pad_token] * (max_prefix_seq_length - len(prefix_tokens))\n    prefix_tokens = prefix_padding + prefix_tokens\n    prefix_input_ids = self.tokenizer.convert_tokens_to_ids(prefix_tokens)\n\n    suffix_padding = [self.pad_token] * (max_suffix_seq_length - len(suffix_tokens))\n    suffix_tokens = suffix_padding + suffix_tokens\n    suffix_input_ids = self.tokenizer.convert_tokens_to_ids(suffix_tokens)\n\n    outputs = HfImageClassificationProcessor.classification(\n        self,\n        image=image,\n    )\n\n    return GenericOutputs(\n        prefix_input_ids=torch.tensor(prefix_input_ids, dtype=torch.long),\n        suffix_input_ids=torch.tensor(suffix_input_ids, dtype=torch.long),\n        pixel_values=outputs.pixel_values,\n    )\n</code></pre>"},{"location":"models/minigpt4/#minigpt4blip2llamamodel","title":"MiniGPT4Blip2LlamaModel","text":"<p>             Bases: <code>Module</code></p> <p>MiniGPT4Blip2LlamaModel is a model that combines the Blip2VisionModel, Blip2QFormerModel, and LlamaForCausalLM models for generation. It inherits from the nn.Module class.</p> <p>Initializes a MiniGPT4Blip2LlamaModel instance.</p> <p>Parameters:</p> Name Type Description Default <code>blip2_config</code> <code>Blip2Config</code> <p>The configuration for the Blip2 model.</p> required <code>llama_config</code> <code>LlamaConfig</code> <p>The configuration for the Llama model.</p> required Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>def __init__(\n    self,\n    blip2_config: Blip2Config,\n    llama_config: LlamaConfig,\n):\n    \"\"\"\n    Initializes a MiniGPT4Blip2LlamaModel instance.\n\n    Args:\n        blip2_config (Blip2Config): The configuration for the Blip2 model.\n        llama_config (LlamaConfig): The configuration for the Llama model.\n    \"\"\"\n    super().__init__()\n    self.blip2_config = blip2_config\n    self.vision_model = Blip2VisionModel(self.blip2_config.vision_config)\n\n    self.query_tokens = nn.Parameter(\n        torch.zeros(\n            1,\n            self.blip2_config.num_query_tokens,\n            self.blip2_config.qformer_config.hidden_size,\n        )\n    )\n    self.qformer = Blip2QFormerModel(self.blip2_config.qformer_config)\n\n    self.llama_config = llama_config\n    self.language_projection = nn.Linear(\n        self.blip2_config.qformer_config.hidden_size, self.llama_config.hidden_size\n    )\n    self.llama = LlamaForCausalLM(self.llama_config)\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaModel.forward","title":"forward","text":"<pre><code>forward(\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs a forward pass of the MiniGPT4Blip2LlamaModel.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens.</p> required <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>The input IDs for the decoder tokens.</p> required <code>prefix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the prefix tokens.</p> <code>None</code> <code>suffix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the suffix tokens.</p> <code>None</code> <code>decoder_attention_mask</code> <code>Tensor</code> <p>The attention mask for the decoder tokens.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>outputs</code> <p>The model outputs.</p> Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs a forward pass of the MiniGPT4Blip2LlamaModel.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values.\n        prefix_input_ids (torch.Tensor): The input IDs for the prefix tokens.\n        suffix_input_ids (torch.Tensor): The input IDs for the suffix tokens.\n        decoder_input_ids (torch.Tensor): The input IDs for the decoder tokens.\n        prefix_attention_mask (torch.Tensor, optional): The attention mask for the prefix tokens.\n        suffix_attention_mask (torch.Tensor, optional): The attention mask for the suffix tokens.\n        decoder_attention_mask (torch.Tensor, optional): The attention mask for the decoder tokens.\n\n    Returns:\n        outputs: The model outputs.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n    image_embeds = vision_outputs[0]\n    image_attention_mask = torch.ones(\n        image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device\n    )\n\n    query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n    query_outputs = self.qformer(\n        query_embeds=query_tokens,\n        encoder_hidden_states=image_embeds,\n        encoder_attention_mask=image_attention_mask,\n    )\n    query_embeds = query_outputs[0]\n\n    language_model_inputs = self.language_projection(query_embeds)\n    language_model_attention_mask = torch.ones(\n        language_model_inputs.size()[:-1],\n        dtype=torch.long,\n        device=language_model_inputs.device,\n    )\n    prefix_inputs_embeds = self.llama.get_input_embeddings()(prefix_input_ids)\n    suffix_inputs_embeds = self.llama.get_input_embeddings()(suffix_input_ids)\n    decoder_input_embeds = self.llama.get_input_embeddings()(decoder_input_ids)\n    inputs_embeds = torch.cat(\n        [\n            prefix_inputs_embeds,\n            language_model_inputs,\n            suffix_inputs_embeds,\n            decoder_input_embeds,\n        ],\n        dim=1,\n    )\n    expected_device = language_model_attention_mask.device\n\n    if prefix_attention_mask is None:\n        prefix_attention_mask = torch.ones(\n            prefix_inputs_embeds.size()[:-1],\n            dtype=torch.long,\n            device=expected_device,\n        )\n\n    if suffix_attention_mask is None:\n        suffix_attention_mask = torch.ones(\n            suffix_inputs_embeds.size()[:-1],\n            dtype=torch.long,\n            device=expected_device,\n        )\n\n    if decoder_attention_mask is None:\n        decoder_attention_mask = torch.ones(\n            decoder_input_embeds.size()[:-1],\n            dtype=torch.long,\n            device=expected_device,\n        )\n\n    attention_mask = torch.cat(\n        [\n            prefix_attention_mask,\n            language_model_attention_mask,\n            suffix_attention_mask,\n            decoder_attention_mask.to(expected_device),\n        ],\n        dim=1,\n    )\n    outputs = self.llama(\n        inputs_embeds=inputs_embeds,\n        attention_mask=attention_mask,\n        return_dict=True,\n    )\n    return outputs\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaModel.generate","title":"generate","text":"<pre><code>generate(\n    pixel_values: torch.FloatTensor,\n    prefix_input_ids: Optional[torch.Tensor] = None,\n    suffix_input_ids: Optional[torch.Tensor] = None,\n    **generate_kwargs\n)\n</code></pre> <p>Generates sequences using the MiniGPT4Blip2LlamaModel.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>FloatTensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens. Defaults to None.</p> <code>None</code> <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens. Defaults to None.</p> <code>None</code> <code>**generate_kwargs</code> <p>Additional keyword arguments for sequence generation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>outputs</code> <p>The generation outputs.</p> Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>def generate(\n    self,\n    pixel_values: torch.FloatTensor,\n    prefix_input_ids: Optional[torch.Tensor] = None,\n    suffix_input_ids: Optional[torch.Tensor] = None,\n    **generate_kwargs,\n):\n    \"\"\"\n    Generates sequences using the MiniGPT4Blip2LlamaModel.\n\n    Args:\n        pixel_values (torch.FloatTensor): The pixel values.\n        prefix_input_ids (torch.Tensor, optional): The input IDs for the prefix tokens. Defaults to None.\n        suffix_input_ids (torch.Tensor, optional): The input IDs for the suffix tokens. Defaults to None.\n        **generate_kwargs: Additional keyword arguments for sequence generation.\n\n    Returns:\n        outputs: The generation outputs.\n    \"\"\"\n    vision_outputs = self.vision_model(\n        pixel_values=pixel_values,\n    )\n    image_embeds = vision_outputs[0]\n    image_attention_mask = torch.ones(\n        image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device\n    )\n\n    query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n    query_outputs = self.qformer(\n        query_embeds=query_tokens,\n        encoder_hidden_states=image_embeds,\n        encoder_attention_mask=image_attention_mask,\n    )\n    query_embeds = query_outputs[0]\n\n    inputs_embeds = self.language_projection(query_embeds)\n\n    attention_mask = torch.ones(\n        inputs_embeds.size(0), inputs_embeds.size(1), dtype=torch.bool\n    ).to(inputs_embeds.device)\n\n    if prefix_input_ids is not None:\n        prefix_inputs_embeds = self.llama.get_input_embeddings()(prefix_input_ids)\n        inputs_embeds = torch.cat([prefix_inputs_embeds, inputs_embeds], dim=1)\n        attention_mask = torch.cat(\n            [prefix_input_ids.ne(self.blip2_config.pad_token_id), attention_mask],\n            dim=1,\n        )\n\n    if suffix_input_ids is not None:\n        suffix_inputs_embeds = self.llama.get_input_embeddings()(suffix_input_ids)\n        inputs_embeds = torch.cat([inputs_embeds, suffix_inputs_embeds], dim=1)\n        attention_mask = torch.cat(\n            [attention_mask, suffix_input_ids.ne(self.blip2_config.pad_token_id)],\n            dim=1,\n        )\n\n    outputs = self.llama.generate(\n        inputs_embeds=inputs_embeds,\n        attention_mask=attention_mask,\n        **generate_kwargs,\n    )\n\n    return outputs\n</code></pre>"},{"location":"models/minigpt4/#minigpt4blip2llamaforgeneration","title":"MiniGPT4Blip2LlamaForGeneration","text":"<p>             Bases: <code>GenericModel</code>, <code>QuantizationMixin</code></p> <p>MiniGPT4Blip2LlamaForGeneration is a generation model that combines the MiniGPT4Blip2LlamaModel with generation capabilities. It inherits from the GenericModel class.</p> <p>Initializes a MiniGPT4Blip2LlamaForGeneration instance.</p> <p>Parameters:</p> Name Type Description Default <code>blip2_config_path</code> <code>str</code> <p>The path to the Blip2 model configuration file.</p> required <code>llama_config_path</code> <code>str</code> <p>The path to the Llama model configuration file.</p> required <code>pad_token_id</code> <code>int</code> <p>The ID of the padding token. Defaults to 0.</p> <code>0</code> <code>freeze_vision_model</code> <code>bool</code> <p>Whether to freeze the parameters of the vision model. Defaults to True.</p> <code>True</code> <code>freeze_qformer_model</code> <code>bool</code> <p>Whether to freeze the parameters of the qformer model. Defaults to True.</p> <code>True</code> <code>freeze_llama_model</code> <code>bool</code> <p>Whether to freeze the parameters of the llama model. Defaults to True.</p> <code>True</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>def __init__(\n    self,\n    blip2_config_path: str,\n    llama_config_path: str,\n    quant_config_path: Optional[str] = None,\n    pad_token_id: Optional[int] = 0,\n    freeze_vision_model: Optional[bool] = True,\n    freeze_qformer_model: Optional[bool] = True,\n    freeze_llama_model: Optional[bool] = True,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a MiniGPT4Blip2LlamaForGeneration instance.\n\n    Args:\n        blip2_config_path (str): The path to the Blip2 model configuration file.\n        llama_config_path (str): The path to the Llama model configuration file.\n        pad_token_id (int, optional): The ID of the padding token. Defaults to 0.\n        freeze_vision_model (bool, optional): Whether to freeze the parameters of the vision model. Defaults to True.\n        freeze_qformer_model (bool, optional): Whether to freeze the parameters of the qformer model. Defaults to True.\n        freeze_llama_model (bool, optional): Whether to freeze the parameters of the llama model. Defaults to True.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.blip2_config = Blip2Config.from_json_file(blip2_config_path)\n    self.blip2_config.pad_token_id = pad_token_id\n    self.llama_config = LlamaConfig.from_json_file(llama_config_path)\n    self.llama_config.gradient_checkpointing = gradient_checkpointing\n    self.base_model = MiniGPT4Blip2LlamaModel(self.blip2_config, self.llama_config)\n    self.init_weights()\n\n    if freeze_vision_model:\n        for param in self.base_model.vision_model.parameters():\n            param.requires_grad = False\n\n    if freeze_qformer_model:\n        for param in self.base_model.qformer.parameters():\n            param.requires_grad = False\n\n    if freeze_llama_model:\n        for param in self.base_model.llama.parameters():\n            param.requires_grad = False\n\n    if quant_config_path is not None:\n        self.quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        self.quantize(\n            self.quant_config, ignore_modules=[\"language_projection\", \"lm_head\"]\n        )\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs a forward pass of the MiniGPT4Blip2LlamaForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens.</p> required <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>The input IDs for the decoder tokens.</p> required <code>prefix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the prefix tokens.</p> <code>None</code> <code>suffix_attention_mask</code> <code>Tensor</code> <p>The attention mask for the suffix tokens.</p> <code>None</code> <code>decoder_attention_mask</code> <code>Tensor</code> <p>The attention mask for the decoder tokens.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>logits</code> <p>The output logits.</p> Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    prefix_attention_mask: Optional[torch.Tensor] = None,\n    suffix_attention_mask: Optional[torch.Tensor] = None,\n    decoder_attention_mask: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs a forward pass of the MiniGPT4Blip2LlamaForGeneration model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values.\n        prefix_input_ids (torch.Tensor): The input IDs for the prefix tokens.\n        suffix_input_ids (torch.Tensor): The input IDs for the suffix tokens.\n        decoder_input_ids (torch.Tensor): The input IDs for the decoder tokens.\n        prefix_attention_mask (torch.Tensor, optional): The attention mask for the prefix tokens.\n        suffix_attention_mask (torch.Tensor, optional): The attention mask for the suffix tokens.\n        decoder_attention_mask (torch.Tensor, optional): The attention mask for the decoder tokens.\n\n    Returns:\n        logits: The output logits.\n    \"\"\"\n    outputs = self.base_model(\n        pixel_values=pixel_values,\n        prefix_input_ids=prefix_input_ids,\n        suffix_input_ids=suffix_input_ids,\n        decoder_input_ids=decoder_input_ids,\n        prefix_attention_mask=prefix_attention_mask,\n        suffix_attention_mask=suffix_attention_mask,\n        decoder_attention_mask=decoder_attention_mask,\n    )\n    logits = outputs.logits[\n        :, -suffix_input_ids.size(1) - decoder_input_ids.size(1) :, :\n    ]\n    return logits\n</code></pre>"},{"location":"models/minigpt4/#unitorch.models.minigpt4.MiniGPT4Blip2LlamaForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the MiniGPT4Blip2LlamaForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The pixel values.</p> required <code>prefix_input_ids</code> <code>Tensor</code> <p>The input IDs for the prefix tokens.</p> required <code>suffix_input_ids</code> <code>Tensor</code> <p>The input IDs for the suffix tokens.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 1.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of the n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early when all beams are finished. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature value for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>outputs</code> <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/minigpt4/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    pixel_values: torch.Tensor,\n    prefix_input_ids: torch.Tensor,\n    suffix_input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the MiniGPT4Blip2LlamaForGeneration model.\n\n    Args:\n        pixel_values (torch.Tensor): The pixel values.\n        prefix_input_ids (torch.Tensor): The input IDs for the prefix tokens.\n        suffix_input_ids (torch.Tensor): The input IDs for the suffix tokens.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 1.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of the n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early when all beams are finished. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p sampling. Defaults to 1.0.\n\n    Returns:\n        outputs (GenericOutputs): The generated sequences and their scores.\n    \"\"\"\n    outputs = self.base_model.generate(\n        pixel_values=pixel_values,\n        prefix_input_ids=prefix_input_ids,\n        suffix_input_ids=suffix_input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(\n        sequences[:, :, : sequences.size(-1)]\n    )\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences.long(),\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/mt5/","title":"unitorch.models.mt5","text":""},{"location":"models/mt5/#mt5processor","title":"MT5Processor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> <p>Initializes an MT5Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>Dictionary of special input tokens and their corresponding IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum length of input sequences. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of generated sequences. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/mt5/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes an MT5Processor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Dict, optional): Dictionary of special input tokens and their corresponding IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum length of input sequences. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum length of generated sequences. Defaults to 48.\n    \"\"\"\n    tokenizer = get_mt5_tokenizer(\n        vocab_path,\n        special_input_ids=special_input_ids,\n    )\n    tokenizer.bos_token_id = 0\n    tokenizer.bos_token = tokenizer.convert_ids_to_tokens(0)\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/mt5/#mt5forgeneration","title":"MT5ForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes an MT5ForGeneration model with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/mt5/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes an MT5ForGeneration model with the provided configuration.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = MT5Config.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = MT5ForConditionalGeneration(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/mt5/#unitorch.models.mt5.MT5ForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs forward pass of the MT5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor of decoder attention mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/mt5/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs forward pass of the MT5ForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor of attention mask.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor of decoder attention mask.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/mt5/#unitorch.models.mt5.MT5ForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the MT5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The decoder's start token ID. Defaults to 2.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The decoder's end token ID. Defaults to 2.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/mt5/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the MT5ForGeneration model.\n\n    Args:\n        input_ids: The input token IDs.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The decoder's start token ID. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The decoder's end token ID. Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/peft/","title":"unitorch.models.peft","text":""},{"location":"models/peft/#bloomloraforclassification","title":"BloomLoraForClassification","text":"<p>             Bases: <code>GenericPeftModel</code></p> Source code in <code>src/unitorch/models/peft/modeling_bloom_lora.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"query_key_value\"],\n    num_classes: Optional[int] = 1,\n    hidden_dropout_prob: Optional[float] = 0.1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    super().__init__()\n    self.config = BloomConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.peft_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n    )\n    self.peft_model = PeftModelForSequenceClassification(\n        BloomModel(self.config), self.peft_config\n    )\n    self.dropout = nn.Dropout(hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.BloomLoraForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the classification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, num_classes).</p> Source code in <code>src/unitorch/models/peft/modeling_bloom_lora.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the classification model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, num_classes).\n    \"\"\"\n    outputs = self.peft_model(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )[0]\n    pooled_output = outputs[:, -1]\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/peft/#bloomloraforgeneration","title":"BloomLoraForGeneration","text":"<p>             Bases: <code>GenericPeftModel</code></p> <p>Bloom Loar model for text generation tasks.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/peft/modeling_bloom_lora.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"query_key_value\"],\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Bloom Loar model for text generation tasks.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = BloomConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.peft_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n    )\n    self.peft_model = PeftModelForCausalLM(\n        BloomForCausalLM(self.config), self.peft_config\n    )\n    self.init_weights()\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.BloomLoraForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length). Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).</p> Source code in <code>src/unitorch/models/peft/modeling_bloom_lora.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the generation model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input tensor of shape (batch_size, sequence_length). Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).\n    \"\"\"\n    outputs = self.peft_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.BloomLoraForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate text using the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 2.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Penalty for repeated tokens. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Penalty for longer sequences. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Penalty for diverse sequences in diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k value for sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p value for sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Generated sequences and their scores.</p> Source code in <code>src/unitorch/models/peft/modeling_bloom_lora.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate text using the generation model.\n\n    Args:\n        input_ids: Input tensor of shape (batch_size, sequence_length).\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): Penalty for repeated tokens. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): Penalty for longer sequences. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Penalty for diverse sequences in diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k value for sampling. Defaults to 50.\n        top_p (float, optional): Top-p value for sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: Generated sequences and their scores.\n    \"\"\"\n    input_seq_length = input_ids.size(1)\n    outputs = self.peft_model.generate(\n        input_ids=input_ids,\n        max_length=max_gen_seq_length + input_seq_length,\n        min_length=min_gen_seq_length + input_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1) - input_seq_length].copy_(\n        sequences[:, :, input_seq_length : sequences.size(-1)]\n    )\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences.long(),\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/peft/#llamaloraforclassification","title":"LlamaLoraForClassification","text":"<p>             Bases: <code>GenericPeftModel</code>, <code>QuantizationMixin</code></p> Source code in <code>src/unitorch/models/peft/modeling_llama_lora.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"q_proj\", \"v_proj\"],\n    num_classes: Optional[int] = 1,\n    hidden_dropout_prob: Optional[float] = 0.1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    super().__init__()\n    self.config = LlamaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.peft_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n    )\n    model = LlamaModel(self.config)\n    if quant_config_path is not None:\n        quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        ignore_modules = target_modules + [\"lm_head\"]\n        model = quantize_model(model, quant_config, ignore_modules=ignore_modules)\n    self.peft_model = PeftModelForSequenceClassification(model, self.peft_config)\n    self.dropout = nn.Dropout(hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.LlamaLoraForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the classification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, num_classes).</p> Source code in <code>src/unitorch/models/peft/modeling_llama_lora.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the classification model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape (batch_size, sequence_length).\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, num_classes).\n    \"\"\"\n    outputs = self.peft_model(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n    )[0]\n    pooled_output = outputs[:, -1]\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/peft/#llamaloraforgeneration","title":"LlamaLoraForGeneration","text":"<p>             Bases: <code>GenericPeftModel</code></p> Source code in <code>src/unitorch/models/peft/modeling_llama_lora.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    quant_config_path: Optional[str] = None,\n    lora_r: Optional[int] = 16,\n    lora_alpha: Optional[int] = 32,\n    lora_dropout: Optional[float] = 0.05,\n    fan_in_fan_out: Optional[bool] = True,\n    target_modules: Optional[Union[List[str], str]] = [\"q_proj\", \"v_proj\"],\n    gradient_checkpointing: Optional[bool] = False,\n):\n    super().__init__()\n    self.config = LlamaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.peft_config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        fan_in_fan_out=fan_in_fan_out,\n        target_modules=target_modules,\n    )\n    model = LlamaForCausalLM(self.config)\n    if quant_config_path is not None:\n        quant_config = QuantizationConfig.from_json_file(quant_config_path)\n        ignore_modules = target_modules + [\"lm_head\"]\n        model = quantize_model(model, quant_config, ignore_modules=ignore_modules)\n    self.peft_model = PeftModelForCausalLM(model, self.peft_config)\n    self.init_weights()\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.LlamaLoraForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length). Defaults to None.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).</p> Source code in <code>src/unitorch/models/peft/modeling_llama_lora.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the generation model.\n\n    Args:\n        input_ids (torch.Tensor, optional): Input tensor of shape (batch_size, sequence_length). Defaults to None.\n        attention_mask (torch.Tensor, optional): Attention mask tensor of shape (batch_size, sequence_length). Defaults to None.\n        position_ids (torch.Tensor, optional): Position IDs tensor of shape (batch_size, sequence_length). Defaults to None.\n\n    Returns:\n        torch Output logits.Tensor: tensor of shape (batch_size, sequence_length, vocab_size).\n    \"\"\"\n    outputs = self.peft_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/peft/#unitorch.models.peft.LlamaLoraForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generate text using the generation model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, sequence_length).</p> required <code>num_beams</code> <code>int</code> <p>Number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The ID of the decoder start token. Defaults to 2.</p> <code>1</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The ID(s) of the decoder end token(s). Defaults to 2.</p> <code>2</code> <code>num_return_sequences</code> <code>int</code> <p>Number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>Minimum length of generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>Maximum length of generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>Penalty for repeated tokens. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>Size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>Penalty for longer sequences. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>Number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>Penalty for diverse sequences in diverse beam search. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>Sampling temperature. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>Top-k value for sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>Top-p value for sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>Generated sequences and their scores.</p> Source code in <code>src/unitorch/models/peft/modeling_llama_lora.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 1,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 2,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generate text using the generation model.\n\n    Args:\n        input_ids: Input tensor of shape (batch_size, sequence_length).\n        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The ID of the decoder start token. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The ID(s) of the decoder end token(s). Defaults to 2.\n        num_return_sequences (int, optional): Number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): Minimum length of generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): Maximum length of generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): Penalty for repeated tokens. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): Size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): Penalty for longer sequences. Defaults to 1.0.\n        num_beam_groups (int, optional): Number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): Penalty for diverse sequences in diverse beam search. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n        top_k (int, optional): Top-k value for sampling. Defaults to 50.\n        top_p (float, optional): Top-p value for sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: Generated sequences and their scores.\n    \"\"\"\n    input_seq_length = input_ids.size(1)\n    outputs = self.peft_model.generate(\n        input_ids=input_ids,\n        max_length=max_gen_seq_length + input_seq_length,\n        min_length=min_gen_seq_length + input_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1) - input_seq_length].copy_(\n        sequences[:, :, input_seq_length : sequences.size(-1)]\n    )\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences.long(),\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/pegasus/","title":"unitorch.models.pegasus","text":""},{"location":"models/pegasus/#pegasusprocessor","title":"PegasusProcessor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> <p>Initializes a PegasusProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Dict</code> <p>Dictionary of special input tokens and their corresponding IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>int</code> <p>The maximum length of input sequences. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of generated sequences. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/pegasus/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes a PegasusProcessor.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        special_input_ids (Dict, optional): Dictionary of special input tokens and their corresponding IDs. Defaults to an empty dictionary.\n        max_seq_length (int, optional): The maximum length of input sequences. Defaults to 128.\n        max_gen_seq_length (int, optional): The maximum length of generated sequences. Defaults to 48.\n    \"\"\"\n    tokenizer = get_pegasus_tokenizer(\n        vocab_path,\n        special_input_ids=special_input_ids,\n    )\n    tokenizer.bos_token_id = 0\n    tokenizer.bos_token = tokenizer.convert_ids_to_tokens(0)\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/pegasus/#pegasusforgeneration","title":"PegasusForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes a PegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/pegasus/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a PegasusForGeneration model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = PegasusConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = PegasusForConditionalGeneration(self.config)\n\n    self.init_weights()\n</code></pre>"},{"location":"models/pegasus/#unitorch.models.pegasus.PegasusForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs a forward pass of the PegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor of decoder attention mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/pegasus/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs a forward pass of the PegasusForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor of attention mask.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor of decoder attention mask.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/pegasus/#unitorch.models.pegasus.PegasusForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the PegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The decoder's start token ID. Defaults to 2.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The decoder's end token ID. Defaults to 2.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/pegasus/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the PegasusForGeneration model.\n\n    Args:\n        input_ids: The input token IDs.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The decoder's start token ID. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The decoder's end token ID. Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/roberta/","title":"unitorch.models.roberta","text":""},{"location":"models/roberta/#robertaprocessor","title":"RobertaProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> <p>Initializes a RobertaProcessor for text classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>The path to the vocabulary file.</p> required <code>merge_path</code> <code>str</code> <p>The path to the merge file.</p> required <code>max_seq_length</code> <code>int</code> <p>The maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>int</code> <p>The ID for the source type. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>int</code> <p>The ID for the target type. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/roberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    merge_path: str,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes a RobertaProcessor for text classification tasks.\n\n    Args:\n        vocab_path (str): The path to the vocabulary file.\n        merge_path (str): The path to the merge file.\n        max_seq_length (int, optional): The maximum sequence length. Defaults to 128.\n        source_type_id (int, optional): The ID for the source type. Defaults to 0.\n        target_type_id (int, optional): The ID for the target type. Defaults to 0.\n    \"\"\"\n    tokenizer = get_roberta_tokenizer(\n        vocab_path,\n        merge_path,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n        position_start_id=tokenizer.pad_token_id + 1,\n    )\n</code></pre>"},{"location":"models/roberta/#robertaforclassification","title":"RobertaForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes a RobertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a RobertaForClassification model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = RobertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.roberta = RobertaModel(self.config)\n    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/roberta/#unitorch.models.roberta.RobertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs forward pass of the RobertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Tensor of token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Tensor of position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/roberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs forward pass of the RobertaForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor, optional): Tensor of attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Tensor of token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Tensor of position IDs. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.roberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = outputs[1]\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/roberta/#robertaformasklm","title":"RobertaForMaskLM","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes a RobertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes a RobertaForMaskLM model.\n\n    Args:\n        config_path (str): The path to the model configuration file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = RobertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.roberta = RobertaModel(self.config, add_pooling_layer=False)\n    self.lm_head = RobertaLMHead(self.config)\n    self.lm_head.decoder.weight = self.roberta.embeddings.word_embeddings.weight\n    self.init_weights()\n</code></pre>"},{"location":"models/roberta/#unitorch.models.roberta.RobertaForMaskLM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Performs forward pass of the RobertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Tensor</code> <p>Tensor of token type IDs. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Tensor</code> <p>Tensor of position IDs. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/roberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Performs forward pass of the RobertaForMaskLM model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor, optional): Tensor of attention mask. Defaults to None.\n        token_type_ids (torch.Tensor, optional): Tensor of token type IDs. Defaults to None.\n        position_ids (torch.Tensor, optional): Tensor of position IDs. Defaults to None.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.roberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    sequence_output = outputs[0]\n    logits = self.lm_head(sequence_output)\n    return logits\n</code></pre>"},{"location":"models/swin/","title":"unitorch.models.swin","text":""},{"location":"models/swin/#swinprocessor","title":"SwinProcessor","text":"<p>             Bases: <code>HfImageClassificationProcessor</code></p> <p>Initializes a SwinProcessor for image classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>The path to the ViTImageProcessor configuration file.</p> required Source code in <code>src/unitorch/models/swin/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initializes a SwinProcessor for image classification tasks.\n\n    Args:\n        vision_config_path (str): The path to the ViTImageProcessor configuration file.\n    \"\"\"\n    vision_processor = ViTImageProcessor.from_json_file(vision_config_path)\n    super().__init__(\n        vision_processor=vision_processor,\n    )\n</code></pre>"},{"location":"models/swin/#swinforimageclassification","title":"SwinForImageClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>Initializes a SwinForImageClassification model for image classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the Swin Transformer configuration file.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes for classification. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/models/swin/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initializes a SwinForImageClassification model for image classification tasks.\n\n    Args:\n        config_path (str): The path to the Swin Transformer configuration file.\n        num_classes (int, optional): The number of classes for classification. Defaults to 1.\n    \"\"\"\n    super().__init__()\n    config = SwinConfig.from_json_file(config_path)\n\n    self.swin = SwinModel(config)\n    self.classifier = nn.Linear(self.swin.num_features, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/swin/#unitorch.models.swin.SwinForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Performs a forward pass of the SwinForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>The input pixel values of the image.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/swin/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Performs a forward pass of the SwinForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): The input pixel values of the image.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.swin(\n        pixel_values=pixel_values,\n    )\n\n    pooled_output = outputs[1]\n    return self.classifier(pooled_output)\n</code></pre>"},{"location":"models/t5/","title":"unitorch.models.t5","text":""},{"location":"models/t5/#t5processor","title":"T5Processor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> <p>Processor for T5-based text generation models.</p> <p>Initializes the T5Processor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>special_input_ids</code> <code>Optional[Dict]</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>max_gen_seq_length</code> <code>Optional[int]</code> <p>Maximum generated sequence length. Defaults to 48.</p> <code>48</code> Source code in <code>src/unitorch/models/t5/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    \"\"\"\n    Initializes the T5Processor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        special_input_ids (Optional[Dict]): Special input IDs. Defaults to an empty dictionary.\n        max_seq_length (Optional[int]): Maximum sequence length. Defaults to 128.\n        max_gen_seq_length (Optional[int]): Maximum generated sequence length. Defaults to 48.\n    \"\"\"\n    tokenizer = get_t5_tokenizer(\n        vocab_path,\n        special_input_ids=special_input_ids,\n    )\n    tokenizer.bos_token_id = 0\n    tokenizer.bos_token = tokenizer.convert_ids_to_tokens(0)\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/t5/#t5forgeneration","title":"T5ForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>T5 model for text generation tasks.</p> <p>Initializes the T5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/t5/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the T5ForGeneration model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = T5Config.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = T5ForConditionalGeneration(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/t5/#unitorch.models.t5.T5ForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs forward pass of the T5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor of decoder attention mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/t5/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs forward pass of the T5ForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor of attention mask.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor of decoder attention mask.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/t5/#unitorch.models.t5.T5ForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the T5ForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The decoder's start token ID. Defaults to 2.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The decoder's end token ID. Defaults to 2.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/t5/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the T5ForGeneration model.\n\n    Args:\n        input_ids: The input token IDs.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The decoder's start token ID. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The decoder's end token ID. Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"models/visualbert/","title":"unitorch.models.visualbert","text":""},{"location":"models/visualbert/#visualbertprocessor","title":"VisualBertProcessor","text":"<p>             Bases: <code>BertProcessor</code></p> <p>Processor for VisualBERT-based models.</p> <p>Initializes the VisualBertProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>special_input_ids</code> <code>Optional[Dict]</code> <p>Special input IDs. Defaults to an empty dictionary.</p> <code>dict()</code> <code>do_lower_case</code> <code>Optional[bool]</code> <p>Whether to convert text to lowercase. Defaults to True.</p> <code>True</code> <code>do_basic_tokenize</code> <code>Optional[bool]</code> <p>Whether to perform basic tokenization. Defaults to True.</p> <code>True</code> <code>do_whole_word_mask</code> <code>Optional[bool]</code> <p>Whether to use whole word masking. Defaults to True.</p> <code>True</code> <code>masked_lm_prob</code> <code>Optional[float]</code> <p>Probability for masked LM. Defaults to 0.15.</p> <code>0.15</code> <code>max_predictions_per_seq</code> <code>Optional[int]</code> <p>Maximum number of masked LM predictions per sequence. Defaults to 20.</p> <code>20</code> Source code in <code>src/unitorch/models/visualbert/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path,\n    max_seq_length: Optional[int] = 128,\n    special_input_ids: Optional[Dict] = dict(),\n    do_lower_case: Optional[bool] = True,\n    do_basic_tokenize: Optional[bool] = True,\n    do_whole_word_mask: Optional[bool] = True,\n    masked_lm_prob: Optional[float] = 0.15,\n    max_predictions_per_seq: Optional[int] = 20,\n):\n    \"\"\"\n    Initializes the VisualBertProcessor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        max_seq_length (Optional[int]): Maximum sequence length. Defaults to 128.\n        special_input_ids (Optional[Dict]): Special input IDs. Defaults to an empty dictionary.\n        do_lower_case (Optional[bool]): Whether to convert text to lowercase. Defaults to True.\n        do_basic_tokenize (Optional[bool]): Whether to perform basic tokenization. Defaults to True.\n        do_whole_word_mask (Optional[bool]): Whether to use whole word masking. Defaults to True.\n        masked_lm_prob (Optional[float]): Probability for masked LM. Defaults to 0.15.\n        max_predictions_per_seq (Optional[int]): Maximum number of masked LM predictions per sequence. Defaults to 20.\n    \"\"\"\n    super().__init__(\n        vocab_path=vocab_path,\n        max_seq_length=max_seq_length,\n        do_lower_case=do_lower_case,\n        do_basic_tokenize=do_basic_tokenize,\n        do_whole_word_mask=do_whole_word_mask,\n        masked_lm_prob=masked_lm_prob,\n        max_predictions_per_seq=max_predictions_per_seq,\n    )\n</code></pre>"},{"location":"models/visualbert/#visualbertforclassification","title":"VisualBertForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>VisualBERT model for classification tasks.</p> <p>Initialize the VisualBertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the VisualBERT model config file.</p> required <code>num_classes</code> <code>int</code> <p>The number of output classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the VisualBertForClassification model.\n\n    Args:\n        config_path (str): The path to the VisualBERT model config file.\n        num_classes (int, optional): The number of output classes for classification. Defaults to 1.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = VisualBertConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.visual_bert = VisualBertModel(self.config)\n    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/visualbert/#unitorch.models.visualbert.VisualBertForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the VisualBertForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>token_type_ids</code> <code>Tensor</code> <p>The token type IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> required <code>visual_embeds</code> <code>Tensor</code> <p>The visual embeddings.</p> required <code>visual_attention_mask</code> <code>Tensor</code> <p>The visual attention mask.</p> required <code>visual_token_type_ids</code> <code>Tensor</code> <p>The visual token type IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The logits for classification.</p> Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the VisualBertForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        token_type_ids (torch.Tensor): The token type IDs.\n        position_ids (torch.Tensor): The position IDs.\n        visual_embeds (torch.Tensor): The visual embeddings.\n        visual_attention_mask (torch.Tensor): The visual attention mask.\n        visual_token_type_ids (torch.Tensor): The visual token type IDs.\n\n    Returns:\n        (torch.Tensor): The logits for classification.\n    \"\"\"\n    outputs = self.visual_bert(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        visual_embeds=visual_embeds,\n        visual_attention_mask=visual_attention_mask,\n        visual_token_type_ids=visual_token_type_ids,\n    )\n    pooled_output = outputs[1]\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/visualbert/#visualbertforpretrain","title":"VisualBertForPretrain","text":"<p>             Bases: <code>GenericModel</code></p> <p>VisualBERT model for pretraining tasks.</p> <p>Initialize the VisualBertForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>The path to the VisualBERT model config file.</p> required <code>gradient_checkpointing</code> <code>bool</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initialize the VisualBertForPretrain model.\n\n    Args:\n        config_path (str): The path to the VisualBERT model config file.\n        gradient_checkpointing (bool, optional): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = VisualBertConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.visual_bert = VisualBertModel(self.config)\n    self.cls = VisualBertPreTrainingHeads(self.config)\n    self.init_weights()\n\n    self.mlm_loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n    self.nsp_loss_fn = nn.CrossEntropyLoss(reduction=\"none\")\n</code></pre>"},{"location":"models/visualbert/#unitorch.models.visualbert.VisualBertForPretrain.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n    nsp_label: torch.Tensor,\n    mlm_label: torch.Tensor,\n    mlm_label_mask: torch.Tensor,\n)\n</code></pre> <p>Forward pass of the VisualBertForPretrain model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>The attention mask.</p> required <code>token_type_ids</code> <code>Tensor</code> <p>The token type IDs.</p> required <code>position_ids</code> <code>Tensor</code> <p>The position IDs.</p> required <code>visual_embeds</code> <code>Tensor</code> <p>The visual embeddings.</p> required <code>visual_attention_mask</code> <code>Tensor</code> <p>The visual attention mask.</p> required <code>visual_token_type_ids</code> <code>Tensor</code> <p>The visual token type IDs.</p> required <code>nsp_label</code> <code>Tensor</code> <p>The next sentence prediction labels.</p> required <code>mlm_label</code> <code>Tensor</code> <p>The masked language modeling labels.</p> required <code>mlm_label_mask</code> <code>Tensor</code> <p>The masked language modeling label mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loss of the model.</p> Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    token_type_ids: torch.Tensor,\n    position_ids: torch.Tensor,\n    visual_embeds: torch.Tensor,\n    visual_attention_mask: torch.Tensor,\n    visual_token_type_ids: torch.Tensor,\n    nsp_label: torch.Tensor,\n    mlm_label: torch.Tensor,\n    mlm_label_mask: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the VisualBertForPretrain model.\n\n    Args:\n        input_ids (torch.Tensor): The input token IDs.\n        attention_mask (torch.Tensor): The attention mask.\n        token_type_ids (torch.Tensor): The token type IDs.\n        position_ids (torch.Tensor): The position IDs.\n        visual_embeds (torch.Tensor): The visual embeddings.\n        visual_attention_mask (torch.Tensor): The visual attention mask.\n        visual_token_type_ids (torch.Tensor): The visual token type IDs.\n        nsp_label (torch.Tensor): The next sentence prediction labels.\n        mlm_label (torch.Tensor): The masked language modeling labels.\n        mlm_label_mask (torch.Tensor): The masked language modeling label mask.\n\n    Returns:\n        (torch.Tensor): The loss of the model.\n    \"\"\"\n    outputs = self.visual_bert(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n        visual_embeds=visual_embeds,\n        visual_attention_mask=visual_attention_mask,\n        visual_token_type_ids=visual_token_type_ids,\n    )\n    sequence_output, pooled_output = outputs[:2]\n    prediction_scores, seq_relationship_score = self.cls(\n        sequence_output, pooled_output\n    )\n\n    batch_size, seq_len, vocab_size = prediction_scores.size()\n    masked_lm_loss = self.mlm_loss_fn(\n        prediction_scores.view(-1, vocab_size), mlm_label.view(-1)\n    ) * mlm_label_mask.view(-1)\n    masked_lm_loss = masked_lm_loss.view(batch_size, seq_len).sum(1) / torch.max(\n        mlm_label_mask.view(batch_size, seq_len).sum(1),\n        torch.ones(batch_size).to(mlm_label_mask.device),\n    )\n    loss = masked_lm_loss.mean()\n\n    loss += self.nsp_loss_fn(\n        seq_relationship_score.view(-1, 2), nsp_label.view(-1)\n    ).mean()\n\n    return loss\n</code></pre>"},{"location":"models/visualbert/#unitorch.models.visualbert.VisualBertForPretrain.get_output_embeddings","title":"get_output_embeddings","text":"<pre><code>get_output_embeddings()\n</code></pre> <p>Get the output embeddings of the model.</p> <p>Returns:</p> Type Description <code>Module</code> <p>The output embeddings.</p> Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def get_output_embeddings(self):\n    \"\"\"\n    Get the output embeddings of the model.\n\n    Returns:\n        (nn.Module): The output embeddings.\n    \"\"\"\n    return self.cls.predictions.decoder\n</code></pre>"},{"location":"models/visualbert/#unitorch.models.visualbert.VisualBertForPretrain.set_output_embeddings","title":"set_output_embeddings","text":"<pre><code>set_output_embeddings(new_embeddings)\n</code></pre> <p>Set the output embeddings of the model.</p> <p>Parameters:</p> Name Type Description Default <code>new_embeddings</code> <code>Module</code> <p>The new output embeddings.</p> required Source code in <code>src/unitorch/models/visualbert/modeling.py</code> <pre><code>def set_output_embeddings(self, new_embeddings):\n    \"\"\"\n    Set the output embeddings of the model.\n\n    Args:\n        new_embeddings (nn.Module): The new output embeddings.\n    \"\"\"\n    self.cls.predictions.decoder = new_embeddings\n</code></pre>"},{"location":"models/vit/","title":"unitorch.models.vit","text":""},{"location":"models/vit/#vitprocessor","title":"ViTProcessor","text":"<p>             Bases: <code>HfImageClassificationProcessor</code></p> <p>Processor for ViT-based image classification models.</p> <p>Initializes the ViTProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vision_config_path</code> <code>str</code> <p>Path to the vision configuration file.</p> required Source code in <code>src/unitorch/models/vit/processing.py</code> <pre><code>def __init__(\n    self,\n    vision_config_path: str,\n):\n    \"\"\"\n    Initializes the ViTProcessor.\n\n    Args:\n        vision_config_path (str): Path to the vision configuration file.\n    \"\"\"\n    vision_processor = ViTImageProcessor.from_json_file(vision_config_path)\n    super().__init__(\n        vision_processor=vision_processor,\n    )\n</code></pre>"},{"location":"models/vit/#vitforimageclassification","title":"ViTForImageClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>ViT model for image classification tasks.</p> <p>Initializes the ViTForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes. Defaults to 1.</p> <code>1</code> Source code in <code>src/unitorch/models/vit/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n):\n    \"\"\"\n    Initializes the ViTForImageClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        num_classes (Optional[int]): Number of classes. Defaults to 1.\n    \"\"\"\n    super().__init__()\n    config = ViTConfig.from_json_file(config_path)\n\n    self.vit = ViTModel(config)\n    self.classifier = nn.Linear(config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/vit/#unitorch.models.vit.ViTForImageClassification.forward","title":"forward","text":"<pre><code>forward(pixel_values: torch.Tensor)\n</code></pre> <p>Forward pass of the ViTForImageClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>pixel_values</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, num_channels, height, width].</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits of shape [batch_size, num_classes].</p> Source code in <code>src/unitorch/models/vit/modeling.py</code> <pre><code>def forward(\n    self,\n    pixel_values: torch.Tensor,\n):\n    \"\"\"\n    Forward pass of the ViTForImageClassification model.\n\n    Args:\n        pixel_values (torch.Tensor): Input tensor of shape [batch_size, num_channels, height, width].\n\n    Returns:\n        (torch.Tensor):Output logits of shape [batch_size, num_classes].\n    \"\"\"\n    vision_outputs = self.vit(\n        pixel_values=pixel_values,\n    )\n    pooled_output = vision_outputs[1]\n    return self.classifier(pooled_output)\n</code></pre>"},{"location":"models/xlm_roberta/","title":"unitorch.models.xlm_roberta","text":""},{"location":"models/xlm_roberta/#xlmrobertaprocessor","title":"XLMRobertaProcessor","text":"<p>             Bases: <code>HfTextClassificationProcessor</code></p> <p>Processor for XLM-RoBERTa model for text classification tasks.</p> <p>Initializes the XLMRobertaProcessor.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_path</code> <code>str</code> <p>Path to the vocabulary file.</p> required <code>max_seq_length</code> <code>Optional[int]</code> <p>Maximum sequence length. Defaults to 128.</p> <code>128</code> <code>source_type_id</code> <code>Optional[int]</code> <p>Source type ID. Defaults to 0.</p> <code>0</code> <code>target_type_id</code> <code>Optional[int]</code> <p>Target type ID. Defaults to 0.</p> <code>0</code> Source code in <code>src/unitorch/models/xlm_roberta/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    max_seq_length: Optional[int] = 128,\n    source_type_id: Optional[int] = 0,\n    target_type_id: Optional[int] = 0,\n):\n    \"\"\"\n    Initializes the XLMRobertaProcessor.\n\n    Args:\n        vocab_path (str): Path to the vocabulary file.\n        max_seq_length (Optional[int]): Maximum sequence length. Defaults to 128.\n        source_type_id (Optional[int]): Source type ID. Defaults to 0.\n        target_type_id (Optional[int]): Target type ID. Defaults to 0.\n    \"\"\"\n    tokenizer = get_xlm_roberta_tokenizer(\n        vocab_path,\n    )\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        source_type_id=source_type_id,\n        target_type_id=target_type_id,\n        position_start_id=self.pad_token_id + 1,\n    )\n</code></pre>"},{"location":"models/xlm_roberta/#xlmrobertaforclassification","title":"XLMRobertaForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>XLM-RoBERTa model for classification tasks.</p> <p>Initializes the XLMRobertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the XLMRobertaForClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        num_classes (Optional[int]): Number of classes. Defaults to 1.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = XLMRobertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.roberta = XLMRobertaModel(self.config)\n    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/xlm_roberta/#unitorch.models.xlm_roberta.XLMRobertaForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the XLMRobertaForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, sequence_length].</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Attention mask tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Tensor]</code> <p>Token type IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Optional[Tensor]</code> <p>Position IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits of shape [batch_size, num_classes].</p> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the XLMRobertaForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape [batch_size, sequence_length].\n        attention_mask (Optional[torch.Tensor]): Attention mask tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        token_type_ids (Optional[torch.Tensor]): Token type IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        position_ids (Optional[torch.Tensor]): Position IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n\n    Returns:\n        (torch.Tensor):Output logits of shape [batch_size, num_classes].\n    \"\"\"\n    outputs = self.roberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = outputs[1]\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/xlm_roberta/#xlmrobertaformasklm","title":"XLMRobertaForMaskLM","text":"<p>             Bases: <code>GenericModel</code></p> <p>XLM-RoBERTa model for masked language modeling tasks.</p> <p>Initializes the XLMRobertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the XLMRobertaForMaskLM model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = XLMRobertaConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.roberta = XLMRobertaModel(self.config, add_pooling_layer=False)\n    self.lm_head = RobertaLMHead(self.config)\n    self.init_weights()\n    self.roberta.embeddings.word_embeddings.weight = self.lm_head.decoder.weight\n</code></pre>"},{"location":"models/xlm_roberta/#unitorch.models.xlm_roberta.XLMRobertaForMaskLM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the XLMRobertaForMaskLM model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, sequence_length].</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Attention mask tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Tensor]</code> <p>Token type IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Optional[Tensor]</code> <p>Position IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits of shape [batch_size, sequence_length, vocabulary_size].</p> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the XLMRobertaForMaskLM model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape [batch_size, sequence_length].\n        attention_mask (Optional[torch.Tensor]): Attention mask tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        token_type_ids (Optional[torch.Tensor]): Token type IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        position_ids (Optional[torch.Tensor]): Position IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n\n    Returns:\n        (torch.Tensor):Output logits of shape [batch_size, sequence_length, vocabulary_size].\n    \"\"\"\n    outputs = self.roberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    sequence_output = outputs[0]\n    logits = self.lm_head(sequence_output)\n    return logits\n</code></pre>"},{"location":"models/xlm_roberta/#xlmrobertaxlforclassification","title":"XLMRobertaXLForClassification","text":"<p>             Bases: <code>GenericModel</code></p> <p>XLM-RoBERTa XL model for classification tasks.</p> <p>Initializes the XLMRobertaXLForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for classification. Defaults to 1.</p> <code>1</code> <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    num_classes: Optional[int] = 1,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the XLMRobertaXLForClassification model.\n\n    Args:\n        config_path (str): Path to the configuration file.\n        num_classes (Optional[int]): Number of classes for classification. Defaults to 1.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = XLMRobertaXLConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.roberta = XLMRobertaXLModel(self.config)\n    self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n    self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n    self.init_weights()\n</code></pre>"},{"location":"models/xlm_roberta/#unitorch.models.xlm_roberta.XLMRobertaXLForClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n)\n</code></pre> <p>Forward pass of the XLMRobertaXLForClassification model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, sequence_length].</p> required <code>attention_mask</code> <code>Optional[Tensor]</code> <p>Attention mask tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>token_type_ids</code> <code>Optional[Tensor]</code> <p>Token type IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <code>position_ids</code> <code>Optional[Tensor]</code> <p>Position IDs tensor of shape [batch_size, sequence_length]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output logits of shape [batch_size, num_classes].</p> Source code in <code>src/unitorch/models/xlm_roberta/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n):\n    \"\"\"\n    Forward pass of the XLMRobertaXLForClassification model.\n\n    Args:\n        input_ids (torch.Tensor): Input tensor of shape [batch_size, sequence_length].\n        attention_mask (Optional[torch.Tensor]): Attention mask tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        token_type_ids (Optional[torch.Tensor]): Token type IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n        position_ids (Optional[torch.Tensor]): Position IDs tensor of shape [batch_size, sequence_length].\n            Defaults to None.\n\n    Returns:\n        (torch.Tensor):Output logits of shape [batch_size, num_classes].\n    \"\"\"\n    outputs = self.roberta(\n        input_ids,\n        attention_mask=attention_mask,\n        token_type_ids=token_type_ids,\n        position_ids=position_ids,\n    )\n    pooled_output = outputs[1]\n\n    pooled_output = self.dropout(pooled_output)\n    logits = self.classifier(pooled_output)\n    return logits\n</code></pre>"},{"location":"models/xpegasus/","title":"unitorch.models.xpegasus","text":""},{"location":"models/xpegasus/#xpegasusprocessor","title":"XPegasusProcessor","text":"<p>             Bases: <code>HfTextGenerationProcessor</code></p> Source code in <code>src/unitorch/models/xpegasus/processing.py</code> <pre><code>def __init__(\n    self,\n    vocab_path: str,\n    special_input_ids: Optional[Dict] = dict(),\n    max_seq_length: Optional[int] = 128,\n    max_gen_seq_length: Optional[int] = 48,\n):\n    tokenizer = get_xpegasus_tokenizer(\n        vocab_path,\n        special_input_ids=special_input_ids,\n    )\n    tokenizer.bos_token_id = 0\n    tokenizer.bos_token = tokenizer.convert_ids_to_tokens(0)\n    tokenizer.sep_token = tokenizer.eos_token\n    tokenizer.sep_token_id = tokenizer.eos_token_id\n    super().__init__(\n        tokenizer=tokenizer,\n        max_seq_length=max_seq_length,\n        max_gen_seq_length=max_gen_seq_length,\n    )\n</code></pre>"},{"location":"models/xpegasus/#xpegasusforgeneration","title":"XPegasusForGeneration","text":"<p>             Bases: <code>GenericModel</code></p> <p>XPegasus model for text generation tasks.</p> <p>Initializes the XPegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the model configuration file.</p> required <code>gradient_checkpointing</code> <code>Optional[bool]</code> <p>Whether to use gradient checkpointing. Defaults to False.</p> <code>False</code> Source code in <code>src/unitorch/models/xpegasus/modeling.py</code> <pre><code>def __init__(\n    self,\n    config_path: str,\n    gradient_checkpointing: Optional[bool] = False,\n):\n    \"\"\"\n    Initializes the XPegasusForGeneration model.\n\n    Args:\n        config_path (str): Path to the model configuration file.\n        gradient_checkpointing (Optional[bool]): Whether to use gradient checkpointing. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.config = PegasusXConfig.from_json_file(config_path)\n    self.config.gradient_checkpointing = gradient_checkpointing\n    self.model = PegasusXForConditionalGeneration(self.config)\n    self.init_weights()\n</code></pre>"},{"location":"models/xpegasus/#unitorch.models.xpegasus.XPegasusForGeneration.forward","title":"forward","text":"<pre><code>forward(\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n)\n</code></pre> <p>Performs forward pass of the XPegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>Tensor of input token IDs.</p> required <code>attention_mask</code> <code>Tensor</code> <p>Tensor of attention mask.</p> required <code>decoder_input_ids</code> <code>Tensor</code> <p>Tensor of decoder input token IDs.</p> required <code>decoder_attention_mask</code> <code>Tensor</code> <p>Tensor of decoder attention mask.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The model's logits.</p> Source code in <code>src/unitorch/models/xpegasus/modeling.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    attention_mask: torch.Tensor,\n    decoder_input_ids: torch.Tensor,\n    decoder_attention_mask: torch.Tensor,\n):\n    \"\"\"\n    Performs forward pass of the XPegasusForGeneration model.\n\n    Args:\n        input_ids (torch.Tensor): Tensor of input token IDs.\n        attention_mask (torch.Tensor): Tensor of attention mask.\n        decoder_input_ids (torch.Tensor): Tensor of decoder input token IDs.\n        decoder_attention_mask (torch.Tensor): Tensor of decoder attention mask.\n\n    Returns:\n        (torch.Tensor):The model's logits.\n    \"\"\"\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        decoder_input_ids=decoder_input_ids,\n        decoder_attention_mask=decoder_attention_mask,\n        return_dict=True,\n    )\n    logits = outputs.logits\n    return logits\n</code></pre>"},{"location":"models/xpegasus/#unitorch.models.xpegasus.XPegasusForGeneration.generate","title":"generate","text":"<pre><code>generate(\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[\n        Union[int, List[int]]\n    ] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n)\n</code></pre> <p>Generates sequences using the XPegasusForGeneration model.</p> <p>Parameters:</p> Name Type Description Default <code>input_ids</code> <code>Tensor</code> <p>The input token IDs.</p> required <code>num_beams</code> <code>int</code> <p>The number of beams for beam search. Defaults to 5.</p> <code>5</code> <code>decoder_start_token_id</code> <code>int</code> <p>The decoder's start token ID. Defaults to 2.</p> <code>0</code> <code>decoder_end_token_id</code> <code>int or List[int]</code> <p>The decoder's end token ID. Defaults to 2.</p> <code>1</code> <code>num_return_sequences</code> <code>int</code> <p>The number of generated sequences to return. Defaults to 1.</p> <code>1</code> <code>min_gen_seq_length</code> <code>int</code> <p>The minimum length of the generated sequences. Defaults to 0.</p> <code>0</code> <code>max_gen_seq_length</code> <code>int</code> <p>The maximum length of the generated sequences. Defaults to 48.</p> <code>48</code> <code>repetition_penalty</code> <code>float</code> <p>The repetition penalty. Defaults to 1.0.</p> <code>1.0</code> <code>no_repeat_ngram_size</code> <code>int</code> <p>The size of n-grams to avoid repeating. Defaults to 0.</p> <code>0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to stop generation early. Defaults to True.</p> <code>True</code> <code>length_penalty</code> <code>float</code> <p>The length penalty. Defaults to 1.0.</p> <code>1.0</code> <code>num_beam_groups</code> <code>int</code> <p>The number of beam groups for diverse beam search. Defaults to 1.</p> <code>1</code> <code>diversity_penalty</code> <code>float</code> <p>The diversity penalty. Defaults to 0.0.</p> <code>0.0</code> <code>do_sample</code> <code>bool</code> <p>Whether to use sampling for generation. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>The temperature for sampling. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>The value for top-k sampling. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>The value for top-p (nucleus) sampling. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>GenericOutputs</code> <p>The generated sequences and their scores.</p> Source code in <code>src/unitorch/models/xpegasus/modeling.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    input_ids: torch.Tensor,\n    num_beams: Optional[int] = 5,\n    decoder_start_token_id: Optional[int] = 0,\n    decoder_end_token_id: Optional[Union[int, List[int]]] = 1,\n    num_return_sequences: Optional[int] = 1,\n    min_gen_seq_length: Optional[int] = 0,\n    max_gen_seq_length: Optional[int] = 48,\n    repetition_penalty: Optional[float] = 1.0,\n    no_repeat_ngram_size: Optional[int] = 0,\n    early_stopping: Optional[bool] = True,\n    length_penalty: Optional[float] = 1.0,\n    num_beam_groups: Optional[int] = 1,\n    diversity_penalty: Optional[float] = 0.0,\n    do_sample: Optional[bool] = False,\n    temperature: Optional[float] = 1.0,\n    top_k: Optional[int] = 50,\n    top_p: Optional[float] = 1.0,\n):\n    \"\"\"\n    Generates sequences using the XPegasusForGeneration model.\n\n    Args:\n        input_ids: The input token IDs.\n        num_beams (int, optional): The number of beams for beam search. Defaults to 5.\n        decoder_start_token_id (int, optional): The decoder's start token ID. Defaults to 2.\n        decoder_end_token_id (int or List[int], optional): The decoder's end token ID. Defaults to 2.\n        num_return_sequences (int, optional): The number of generated sequences to return. Defaults to 1.\n        min_gen_seq_length (int, optional): The minimum length of the generated sequences. Defaults to 0.\n        max_gen_seq_length (int, optional): The maximum length of the generated sequences. Defaults to 48.\n        repetition_penalty (float, optional): The repetition penalty. Defaults to 1.0.\n        no_repeat_ngram_size (int, optional): The size of n-grams to avoid repeating. Defaults to 0.\n        early_stopping (bool, optional): Whether to stop generation early. Defaults to True.\n        length_penalty (float, optional): The length penalty. Defaults to 1.0.\n        num_beam_groups (int, optional): The number of beam groups for diverse beam search. Defaults to 1.\n        diversity_penalty (float, optional): The diversity penalty. Defaults to 0.0.\n        do_sample (bool, optional): Whether to use sampling for generation. Defaults to False.\n        temperature (float, optional): The temperature for sampling. Defaults to 1.0.\n        top_k (int, optional): The value for top-k sampling. Defaults to 50.\n        top_p (float, optional): The value for top-p (nucleus) sampling. Defaults to 1.0.\n\n    Returns:\n        GenericOutputs: The generated sequences and their scores.\n    \"\"\"\n    outputs = self.model.generate(\n        input_ids,\n        max_length=max_gen_seq_length,\n        min_length=min_gen_seq_length,\n        num_beams=num_beams,\n        do_sample=do_sample,\n        decoder_start_token_id=decoder_start_token_id,\n        no_repeat_ngram_size=no_repeat_ngram_size,\n        early_stopping=early_stopping,\n        length_penalty=length_penalty,\n        repetition_penalty=repetition_penalty,\n        num_return_sequences=num_return_sequences,\n        bos_token_id=decoder_start_token_id,\n        eos_token_id=decoder_end_token_id,\n        num_beam_groups=num_beam_groups,\n        diversity_penalty=diversity_penalty,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        return_dict_in_generate=True,\n        output_scores=True,\n    )\n\n    sequences = outputs.sequences.reshape(\n        -1, num_return_sequences, outputs.sequences.size(-1)\n    )\n    outputs.sequences = torch.zeros(\n        sequences.size(0), num_return_sequences, max_gen_seq_length\n    ).to(device=sequences.device)\n    outputs.sequences[:, :, : sequences.size(-1)].copy_(sequences)\n\n    if num_return_sequences == 1:\n        outputs.sequences = outputs.sequences.reshape(-1, max_gen_seq_length)\n\n    return GenericOutputs(\n        sequences=outputs.sequences,\n        sequences_scores=outputs.sequences_scores,\n    )\n</code></pre>"},{"location":"coverage/","title":"Coverage","text":""}]}